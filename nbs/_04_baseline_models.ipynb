{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "> This notebook tries generate baseline models to evaluate if DCAE models have a good performance or nor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore import test\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from timecluster_extension.load import *\n",
    "from timecluster_extension.dr import *\n",
    "from timecluster_extension.visualization import *\n",
    "from timecluster_extension.utils import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from yaml import load, FullLoader\n",
    "from fastcore.utils import Path\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a run to save the models (job_type = \"baseline_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "run_baseline = wandb.init(entity = \"pacmel\",\n",
    "                      project=\"timecluster-extension\",\n",
    "                      job_type='baseline_models',\n",
    "                      allow_val_change=True,\n",
    "                      resume=False)\n",
    "config = wandb.config  # Object for storing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_name_and_version = 'JNK:train_10days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_artifact = run_baseline.use_artifact(artifact_name_and_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (uncomment to override the yaml file)\n",
    "config.update(\n",
    "    {\n",
    "          'ds_train_artifact_type': ds_train_artifact.type,\n",
    "          'ds_train_artifact_name': ds_train_artifact.name,\n",
    "          'ds_train_artifact_digest': ds_train_artifact.digest,\n",
    "    }, \n",
    "    allow_val_change=True)\n",
    "ds_train_artifact.type, ds_train_artifact.name, ds_train_artifact.digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds_train_artifact.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.w = ifnone(config.get('w'), 48)\n",
    "config.stride = ifnone(config.get('stride'), 1)\n",
    "config.t = ifnone(config.get('t'), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters (uncomment to override the yaml file)\n",
    "config.update({\n",
    "    'w': config.w,\n",
    "    'stride': config.stride,\n",
    "    't': config.t  # TODO: Not supported yet\n",
    "    }, allow_val_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.equals(config.w % 12, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df_slicer(df_train, w=config.w, s=config.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = input_data#Take 10 windows data_test[0:10,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export utils\n",
    "def baseline_model_predictor(input_array, operation = \"mean\"):\n",
    "    \" Perform an arithmetic operation (median or average) on a three-dimensional numpy array from df_slicer\"\n",
    "    # Calculate mean/median for each window in the dataset\n",
    "    if operation == \"mean\":\n",
    "        prediction = np.mean(input_array,axis=1)\n",
    "    elif operation == \"median\":\n",
    "        prediction = np.median(input_array,axis=1)\n",
    "    # Generate an output numpy array with the same size that input_array with\n",
    "    # baseline predictions\n",
    "    # Create a 3-d numpy array with ones\n",
    "    output_array = np.ones(input_array.shape)\n",
    "    # Multiply it by the prediction array, with a new dimension in axis 1\n",
    "    output_array = output_array * np.expand_dims(prediction, axis=1)\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_type = \"median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model_predictor(data_test, baseline_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "        'baseline_type': baseline_type\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output have the same shape\n",
    "assert y_pred.shape == data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export utils\n",
    "import tensorflow as tf\n",
    "def get_windows_mse(predictions, original_data):\n",
    "    \" Function that calculates the mse for each of the windows in which an auto-encoder model has made a prediction.\"\n",
    "    # Test that dimensions are correct.\n",
    "    assert predictions.shape == original_data.shape\n",
    "    # Create a mse object\n",
    "    mse = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # We need to adapt the axes to calculate the mse in the manner we want.\n",
    "    prediction_swaped = np.swapaxes(predictions,1,2)\n",
    "    original_data_swaped = np.swapaxes(original_data,1,2)\n",
    "    # Calculate mses\n",
    "    windows_mse = mse(original_data_swaped, prediction_swaped).numpy()\n",
    "    \n",
    "    return windows_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "windows_mse = get_windows_mse(y_pred,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_mse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE per time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_mse.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_mse.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log results on wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline.log({\n",
    "    'mse_overall': windows_mse.mean(),\n",
    "    'mse_signal': windows_mse.mean(axis=0), # MSE of each variable\n",
    "    'mse_raw': windows_mse # Raw mse. It has the mse for each window and signal\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot figure to visualize MSE per variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(list(df_train.columns),  windows_mse.mean(axis=0), align='center', alpha=0.5)\n",
    "plt.xticks(list(df_train.columns), rotation='vertical')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('variable')\n",
    "\n",
    "\n",
    "wandb.log({\"mse_signal_plot\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It proves that `get_windows_mse()` is working correctly compared to the sklearn function. This check is done because the function is swapped, and it is convenient that the results are the same regardless of the method used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rme_sklearn = [0]* y_pred.shape[2]\n",
    "for i in range(0,y_pred.shape[2]):\n",
    "    y_pred_sel = y_pred[:,:,i]\n",
    "    test_sel = data_test[:,:,i]\n",
    "    rme_sklearn[i] = mean_squared_error(test_sel,y_pred_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rme_sklearn_round = np.around(rme_sklearn, decimals=3)\n",
    "rme_keras_round = np.around(windows_mse.mean(axis=0),decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.all_equal(rme_sklearn_round, rme_keras_round)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
