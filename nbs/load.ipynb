{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> Methods for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "from dvats.imports import *\n",
    "from dvats.utils import *\n",
    "import pickle\n",
    "import pyarrow.feather as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.imports import beep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series artifacts (to be used with weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is meant to extend `wandb.Artifact` for logging/using files with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSArtifact(wandb.Artifact):\n",
    "\n",
    "    default_storage_path = Path(Path.home()/'data/wandb_artifacts/')\n",
    "    date_format = '%Y-%m-%d %H:%M:%S' # TODO add milliseconds\n",
    "    handle_missing_values_techniques = {\n",
    "        'linear_interpolation': lambda df : df.interpolate(method='linear', limit_direction='both'),\n",
    "        'overall_mean': lambda df : df.fillna(df.mean()),\n",
    "        'overall_median': lambda df : df.fillna(df.median()),\n",
    "        'backward_fill' : lambda df : df.fillna(method='bfill'),\n",
    "        'forward_fill' : lambda df : df.fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "    \"Class that represents a wandb artifact containing time series data. sd stands for start_date \\\n",
    "    and ed for end_date. Both should be pd.Timestamps\"\n",
    "\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, name, sd:pd.Timestamp, ed:pd.Timestamp, **kwargs):\n",
    "        super().__init__(type='dataset', name=name, **kwargs)\n",
    "        self.sd = sd\n",
    "        self.ed = ed\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['TS'] = dict(sd = self.sd.strftime(self.date_format),\n",
    "                                   ed = self.ed.strftime(self.date_format))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_daily_csv_files(cls, root_path, fread=pd.read_csv, start_date=None, end_date=None, metadata=None, **kwargs):\n",
    "\n",
    "        \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "        to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "        defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "        artifact name, and the date range as version.\"\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(__init__)\n",
    "    def from_df(cls, df:pd.DataFrame, name:str, path:str=None, sd:pd.Timestamp=None, ed:pd.Timestamp=None,\n",
    "                normalize:bool=False, missing_values_technique:str=None, resampling_freq:str=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Create a TSArtifact of type `dataset`, using the DataFrame `df` samples from \\\n",
    "        `sd` (start date) to `ed` (end date). Dates must be passed as `datetime.datetime` \\\n",
    "        objects. The transformed DataFrame is stored as a pickle file in the path `path` \\\n",
    "        and its reference is added to the artifact entries. Additionally, the dataset can \\\n",
    "        be normalized (see `normalize` argument) or transformed using missing values \\\n",
    "        handling techniques (see `missing_values_technique` argument) or resampling (see \\\n",
    "        `resampling_freq` argument).\n",
    "\n",
    "        Arguments:\n",
    "            df: (DataFrame) The dataframe you want to convert into an artifact.\n",
    "            name: (str) The artifact name.\n",
    "            path: (str, optional) The path where the file, containing the new transformed \\\n",
    "                dataframe, is saved. Default None.\n",
    "            sd: (sd, optional) Start date. By default, the first index of `df` is taken.\n",
    "            ed: (ed, optional) End date. By default, the last index of `df` is taken.\n",
    "            normalize: (bool, optional) If the dataset values should be normalized. Default\\\n",
    "                False.\n",
    "            missing_values_technique: (str, optional) The technique used to handle missing \\\n",
    "                values. Options: \"linear_iterpolation\", \"overall_mean\", \"overall_median\" or \\\n",
    "                None. Default None.\n",
    "            resampling_freq: (str, optional) The offset string or object representing \\\n",
    "                frequency conversion for time series resampling. Default None.\n",
    "\n",
    "        Returns:\n",
    "            TSArtifact object.\n",
    "        \"\"\"\n",
    "        sd = df.index[0] if sd is None else sd\n",
    "        ed = df.index[-1] if ed is None else ed\n",
    "        obj = cls(name, sd=sd, ed=ed, **kwargs)\n",
    "        df = df.query('@obj.sd <= index <= @obj.ed')\n",
    "        obj.metadata['TS']['created'] = 'from-df'\n",
    "        obj.metadata['TS']['n_vars'] = df.columns.__len__()\n",
    "\n",
    "        # Handle Missing Values\n",
    "        df = obj.handle_missing_values_techniques[missing_values_technique](df) if missing_values_technique is not None else df\n",
    "        obj.metadata['TS']['handle_missing_values_technique'] = missing_values_technique.__str__()\n",
    "        obj.metadata['TS']['has_missing_values'] = np.any(df.isna().values).__str__()\n",
    "\n",
    "        # Indexing and Resampling\n",
    "        if resampling_freq: df = df.resample(resampling_freq).mean()\n",
    "        obj.metadata['TS']['n_samples'] = len(df)\n",
    "        obj.metadata['TS']['freq'] = str(df.index.freq)\n",
    "\n",
    "        # Time Series Variables\n",
    "        obj.metadata['TS']['vars'] = list(df.columns)\n",
    "\n",
    "        # Normalization - Save the previous means and stds\n",
    "        if normalize:\n",
    "            obj.metadata['TS']['normalization'] = dict(means = df.describe().loc['mean'].to_dict(),\n",
    "                                                       stds = df.describe().loc['std'].to_dict())\n",
    "            df = normalize_columns(df)\n",
    "\n",
    "        # Hash and save\n",
    "        hash_code = str(pd.util.hash_pandas_object(df).sum()) #Â str(hash(df.values.tobytes()))\n",
    "        path = obj.default_storage_path/f'{hash_code}' if path is None else Path(path)/f'{hash_code}'\n",
    "        print(\"About to write df to \", path)\n",
    "        ft.write_feather(df, path, compression = 'lz4')\n",
    "        #feather.write_dataframe\n",
    "        obj.metadata['TS']['hash'] = hash_code\n",
    "        obj.add_file(str(path))\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSArtifact class TEST\n",
    "\n",
    "# resampling frequency\n",
    "resampling_freq = '5s'\n",
    "# handle missing values technique\n",
    "missing_values_technique='overall_median'\n",
    "\n",
    "# testing dataframe\n",
    "df_test = pd.util.testing.makeMissingDataframe()\n",
    "df_test.index = pd.date_range(start='2021-01-01', periods=len(df_test), freq='s')\n",
    "\n",
    "artifact = TSArtifact.from_df(df_test, \n",
    "                              name='JNK', \n",
    "                              missing_values_technique=missing_values_technique,\n",
    "                              resampling_freq=resampling_freq, \n",
    "                              normalize=True)\n",
    "artifact.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash = artifact.metadata['TS']['hash']\n",
    "path = \"../../data/wandb_artifacts/\"+hash\n",
    "print(path)\n",
    "f = ft.read_feather(path)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ft.read_feather(\"/home/macu/data/wandb_artifacts/-2535364569820284064\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a `wandb.apis.public.Artifact` object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original `TSArtifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_df(self:wandb.apis.public.Artifact):\n",
    "    \"Download the files of a saved wandb artifact and process them as a single dataframe. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\n",
    "    # The way we have to ensure that the argument comes from a TS arfitact is the metadata\n",
    "    if self.metadata.get('TS') is None:\n",
    "        print(f'ERROR:{self} does not come from a logged TSArtifact')\n",
    "        return None\n",
    "    dir = Path(self.download())\n",
    "    if self.metadata['TS']['created'] == 'from-df':\n",
    "        # Call read_pickle with the single file from dir\n",
    "        #return pd.read_pickle(dir.ls()[0])\n",
    "        return ft.read_feather(dir.ls()[0])\n",
    "    else:\n",
    "        print(\"ERROR: Only from_df method is allowed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can write a method to cast a downloaded wandb artifact (instance from `wandb.apis.public,Artifact`) to a TSArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_tsartifact(self:wandb.apis.public.Artifact):\n",
    "    \"Cast an artifact as a TS artifact. The artifact must have been created from one of the \\\n",
    "    class creation methods of the class `TSArtifact`. This is useful to go back to a TSArtifact \\\n",
    "    after downloading an artifact through the wand API\"\n",
    "    return TSArtifact(name=self.digest, #TODO change this\n",
    "                      sd=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      ed=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      description=self.description,\n",
    "                      metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject or infer frequencies in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(pd.to_datetime)\n",
    "def infer_or_inject_freq(df, injected_freq='1s', start_date=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Infer index frequency. If there's not a proper time index, create fake timestamps,\n",
    "        keeping the desired `injected_freq`. If that is None, set a default one of 1 second.\n",
    "        start_date: the first date of the index (int or string).\n",
    "    \"\"\"\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    if inferred_freq == 'N':\n",
    "        timedelta = pd.to_timedelta(injected_freq)\n",
    "        df.index = pd.to_datetime(ifnone(start_date, 0), **kwargs) + timedelta*df.index\n",
    "        df.index.freq = pd.infer_freq(df.index)\n",
    "    else:\n",
    "        df.index.freq = inferred_freq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = pd.DataFrame([1, 2, 3])\n",
    "foo = infer_or_inject_freq(foo)\n",
    "bar = infer_or_inject_freq(bar, injected_freq='2s')\n",
    "test_eq(foo.index.freq, '1s')\n",
    "test_eq(bar.index.freq, '2s')\n",
    "foo, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = infer_or_inject_freq(foo, injected_freq='1W', start_date='01/01/2020')\n",
    "baz = infer_or_inject_freq(foo, injected_freq='1W', start_date='2020-01-01', format = '%Y-%m-%d')\n",
    "test_eq(bar, baz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "    print(f\"File downloaded as: {destination}\")\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    print(destination)\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "import zipfile\n",
    "\n",
    "def zip_contents(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        return zip_file.namelist()\n",
    "\n",
    "\n",
    "def unzip_mat(all_one, zip_path, extract_path, case = '', print_flag = True):\n",
    "    if print_flag: print(\"--> Unzip_mat\", all_one, zip_path, extract_path, case, print_flag)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        mat_files = [file for file in zip_ref.namelist() if file.endswith('.mat') and not file.startswith('__MACOSX/')]\n",
    "        if print_flag: print(mat_files)\n",
    "        if all_one == \"all\":\n",
    "            # Extract\n",
    "            for file in mat_files:\n",
    "                zip_ref.extract(file, extract_path)\n",
    "            return f\"{mat_files} extracted to {extract_path}\"\n",
    "        \n",
    "        elif all_one == \"one\":\n",
    "            if case == \"\":\n",
    "                # Extract first .mat\n",
    "                zip_ref.extract(mat_files[0], extract_path)\n",
    "                return f\"{mat_files[0]} extracted to  {extract_path}\"\n",
    "            else:\n",
    "                # Extract <case>.mat\n",
    "                mat_file = next((file for file in mat_files if case in file), None)\n",
    "                if mat_file:\n",
    "                    zip_ref.extract(mat_file, extract_path)\n",
    "                    return f\"{mat_file} extracted to {extract_path}\"\n",
    "                else:\n",
    "                    return \"None \"+case+\".mat found.\"\n",
    "        else:\n",
    "            return \"First parameter must be 'all' or 'one'.\"\n",
    "        print(\"unzip_path -->\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Unzip_mat all /home/macu/data/InsectData-fig11.zip /home/macu/data  True\n",
      "['Insect_one_million.mat']\n",
      "['Insect_one_million.mat'] extracted to /home/macu/data\n",
      "--> Unzip_mat one /home/macu/data/InsectData-fig11.zip /home/macu/data  True\n",
      "['Insect_one_million.mat']\n",
      "Insect_one_million.mat extracted to  /home/macu/data\n",
      "--> Unzip_mat one /home/macu/data/InsectData-fig11.zip /home/macu/data Insect_one_million True\n",
      "['Insect_one_million.mat']\n",
      "Insect_one_million.mat extracted to /home/macu/data\n",
      "--> Unzip_mat one /home/macu/data/InsectData-fig11.zip /home/macu/data Insect_one_millione True\n",
      "['Insect_one_million.mat']\n",
      "NoneInsect_one_millione.mat found.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Downloading insects (by Eamonn Keogh)\n",
    "file_id = '1qq1z2mVRd7PzDqX0TDAwY7BcWVjnXUfQ'\n",
    "data_path = os.path.expanduser('~/data')\n",
    "destination = os.path.join(data_path, 'InsectData-fig11.zip')\n",
    "\n",
    "# Llamada a la funciÃ³n\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "zip_contents(destination)\n",
    "print(unzip_mat('all', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_million'))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_millione'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "def mat2csv(mat_file_path, csv_file_folder = '~/data/', print_flag=False):\n",
    "    # Carga el archivo .mat, omitiendo las variables meta de MATLAB\n",
    "    mat = scipy.io.loadmat(mat_file_path, squeeze_me=True, struct_as_record=False)\n",
    "    \n",
    "    # Itera sobre todas las variables encontradas en el archivo .mat\n",
    "    for variable_name, data in mat.items():\n",
    "        if variable_name.startswith('__') or isinstance(data, scipy.io.matlab.mio5_params.mat_struct):\n",
    "            # Omite variables meta de MATLAB o estructuras (que requieren un manejo especial)\n",
    "            continue\n",
    "        \n",
    "        # Convierte la data a un DataFrame de pandas, manejando diferentes tipos de datos\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:  # Es un ndarray estructurado\n",
    "                data_df = pd.DataFrame(data)\n",
    "            else:  # Es un ndarray regular\n",
    "                data_df = pd.DataFrame(data, columns=[variable_name])\n",
    "        else:\n",
    "            # Para otros tipos de datos, los convertimos en un DataFrame simple\n",
    "            data_df = pd.DataFrame([data], columns=[variable_name])\n",
    "        \n",
    "        # Define la ruta del archivo .csv de salida\n",
    "        csv_file_path = csv_file_folder+ variable_name + '.csv'\n",
    "        \n",
    "        # Guarda el DataFrame como un archivo .csv\n",
    "        data_df.to_csv(csv_file_path, index=False)\n",
    "        if print_flag:\n",
    "            print(data_df.shape)\n",
    "            display(data_df.head(5))\n",
    "            print(f\"Matlab matrix '{variable_name}' converted to CSV in: {csv_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109842, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76857/324087631.py:10: DeprecationWarning: Please import `mat_struct` from the `scipy.io.matlab` namespace; the `scipy.io.matlab.mio5_params` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  if variable_name.startswith('__') or isinstance(data, scipy.io.matlab.mio5_params.mat_struct):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penguin_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.259033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.269287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.271240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   penguin_sample\n",
       "0        0.253906\n",
       "1        0.259033\n",
       "2        0.269287\n",
       "3        0.271240\n",
       "4        0.265137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matlab matrix 'penguin_sample' converted to CSV in: ~/data/penguin_sample.csv\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "path = '/home/macu/data/MP_first_test_penguin_sample.mat'\n",
    "mat2csv(path, print_flag = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from nbdev.export import *\n",
    "#notebook2script()\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
