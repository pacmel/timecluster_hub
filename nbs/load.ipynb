{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> Methods for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "from dvats.imports import *\n",
    "from dvats.utils import *\n",
    "import pickle\n",
    "import pyarrow.feather as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.imports import beep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/macu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = Path.home()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series artifacts (to be used with weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is meant to extend `wandb.Artifact` for logging/using files with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSArtifact(wandb.Artifact):\n",
    "\n",
    "    default_storage_path = Path(Path.home()/'data/wandb_artifacts/')\n",
    "    date_format = '%Y-%m-%d %H:%M:%S' # TODO add milliseconds\n",
    "    handle_missing_values_techniques = {\n",
    "        'linear_interpolation': lambda df : df.interpolate(method='linear', limit_direction='both'),\n",
    "        'overall_mean': lambda df : df.fillna(df.mean()),\n",
    "        'overall_median': lambda df : df.fillna(df.median()),\n",
    "        'backward_fill' : lambda df : df.fillna(method='bfill'),\n",
    "        'forward_fill' : lambda df : df.fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "    \"Class that represents a wandb artifact containing time series data. sd stands for start_date \\\n",
    "    and ed for end_date. Both should be pd.Timestamps\"\n",
    "\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, name, sd:pd.Timestamp, ed:pd.Timestamp, **kwargs):\n",
    "        super().__init__(type='dataset', name=name, **kwargs)\n",
    "        self.sd = sd\n",
    "        self.ed = ed\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['TS'] = dict(sd = self.sd.strftime(self.date_format),\n",
    "                                   ed = self.ed.strftime(self.date_format))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_daily_csv_files(cls, root_path, fread=pd.read_csv, start_date=None, end_date=None, metadata=None, **kwargs):\n",
    "\n",
    "        \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "        to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "        defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "        artifact name, and the date range as version.\"\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(__init__)\n",
    "    def from_df(cls, df:pd.DataFrame, name:str, path:str=None, sd:pd.Timestamp=None, ed:pd.Timestamp=None,\n",
    "                normalize:bool=False, missing_values_technique:str=None, resampling_freq:str=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Create a TSArtifact of type `dataset`, using the DataFrame `df` samples from \\\n",
    "        `sd` (start date) to `ed` (end date). Dates must be passed as `datetime.datetime` \\\n",
    "        objects. The transformed DataFrame is stored as a pickle file in the path `path` \\\n",
    "        and its reference is added to the artifact entries. Additionally, the dataset can \\\n",
    "        be normalized (see `normalize` argument) or transformed using missing values \\\n",
    "        handling techniques (see `missing_values_technique` argument) or resampling (see \\\n",
    "        `resampling_freq` argument).\n",
    "\n",
    "        Arguments:\n",
    "            df: (DataFrame) The dataframe you want to convert into an artifact.\n",
    "            name: (str) The artifact name.\n",
    "            path: (str, optional) The path where the file, containing the new transformed \\\n",
    "                dataframe, is saved. Default None.\n",
    "            sd: (sd, optional) Start date. By default, the first index of `df` is taken.\n",
    "            ed: (ed, optional) End date. By default, the last index of `df` is taken.\n",
    "            normalize: (bool, optional) If the dataset values should be normalized. Default\\\n",
    "                False.\n",
    "            missing_values_technique: (str, optional) The technique used to handle missing \\\n",
    "                values. Options: \"linear_iterpolation\", \"overall_mean\", \"overall_median\" or \\\n",
    "                None. Default None.\n",
    "            resampling_freq: (str, optional) The offset string or object representing \\\n",
    "                frequency conversion for time series resampling. Default None.\n",
    "\n",
    "        Returns:\n",
    "            TSArtifact object.\n",
    "        \"\"\"\n",
    "        sd = df.index[0] if sd is None else sd\n",
    "        ed = df.index[-1] if ed is None else ed\n",
    "        obj = cls(name, sd=sd, ed=ed, **kwargs)\n",
    "        df = df.query('@obj.sd <= index <= @obj.ed')\n",
    "        obj.metadata['TS']['created'] = 'from-df'\n",
    "        obj.metadata['TS']['n_vars'] = df.columns.__len__()\n",
    "\n",
    "        # Handle Missing Values\n",
    "        df = obj.handle_missing_values_techniques[missing_values_technique](df) if missing_values_technique is not None else df\n",
    "        obj.metadata['TS']['handle_missing_values_technique'] = missing_values_technique.__str__()\n",
    "        obj.metadata['TS']['has_missing_values'] = np.any(df.isna().values).__str__()\n",
    "\n",
    "        # Indexing and Resampling\n",
    "        if resampling_freq: df = df.resample(resampling_freq).mean()\n",
    "        obj.metadata['TS']['n_samples'] = len(df)\n",
    "        obj.metadata['TS']['freq'] = str(df.index.freq)\n",
    "\n",
    "        # Time Series Variables\n",
    "        obj.metadata['TS']['vars'] = list(df.columns)\n",
    "\n",
    "        # Normalization - Save the previous means and stds\n",
    "        if normalize:\n",
    "            obj.metadata['TS']['normalization'] = dict(means = df.describe().loc['mean'].to_dict(),\n",
    "                                                       stds = df.describe().loc['std'].to_dict())\n",
    "            df = normalize_columns(df)\n",
    "\n",
    "        # Hash and save\n",
    "        hash_code = str(pd.util.hash_pandas_object(df).sum()) #Â str(hash(df.values.tobytes()))\n",
    "        path = obj.default_storage_path/f'{hash_code}' if path is None else Path(path)/f'{hash_code}'\n",
    "        print(\"About to write df to \", path)\n",
    "        ft.write_feather(df, path, compression = 'lz4')\n",
    "        #feather.write_dataframe\n",
    "        obj.metadata['TS']['hash'] = hash_code\n",
    "        obj.add_file(str(path))\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to write df to  /home/macu/data/wandb_artifacts/7736998280633728827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TS': {'sd': '2021-01-01 00:00:00',\n",
       "  'ed': '2021-01-01 00:00:29',\n",
       "  'created': 'from-df',\n",
       "  'n_vars': 4,\n",
       "  'handle_missing_values_technique': 'overall_median',\n",
       "  'has_missing_values': 'False',\n",
       "  'n_samples': 6,\n",
       "  'freq': '<5 * Seconds>',\n",
       "  'vars': ['A', 'B', 'C', 'D'],\n",
       "  'normalization': {'means': {'A': -0.029637609663132728,\n",
       "    'B': -0.40026973314124753,\n",
       "    'C': -0.02134916042814836,\n",
       "    'D': 0.04137270661806195},\n",
       "   'stds': {'A': 0.39932602098066644,\n",
       "    'B': 0.34830011744118783,\n",
       "    'C': 0.36764279679757034,\n",
       "    'D': 0.25373970381817534}},\n",
       "  'hash': '7736998280633728827'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TSArtifact class TEST\n",
    "\n",
    "# resampling frequency\n",
    "resampling_freq = '5s'\n",
    "# handle missing values technique\n",
    "missing_values_technique='overall_median'\n",
    "\n",
    "# testing dataframe\n",
    "df_test = pd.util.testing.makeMissingDataframe()\n",
    "df_test.index = pd.date_range(start='2021-01-01', periods=len(df_test), freq='s')\n",
    "\n",
    "artifact = TSArtifact.from_df(df_test, \n",
    "                              name='JNK', \n",
    "                              missing_values_technique=missing_values_technique,\n",
    "                              resampling_freq=resampling_freq, \n",
    "                              normalize=True)\n",
    "artifact.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/wandb_artifacts/7736998280633728827\n",
      "                            A         B         C         D\n",
      "2021-01-01 00:00:00  0.382090  1.138692  0.910023  0.048650\n",
      "2021-01-01 00:00:05 -1.609279 -0.059981 -1.187677 -0.445947\n",
      "2021-01-01 00:00:10 -0.785577 -1.696949  1.317957 -0.902181\n",
      "2021-01-01 00:00:15  1.083399  0.066801 -0.017103  1.428413\n",
      "2021-01-01 00:00:20  0.320671 -0.300320 -1.011853  0.919442\n",
      "2021-01-01 00:00:25  0.608696  0.851757 -0.011347 -1.048376\n"
     ]
    }
   ],
   "source": [
    "hash = artifact.metadata['TS']['hash']\n",
    "path = \"../../data/wandb_artifacts/\"+hash\n",
    "print(path)\n",
    "f = ft.read_feather(path)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ft.read_feather(\"/home/macu/data/wandb_artifacts/-2535364569820284064\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a `wandb.apis.public.Artifact` object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original `TSArtifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_df(self:wandb.apis.public.Artifact):\n",
    "    \"Download the files of a saved wandb artifact and process them as a single dataframe. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\n",
    "    # The way we have to ensure that the argument comes from a TS arfitact is the metadata\n",
    "    if self.metadata.get('TS') is None:\n",
    "        print(f'ERROR:{self} does not come from a logged TSArtifact')\n",
    "        return None\n",
    "    dir = Path(self.download())\n",
    "    if self.metadata['TS']['created'] == 'from-df':\n",
    "        # Call read_pickle with the single file from dir\n",
    "        #return pd.read_pickle(dir.ls()[0])\n",
    "        return ft.read_feather(dir.ls()[0])\n",
    "    else:\n",
    "        print(\"ERROR: Only from_df method is allowed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can write a method to cast a downloaded wandb artifact (instance from `wandb.apis.public,Artifact`) to a TSArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_tsartifact(self:wandb.apis.public.Artifact):\n",
    "    \"Cast an artifact as a TS artifact. The artifact must have been created from one of the \\\n",
    "    class creation methods of the class `TSArtifact`. This is useful to go back to a TSArtifact \\\n",
    "    after downloading an artifact through the wand API\"\n",
    "    return TSArtifact(name=self.digest, #TODO change this\n",
    "                      sd=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      ed=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      description=self.description,\n",
    "                      metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject or infer frequencies in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(pd.to_datetime)\n",
    "def infer_or_inject_freq(df, injected_freq='1s', start_date=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Infer index frequency. If there's not a proper time index, create fake timestamps,\n",
    "        keeping the desired `injected_freq`. If that is None, set a default one of 1 second.\n",
    "        start_date: the first date of the index (int or string).\n",
    "    \"\"\"\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    if inferred_freq == 'N':\n",
    "        timedelta = pd.to_timedelta(injected_freq)\n",
    "        df.index = pd.to_datetime(ifnone(start_date, 0), **kwargs) + timedelta*df.index\n",
    "        df.index.freq = pd.infer_freq(df.index)\n",
    "    else:\n",
    "        df.index.freq = inferred_freq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     0\n",
       " 1970-01-01 00:00:00  1\n",
       " 1970-01-01 00:00:01  2\n",
       " 1970-01-01 00:00:02  3,\n",
       "                      0\n",
       " 1970-01-01 00:00:00  1\n",
       " 1970-01-01 00:00:02  2\n",
       " 1970-01-01 00:00:04  3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = pd.DataFrame([1, 2, 3])\n",
    "foo = infer_or_inject_freq(foo)\n",
    "bar = infer_or_inject_freq(bar, injected_freq='2s')\n",
    "test_eq(foo.index.freq, '1s')\n",
    "test_eq(bar.index.freq, '2s')\n",
    "foo, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = infer_or_inject_freq(foo, injected_freq='1W', start_date='01/01/2020')\n",
    "baz = infer_or_inject_freq(foo, injected_freq='1W', start_date='2020-01-01', format = '%Y-%m-%d')\n",
    "test_eq(bar, baz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "#from nbdev.export import *\n",
    "#notebook2script()\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
