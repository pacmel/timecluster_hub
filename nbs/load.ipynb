{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> Methods for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "from dvats.imports import *\n",
    "from dvats.utils import *\n",
    "import pickle\n",
    "import pyarrow.feather as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.imports import beep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series artifacts (to be used with weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is meant to extend `wandb.Artifact` for logging/using files with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSArtifact(wandb.Artifact):\n",
    "\n",
    "    default_storage_path = Path(Path.home()/'data/wandb_artifacts/')\n",
    "    date_format = '%Y-%m-%d %H:%M:%S' # TODO add milliseconds\n",
    "    handle_missing_values_techniques = {\n",
    "        'linear_interpolation': lambda df : df.interpolate(method='linear', limit_direction='both'),\n",
    "        'overall_mean': lambda df : df.fillna(df.mean()),\n",
    "        'overall_median': lambda df : df.fillna(df.median()),\n",
    "        'backward_fill' : lambda df : df.fillna(method='bfill'),\n",
    "        'forward_fill' : lambda df : df.fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "    \"Class that represents a wandb artifact containing time series data. sd stands for start_date \\\n",
    "    and ed for end_date. Both should be pd.Timestamps\"\n",
    "\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, name, sd:pd.Timestamp, ed:pd.Timestamp, **kwargs):\n",
    "        super().__init__(type='dataset', name=name, **kwargs)\n",
    "        self.sd = sd\n",
    "        self.ed = ed\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['TS'] = dict(sd = self.sd.strftime(self.date_format),\n",
    "                                   ed = self.ed.strftime(self.date_format))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_daily_csv_files(cls, root_path, fread=pd.read_csv, start_date=None, end_date=None, metadata=None, **kwargs):\n",
    "\n",
    "        \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "        to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "        defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "        artifact name, and the date range as version.\"\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(__init__)\n",
    "    def from_df(cls, df:pd.DataFrame, name:str, path:str=None, sd:pd.Timestamp=None, ed:pd.Timestamp=None,\n",
    "                normalize:bool=False, missing_values_technique:str=None, resampling_freq:str=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Create a TSArtifact of type `dataset`, using the DataFrame `df` samples from \\\n",
    "        `sd` (start date) to `ed` (end date). Dates must be passed as `datetime.datetime` \\\n",
    "        objects. The transformed DataFrame is stored as a pickle file in the path `path` \\\n",
    "        and its reference is added to the artifact entries. Additionally, the dataset can \\\n",
    "        be normalized (see `normalize` argument) or transformed using missing values \\\n",
    "        handling techniques (see `missing_values_technique` argument) or resampling (see \\\n",
    "        `resampling_freq` argument).\n",
    "\n",
    "        Arguments:\n",
    "            df: (DataFrame) The dataframe you want to convert into an artifact.\n",
    "            name: (str) The artifact name.\n",
    "            path: (str, optional) The path where the file, containing the new transformed \\\n",
    "                dataframe, is saved. Default None.\n",
    "            sd: (sd, optional) Start date. By default, the first index of `df` is taken.\n",
    "            ed: (ed, optional) End date. By default, the last index of `df` is taken.\n",
    "            normalize: (bool, optional) If the dataset values should be normalized. Default\\\n",
    "                False.\n",
    "            missing_values_technique: (str, optional) The technique used to handle missing \\\n",
    "                values. Options: \"linear_iterpolation\", \"overall_mean\", \"overall_median\" or \\\n",
    "                None. Default None.\n",
    "            resampling_freq: (str, optional) The offset string or object representing \\\n",
    "                frequency conversion for time series resampling. Default None.\n",
    "\n",
    "        Returns:\n",
    "            TSArtifact object.\n",
    "        \"\"\"\n",
    "        sd = df.index[0] if sd is None else sd\n",
    "        ed = df.index[-1] if ed is None else ed\n",
    "        obj = cls(name, sd=sd, ed=ed, **kwargs)\n",
    "        df = df.query('@obj.sd <= index <= @obj.ed')\n",
    "        obj.metadata['TS']['created'] = 'from-df'\n",
    "        obj.metadata['TS']['n_vars'] = df.columns.__len__()\n",
    "\n",
    "        # Handle Missing Values\n",
    "        df = obj.handle_missing_values_techniques[missing_values_technique](df) if missing_values_technique is not None else df\n",
    "        obj.metadata['TS']['handle_missing_values_technique'] = missing_values_technique.__str__()\n",
    "        obj.metadata['TS']['has_missing_values'] = np.any(df.isna().values).__str__()\n",
    "\n",
    "        # Indexing and Resampling\n",
    "        if resampling_freq: df = df.resample(resampling_freq).mean()\n",
    "        obj.metadata['TS']['n_samples'] = len(df)\n",
    "        obj.metadata['TS']['freq'] = str(df.index.freq)\n",
    "\n",
    "        # Time Series Variables\n",
    "        obj.metadata['TS']['vars'] = list(df.columns)\n",
    "\n",
    "        # Normalization - Save the previous means and stds\n",
    "        if normalize:\n",
    "            obj.metadata['TS']['normalization'] = dict(means = df.describe().loc['mean'].to_dict(),\n",
    "                                                       stds = df.describe().loc['std'].to_dict())\n",
    "            df = normalize_columns(df)\n",
    "\n",
    "        # Hash and save\n",
    "        hash_code = str(pd.util.hash_pandas_object(df).sum()) #Â str(hash(df.values.tobytes()))\n",
    "        path = obj.default_storage_path/f'{hash_code}' if path is None else Path(path)/f'{hash_code}'\n",
    "        print(\"About to write df to \", path)\n",
    "        ft.write_feather(df, path, compression = 'lz4')\n",
    "        #feather.write_dataframe\n",
    "        obj.metadata['TS']['hash'] = hash_code\n",
    "        obj.add_file(str(path))\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSArtifact class TEST\n",
    "\n",
    "# resampling frequency\n",
    "resampling_freq = '5s'\n",
    "# handle missing values technique\n",
    "missing_values_technique='overall_median'\n",
    "\n",
    "# testing dataframe\n",
    "df_test = pd.util.testing.makeMissingDataframe()\n",
    "df_test.index = pd.date_range(start='2021-01-01', periods=len(df_test), freq='s')\n",
    "\n",
    "artifact = TSArtifact.from_df(df_test, \n",
    "                              name='JNK', \n",
    "                              missing_values_technique=missing_values_technique,\n",
    "                              resampling_freq=resampling_freq, \n",
    "                              normalize=True)\n",
    "artifact.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash = artifact.metadata['TS']['hash']\n",
    "path = \"../../data/wandb_artifacts/\"+hash\n",
    "print(path)\n",
    "f = ft.read_feather(path)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ft.read_feather(\"/home/macu/data/wandb_artifacts/-2535364569820284064\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a `wandb.apis.public.Artifact` object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original `TSArtifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_df(self:wandb.apis.public.Artifact):\n",
    "    \"Download the files of a saved wandb artifact and process them as a single dataframe. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\n",
    "    # The way we have to ensure that the argument comes from a TS arfitact is the metadata\n",
    "    if self.metadata.get('TS') is None:\n",
    "        print(f'ERROR:{self} does not come from a logged TSArtifact')\n",
    "        return None\n",
    "    dir = Path(self.download())\n",
    "    if self.metadata['TS']['created'] == 'from-df':\n",
    "        # Call read_pickle with the single file from dir\n",
    "        #return pd.read_pickle(dir.ls()[0])\n",
    "        return ft.read_feather(dir.ls()[0])\n",
    "    else:\n",
    "        print(\"ERROR: Only from_df method is allowed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can write a method to cast a downloaded wandb artifact (instance from `wandb.apis.public,Artifact`) to a TSArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_tsartifact(self:wandb.apis.public.Artifact):\n",
    "    \"Cast an artifact as a TS artifact. The artifact must have been created from one of the \\\n",
    "    class creation methods of the class `TSArtifact`. This is useful to go back to a TSArtifact \\\n",
    "    after downloading an artifact through the wand API\"\n",
    "    return TSArtifact(name=self.digest, #TODO change this\n",
    "                      sd=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      ed=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      description=self.description,\n",
    "                      metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject or infer frequencies in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(pd.to_datetime)\n",
    "def infer_or_inject_freq(df, injected_freq='1s', start_date=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Infer index frequency. If there's not a proper time index, create fake timestamps,\n",
    "        keeping the desired `injected_freq`. If that is None, set a default one of 1 second.\n",
    "        start_date: the first date of the index (int or string).\n",
    "    \"\"\"\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    if inferred_freq == 'N':\n",
    "        timedelta = pd.to_timedelta(injected_freq)\n",
    "        df.index = pd.to_datetime(ifnone(start_date, 0), **kwargs) + timedelta*df.index\n",
    "        df.index.freq = pd.infer_freq(df.index)\n",
    "    else:\n",
    "        df.index.freq = inferred_freq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = pd.DataFrame([1, 2, 3])\n",
    "foo = infer_or_inject_freq(foo)\n",
    "bar = infer_or_inject_freq(bar, injected_freq='2s')\n",
    "test_eq(foo.index.freq, '1s')\n",
    "test_eq(bar.index.freq, '2s')\n",
    "foo, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = infer_or_inject_freq(foo, injected_freq='1W', start_date='01/01/2020')\n",
    "baz = infer_or_inject_freq(foo, injected_freq='1W', start_date='2020-01-01', format = '%Y-%m-%d')\n",
    "test_eq(bar, baz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from nbdev.export import *\n",
    "#notebook2script()\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
