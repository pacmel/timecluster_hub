{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa093821-a793-49fd-a8c2-32cacdbdc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6adb50-be0c-4f0b-9beb-38d4e78c5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%load_ext autoreload --> Not working TODO:REVISAR\n",
    "# %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bedfb7-6a74-4f6c-a769-cc79fe37686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dvats.memory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11f72ce-0e76-4e2a-9394-1c4c632b5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Classes & types\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a4a02-5015-4a3e-9a64-6089de4e9532",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "> Architectures and functions for creating encoders that create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b014a18f-8538-4ed7-98ee-ddb165692553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from tsai.callback.MVP import *\n",
    "from tsai.imports import *\n",
    "from tsai.models.InceptionTimePlus import InceptionTimePlus\n",
    "from tsai.models.explainability import get_acts_and_grads\n",
    "from tsai.models.layers import *\n",
    "from tsai.data.validation import combine_split_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c94f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04637a46",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c036898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class DCAE_torch(Module):\n",
    "    def __init__(self, c_in, seq_len, delta, nfs=[64, 32, 12], kss=[10, 5, 5],\n",
    "                 pool_szs=[2,2,3], output_fsz=10):\n",
    "        \"\"\"\n",
    "        Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions,\n",
    "        sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be\n",
    "        contained in the Dense layer of the network. The the number of features\n",
    "        maps (filters), the filter size and the pool size can also be adjusted.\"\n",
    "        \"\"\"\n",
    "        assert all_equal([len(x) for x in [nfs, kss, pool_szs]], np.repeat(len(nfs), 3)), \\\n",
    "            'nfs, kss, and pool_szs must have the same length'\n",
    "        assert np.prod(pool_szs) == nfs[-1], \\\n",
    "            'The number of filters in the last conv layer must be equal to the product of pool sizes'\n",
    "        assert seq_len % np.prod(pool_szs) == 0, \\\n",
    "            'The product of pool sizes must be a divisor of the window size'\n",
    "        layers = []\n",
    "        for i in range_of(kss):\n",
    "            layers += [Conv1d(ni=nfs[i-1] if i>0 else c_in, nf=nfs[i], ks=kss[i]),\n",
    "                       nn.MaxPool1d(kernel_size=pool_szs[i])]\n",
    "        self.downsample = nn.Sequential(*layers)\n",
    "        self.bottleneck = nn.Sequential(OrderedDict([\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('latent_in', nn.Linear(seq_len, delta)),\n",
    "            ('latent_out', nn.Linear(delta, seq_len)),\n",
    "            ('reshape', Reshape(nfs[-1], seq_len // np.prod(pool_szs)))\n",
    "        ]))\n",
    "        layers = []\n",
    "        for i in reversed(range_of(kss)):\n",
    "            layers += [Conv1d(ni=nfs[i+1] if i != (len(nfs)-1) else nfs[-1],\n",
    "                              nf=nfs[i], ks=kss[i]),\n",
    "                       nn.Upsample(scale_factor=pool_szs[i])]\n",
    "        layers += [Conv1d(ni=nfs[0], nf=c_in, kernel_size=output_fsz)]\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59aa6e7-36df-4697-993e-60f737bb0f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 48])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = torch.rand(3, 1, 48)\n",
    "m = DCAE_torch(c_in=foo.shape[1], seq_len=foo.shape[2], delta=12)\n",
    "m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a4a1d-389c-481a-a54d-3f86cd2115f5",
   "metadata": {},
   "source": [
    "### Dictionary to get the default backbone modules to get the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb9f1b6-ae45-4b6e-b535-c7f121208721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ENCODER_EMBS_MODULE_NAME = {\n",
    "    InceptionTimePlus: 'backbone', # for mvp based models\n",
    "    DCAE_torch: 'bottleneck.latent_in'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a268f-8b4e-4432-b203-79263a247c4c",
   "metadata": {},
   "source": [
    "### Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838da6cc-8fa7-4273-bf7a-20475574bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.learner import Learner\n",
    "from tsai.data.core import TSDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790db11-4e3c-4940-80e7-e4a4c7d1d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_ensure_batch_size_(\n",
    "    dls        : TSDataLoaders,\n",
    "    batch_size : int = None,\n",
    "    verbose    : int = 0\n",
    ") -> None:\n",
    "    if batch_size is None:\n",
    "        if verbose > 1: \n",
    "            print(\"[ Get Encoder Embeddings Ensure Batch Size ] No batch size proposed\")\n",
    "        if dls.bs == 0: \n",
    "            if verbose > 1: \n",
    "                print(\"[ Get Encoder Embeddings Ensure Batch Size ] Using value 64 as 0 is not a valid value.\")\n",
    "            enc_learn.dls.bs = 64\n",
    "        elif verbose > 1: \n",
    "            print(f\"[ Get Encoder Embeddings Ensure Batch Size ] Using the original value: {dls.bs}\")\n",
    "    else:\n",
    "        dls.bs = batch_size\n",
    "        if verbose > 1: \n",
    "            print(f\"[ Get Encoder Embeddings Ensure Batch Size ] Batch size proposed. Using {dls.bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "838da6cc-8fa7-4273-bf7a-20475574bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.learner import Learner\n",
    "from tsai.data.core import TSDataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f790db11-4e3c-4940-80e7-e4a4c7d1d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs_ensure_batch_size_(\n",
    "    dls        : TSDataLoaders,\n",
    "    batch_size : int = None,\n",
    "    verbose    : int = 0\n",
    ") -> None:\n",
    "    if batch_size is None:\n",
    "        if verbose > 1: \n",
    "            print(\"[ Get Encoder Embeddings Ensure Batch Size ] No batch size proposed\")\n",
    "        if dls.bs == 0: \n",
    "            if verbose > 1: \n",
    "                print(\"[ Get Encoder Embeddings Ensure Batch Size ] Using value 64 as 0 is not a valid value.\")\n",
    "            enc_learn.dls.bs = 64\n",
    "        elif verbose > 1: \n",
    "            print(f\"[ Get Encoder Embeddings Ensure Batch Size ] Using the original value: {dls.bs}\")\n",
    "    else:\n",
    "        dls.bs = batch_size\n",
    "        if verbose > 1: \n",
    "            print(f\"[ Get Encoder Embeddings Ensure Batch Size ] Batch size proposed. Using {dls.bs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65c66ae6-3178-49dc-bd16-64c082012e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_enc_embs(\n",
    "    X               : List [ List [ List [ float ] ] ], \n",
    "    enc_learn       : Learner, \n",
    "    module          : str  = None, \n",
    "    cpu             : bool = False, \n",
    "    average_seq_dim : bool = True, \n",
    "    to_numpy        : bool = True,\n",
    "    batch_size      : int  = None,\n",
    "    verbose         : int  = 0\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `batch_size`: force data loader to use the input batch size\n",
    "        - `verbose`: print flag. More big, more information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cpu:\n",
    "        if verbose > 0: print(\"[ Get Encoder Embeddings ] CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if verbose > 0: print(\"[ Get Encoder Embeddings ] --> GPU\")\n",
    "        if verbose > 1: print(\"[ Get Encoder Embeddings ] GPU | Ensure empty cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "        if verbose > 1: print(\"[ Get Encoder Embeddings ] GPU | Move & exec into CUDA\")\n",
    "        enc_learn.dls.cuda()\n",
    "        enc_learn.cuda()\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 1: \n",
    "                print(\"[ Get Encoder Embeddings ] GPU | CUDA is available\")\n",
    "                print(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device id {torch.cuda.current_device()}\")\n",
    "                print(f\"[ Get Encoder Embeddings ] GPU | CUDA is available | current device name {torch.cuda.get_device_name(torch.cuda.current_device())}\")            \n",
    "        else:\n",
    "            if verbose > 1: print(\"[ Get Encoder Embeddings ] GPU | CUDA is not available\")\n",
    "        if verbose > 0: print(\"[ Get Encoder Embeddings ] GPU -->\")\n",
    "\n",
    "    #if verbose > 0: print(\"[ Get Encoder Embeddings ] Ensure the correct batch size\")\n",
    "    #get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] Set dataloader from X (enc_learn does not contain dls)\")\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    get_enc_embs_ensure_batch_size_(aux_dl, batch_size, verbose)\n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] Get module\")\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] get_acts_and_grads \")\n",
    "    if verbose > 1: print(f\"[ Get Encoder Embeddings ] get_acts_and_grads bs = {aux_dl.bs}\")\n",
    "    \n",
    "    embs = [\n",
    "        get_acts_and_grads(\n",
    "            model   = enc_learn.model,\n",
    "            modules = module,\n",
    "            x       = xb[0], \n",
    "            cpu     = cpu\n",
    "        )[0] \n",
    "        for xb in aux_dl\n",
    "    ]\n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | --> Concat\")\n",
    "    if not cpu:\n",
    "        if verbose > 1: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory\")\n",
    "        total_emb_size = sum([emb.element_size() * emb.nelement() for emb in embs])\n",
    "        free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "        if (total_emb_size < free_memory):\n",
    "            if verbose > 1: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Fits in GPU -> Computing in GPU\")\n",
    "            embs=[emb.cuda() for emb in embs]\n",
    "        else:\n",
    "            if verbose > 1: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | Check neccesary & free memory | Does not fit in GPU -> Computing in CPU\")\n",
    "            embs=[emb.cpu() for emb in embs]\n",
    "    if verbose > 1: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat | to_concat\")\n",
    "    embs = to_concat(embs)\n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] get_acts_and_grads | Concat -->\")\n",
    "    \n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] Reduce to 2 dimensions.\")\n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    if verbose > 0: print(\"[ Get Encoder Embeddings ] Ensure CPU saving & numpy format\")\n",
    "    if to_numpy: embs = embs.numpy() if cpu else embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ad3b28-43c0-4df3-be57-3eecb76b17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_enc_embs_set_stride_set_batch_size(\n",
    "    X                  : List [ List [ List [ float ] ] ], \n",
    "    enc_learn          : Learner, \n",
    "    stride             : int, \n",
    "    batch_size         : int, \n",
    "    module             : str  = None, \n",
    "    cpu                : bool = False, \n",
    "    average_seq_dim    : bool = True, \n",
    "    to_numpy           : bool = True, \n",
    "    verbose            : int  = 0, \n",
    "    time_flag          : bool = False, \n",
    "    chunk_size         : int  = 0, \n",
    "    check_memory_usage : bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `X`: encoder input\n",
    "        - `enc_learn`: trained encoder\n",
    "        - `stride`: stride used for the training. Neccesary for adjusting the encoder input\n",
    "        - `batch_size`: value to force the dataloader to use.\n",
    "        - `module`: for geting the embeddings of an specific layer.\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "        - `verbose`: For printing messages. More big, more messages.\n",
    "        - `time_flag`: To take note of the execution time required by this function\n",
    "        - `chunk_size`: For spliting the embedings reading in batches of `chunk_size` size.\n",
    "        - `check_memory_usage`: For showing messages of the current state of the memory.\n",
    "    \"\"\"\n",
    "    if time_flag:\n",
    "        t_start = time.time()\n",
    "    if verbose > 0:\n",
    "        print(\"--> get_enc_embs_set_stride_set_batch_size\")\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    X = X[::stride]\n",
    "    enc_learn.dls.bs = batch_size \n",
    "\n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "    \n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Check CUDA | X ~ \", X.shape[0])\n",
    "    if cpu:\n",
    "        if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Get enc embs CPU\")\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            if verbose > 0: \n",
    "                print(\"get_enc_embs_set_stride_set_batch_size | CUDA device id:\", torch.cuda.current_device())\n",
    "                print(\"get_enc_embs_set_stride_set_batch_size | CUDA device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "                print(\"get_enc_embs_set_stride_set_batch_size | Ensure empty cache & move 2 GPU\")\n",
    "            torch.cuda.empty_cache()\n",
    "            enc_learn.dls.cuda()\n",
    "            enc_learn.cuda()\n",
    "        else:\n",
    "            if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | No cuda available. Set CPU = true\")\n",
    "            cpu = True\n",
    "            \n",
    "    get_enc_embs_ensure_batch_size_(enc_learn.dls, batch_size, verbose)\n",
    "\n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Set dataset from X (enc_learn does not contain dls)\")\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    aux_dl.bs = enc_learn.dls.bs if enc_learn.dls.bs>0 else 64\n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Get module\")\n",
    "    module = nested_attr(enc_learn.model,ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) if module is None else module\n",
    "    \n",
    "    if verbose > 0: \n",
    "        #print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | module \", module)\n",
    "        print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl len\", len(aux_dl))\n",
    "        print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl.batch_len \", len(next(iter(aux_dl))))\n",
    "        print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl.bs \", aux_dl.bs)\n",
    "        if (not cpu):\n",
    "            total = torch.cuda.get_device_properties(device).total_memory\n",
    "            used = torch.cuda.memory_allocated(torch.cuda.current_device())\n",
    "            reserved = torch.cuda.memory_reserved(torch.cuda.current_device())\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | total_mem \", total)\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | used_mem \", used)\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | reserved_mem \", reserved)\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | available_mem \", total-reserved)\n",
    "            sys.stdout.flush()\n",
    "                                              \n",
    "    if (cpu or ( chunk_size == 0 )):\n",
    "        embs = [\n",
    "            get_acts_and_grads(\n",
    "                model=enc_learn.model,\n",
    "                modules=module, \n",
    "                x=xb[0], \n",
    "                cpu=cpu\n",
    "            )[0] \n",
    "            for xb in aux_dl\n",
    "        ]\n",
    "        if not cpu: embs=[emb.cpu() for emb in embs]\n",
    "    else:\n",
    "        embs = []\n",
    "        total_chunks=max(1,round(len(X)/chunk_size))\n",
    "        if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | aux_dl len | \" + str(len(X)) + \" chunk size: \" + str(chunk_size) + \" => \" + str(total_chunks) + \" chunks\")\n",
    "        for i in range(0, total_chunks):\n",
    "            if verbose > 0: \n",
    "                print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | Chunk [ \" + str(i) + \"/\"+str(total_chunks)+\"] => \" + str(round(i*100/total_chunks)) + \"%\")\n",
    "                sys.stdout.flush()\n",
    "            chunk = [batch for (n, batch) in enumerate(aux_dl) if (chunk_size*i <= n  and chunk_size*(i+1) > n) ]\n",
    "            chunk_embs = [\n",
    "                get_acts_and_grads(\n",
    "                    model=enc_learn.model,\n",
    "                    modules=module,\n",
    "                    x=xb[0], \n",
    "                    cpu=cpu\n",
    "                )[0]\n",
    "                for xb in chunk\n",
    "            ]\n",
    "            # Mueve los embeddings del bloque a la CPU\n",
    "            chunk_embs = [emb.cpu() for emb in chunk_embs]\n",
    "            embs.extend(chunk_embs)\n",
    "            torch.cuda.empty_cache()\n",
    "        if verbose > 0: \n",
    "            print(\"get_enc_embs_set_stride_set_batch_size | Get acts and grads | 100%\")\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | concat embeddings\")\n",
    "    \n",
    "    embs = to_concat(embs)\n",
    "    \n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Reduce\")\n",
    "    \n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    \n",
    "    if verbose > 0: print(\"get_enc_embs_set_stride_set_batch_size | Convert to numpy\")\n",
    "    \n",
    "    if to_numpy: \n",
    "        if cpu or chunk_size > 0:\n",
    "            embs = embs.numpy() \n",
    "        else: \n",
    "            embs = embs.cpu().numpy()\n",
    "            torch.cuda.empty_cache()\n",
    "    if time_flag:\n",
    "        t = time.time()-t_start\n",
    "        if verbose > 0:\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size \" + str(t) + \" seconds -->\")\n",
    "        else:\n",
    "            print(\"get_enc_embs_set_stride_set_batch_size \" + str(t) + \" seconds\")\n",
    "    if check_memory_usage: gpu_memory_status()\n",
    "    if verbose > 0: \n",
    "        print(\"get_enc_embs_set_stride_set_batch_size -->\")\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c9b646-2a6c-460b-bf56-e10fb740ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import wandb\n",
    "from dvats.utils import *\n",
    "wandb_api = wandb.Api()\n",
    "#enc_artifact = wandb_api.artifact('deepvats/mvp-SWV:latest')\n",
    "#enc_learner = enc_artifact.to_obj()\n",
    "#X = torch.rand(9, 1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8101184-0680-416f-a378-998c8fd00502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%time\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ade89-a3ad-4358-960d-5d21806ba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#%%time #TODO dont work with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=False)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__, embs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e63662-c5d3-47ea-a43f-5db9a2437745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "#%%time #TODO --> dont works with nb2py\n",
    "#embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=True)\n",
    "#test_eq(embs.shape[0], X.shape[0])\n",
    "#embs.shape, embs.__class__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
