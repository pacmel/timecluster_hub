{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6a4a02-5015-4a3e-9a64-6089de4e9532",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "> Architectures and functions for creating encoders that create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b014a18f-8538-4ed7-98ee-ddb165692553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macu/env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/macu/env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "from tsai.callback.MVP import *\n",
    "from tsai.imports import *\n",
    "from tsai.models.InceptionTimePlus import InceptionTimePlus\n",
    "from tsai.models.explainability import get_acts_and_grads\n",
    "from tsai.models.layers import *\n",
    "from tsai.data.validation import combine_split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c94f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from tsai.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04637a46",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c036898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class DCAE_torch(Module):\n",
    "    def __init__(self, c_in, seq_len, delta, nfs=[64, 32, 12], kss=[10, 5, 5],\n",
    "                 pool_szs=[2,2,3], output_fsz=10):\n",
    "        \"\"\"\n",
    "        Create a Deep Convolutional Autoencoder for multivariate time series of `d` dimensions,\n",
    "        sliced with a window size of `w`. The parameter `delta` sets the number of latent features that will be\n",
    "        contained in the Dense layer of the network. The the number of features\n",
    "        maps (filters), the filter size and the pool size can also be adjusted.\"\n",
    "        \"\"\"\n",
    "        assert all_equal([len(x) for x in [nfs, kss, pool_szs]], np.repeat(len(nfs), 3)), \\\n",
    "            'nfs, kss, and pool_szs must have the same length'\n",
    "        assert np.prod(pool_szs) == nfs[-1], \\\n",
    "            'The number of filters in the last conv layer must be equal to the product of pool sizes'\n",
    "        assert seq_len % np.prod(pool_szs) == 0, \\\n",
    "            'The product of pool sizes must be a divisor of the window size'\n",
    "        layers = []\n",
    "        for i in range_of(kss):\n",
    "            layers += [Conv1d(ni=nfs[i-1] if i>0 else c_in, nf=nfs[i], ks=kss[i]),\n",
    "                       nn.MaxPool1d(kernel_size=pool_szs[i])]\n",
    "        self.downsample = nn.Sequential(*layers)\n",
    "        self.bottleneck = nn.Sequential(OrderedDict([\n",
    "            ('flatten', nn.Flatten()),\n",
    "            ('latent_in', nn.Linear(seq_len, delta)),\n",
    "            ('latent_out', nn.Linear(delta, seq_len)),\n",
    "            ('reshape', Reshape(nfs[-1], seq_len // np.prod(pool_szs)))\n",
    "        ]))\n",
    "        layers = []\n",
    "        for i in reversed(range_of(kss)):\n",
    "            layers += [Conv1d(ni=nfs[i+1] if i != (len(nfs)-1) else nfs[-1],\n",
    "                              nf=nfs[i], ks=kss[i]),\n",
    "                       nn.Upsample(scale_factor=pool_szs[i])]\n",
    "        layers += [Conv1d(ni=nfs[0], nf=c_in, kernel_size=output_fsz)]\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a5831fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/macu/lib/orelm/algorithms\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "lib_orelm_path = os.path.expanduser(\"~/lib/orelm/algorithms\")\n",
    "sys.path.append(lib_orelm_path)\n",
    "print(lib_orelm_path)\n",
    "import algorithms.OR_ELM  as orelm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753050b7-b828-4971-b3e3-471697c0b6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'orelm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morelm_torch\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nbs/orelm_torch.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mORELM_torch\u001b[39;00m(Module):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m----> 8\u001b[0m       inputs, \n\u001b[1;32m      9\u001b[0m       outputs, \n\u001b[1;32m     10\u001b[0m       numHiddenNeurons, \n\u001b[1;32m     11\u001b[0m       activationFunction, \u001b[38;5;66;03m#? Must always be \"sig\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m       LN    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m#? - Layer normalization boolean\u001b[39;00m\n\u001b[1;32m     13\u001b[0m       AE    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m#? - Para AlaiÃ±e, siempre es True\u001b[39;00m\n\u001b[1;32m     14\u001b[0m       ORTH  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     15\u001b[0m       inputWeightForgettingFactor   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m,\n\u001b[1;32m     16\u001b[0m       outputWeightForgettingFactor  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m,\n\u001b[1;32m     17\u001b[0m       hiddenWeightForgettingFactor  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m\n\u001b[1;32m     18\u001b[0m     ): \n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m inputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \\\n\u001b[1;32m     20\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs must be numeric and greater than or equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m outputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \\\n\u001b[1;32m     22\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs must be numeric and greater than or equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'orelm'"
     ]
    }
   ],
   "source": [
    "import nbs.orelm_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = torch.rand(3, 1, 48)\n",
    "m = DCAE_torch(c_in=foo.shape[1], seq_len=foo.shape[2], delta=12)\n",
    "m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a4a1d-389c-481a-a54d-3f86cd2115f5",
   "metadata": {},
   "source": [
    "### Dictionary to get the default backbone modules to get the embeddings from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9f1b6-ae45-4b6e-b535-c7f121208721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENCODER_EMBS_MODULE_NAME = {\n",
    "    InceptionTimePlus: 'backbone', # for mvp based models\n",
    "    DCAE_torch: 'bottleneck.latent_in'\n",
    "    ORELM_torch: 'outputs'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a268f-8b4e-4432-b203-79263a247c4c",
   "metadata": {},
   "source": [
    "### Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c66ae6-3178-49dc-bd16-64c082012e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_enc_embs(X, enc_learn, module=None, cpu=False, average_seq_dim=True, to_numpy=True):\n",
    "    \"\"\"\n",
    "        Get the embeddings of X from an encoder, passed in `enc_learn as a fastai\n",
    "        learner. By default, the embeddings are obtained from the last layer\n",
    "        before the model head, although any layer can be passed to `model`.\n",
    "        Input\n",
    "        - `cpu`: Whether to do the model inference in cpu of gpu (GPU recommended)\n",
    "        - `average_seq_dim`: Whether to aggregate the embeddings in the sequence dimensions\n",
    "        - `to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n",
    "    \"\"\"\n",
    "    if cpu:\n",
    "        enc_learn.dls.cpu()\n",
    "        enc_learn.cpu()\n",
    "    else:\n",
    "        enc_learn.dls.cuda()\n",
    "        enc_learn.cuda()\n",
    "    if enc_learn.dls.bs == 0: enc_learn.dls.bs = 64\n",
    "    aux_dl = enc_learn.dls.valid.new_dl(X=X)\n",
    "    aux_dl.bs = enc_learn.dls.bs if enc_learn.dls.bs>0 else 64\n",
    "    module = nested_attr(enc_learn.model,\n",
    "                         ENCODER_EMBS_MODULE_NAME[type(enc_learn.model)]) \\\n",
    "                if module is None else module\n",
    "    embs = [get_acts_and_grads(model=enc_learn.model,\n",
    "                               modules=module,\n",
    "                               x=xb[0], cpu=cpu)[0] for xb in aux_dl]\n",
    "    embs = to_concat(embs)\n",
    "    if embs.ndim == 3 and average_seq_dim: embs = embs.mean(axis=2)\n",
    "    if to_numpy: embs = embs.numpy() if cpu else embs.cpu().numpy()\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc6c6c-df80-44de-a49e-f27e86b8964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from dvats.utils import *\n",
    "wandb_api = wandb.Api()\n",
    "enc_artifact = wandb_api.artifact('tchub/learner-mvp:run-tchub-3tipekxw')\n",
    "enc_learner = enc_artifact.to_obj()\n",
    "X = torch.rand(9, 1, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf504f6-a765-4a8e-b7e3-7ea521bdd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "%%time\n",
    "embs = get_enc_embs(X, enc_learner, cpu=True)\n",
    "test_eq(embs.shape[0], X.shape[0])\n",
    "embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ddae2-e646-442f-86c2-2a8c8ad8606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=False)\n",
    "test_eq(embs.shape[0], X.shape[0])\n",
    "embs.shape, embs.__class__, embs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7336d9-7e63-4635-8552-4a5d86492ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embs = get_enc_embs(X, enc_learner, cpu=False, to_numpy=True)\n",
    "test_eq(embs.shape[0], X.shape[0])\n",
    "embs.shape, embs.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e92c8-3932-470f-8f6a-1b7810032c18",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6132f-be0d-4e4e-abab-754cbe9a365c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "beep(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
