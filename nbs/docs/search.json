[
  {
    "objectID": "utils.html#pandas-dataframe-utilities",
    "href": "utils.html#pandas-dataframe-utilities",
    "title": "Utils",
    "section": "pandas Dataframe utilities",
    "text": "pandas Dataframe utilities\n\nNormalize columns\n\n\n\nnormalize_columns\n\n normalize_columns (df:pandas.core.frame.DataFrame)\n\nNormalize columns from df to have 0 mean and 1 standard deviation\n\nfoo = generate_TS_df(3, 3)\nfoo.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\ncount\n3.000000\n3.000000\n3.000000\n\n\nmean\n-0.369499\n0.672330\n0.044525\n\n\nstd\n0.390197\n1.413048\n1.052690\n\n\nmin\n-0.679825\n-0.240536\n-1.153047\n\n\n25%\n-0.588526\n-0.141487\n-0.345038\n\n\n50%\n-0.497226\n-0.042439\n0.462970\n\n\n75%\n-0.214336\n1.128763\n0.643311\n\n\nmax\n0.068554\n2.299964\n0.823651\n\n\n\n\n\n\n\n\nbar = normalize_columns(foo)\nbar.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\ncount\n3.000000e+00\n3.000000e+00\n3.000000e+00\n\n\nmean\n-1.110223e-16\n3.700743e-17\n7.401487e-17\n\n\nstd\n9.999997e-01\n9.999999e-01\n9.999999e-01\n\n\nmin\n-7.953070e-01\n-6.460257e-01\n-1.137630e+00\n\n\n25%\n-5.613231e-01\n-5.759301e-01\n-3.700644e-01\n\n\n50%\n-3.273393e-01\n-5.058345e-01\n3.975011e-01\n\n\n75%\n3.976535e-01\n3.230129e-01\n5.688150e-01\n\n\nmax\n1.122646e+00\n1.151860e+00\n7.401288e-01\n\n\n\n\n\n\n\n\ntest_close(bar.describe().loc['mean'].values, np.repeat(0.0, len(bar.columns)))\n\n\ntest_close(bar.describe().loc['std'].values, np.repeat(1.0, len(bar.columns)))\n\n\n\nRemove constant columns\n\n\n\nremove_constant_columns\n\n remove_constant_columns (df:pandas.core.frame.DataFrame)\n\n\nfoo = generate_TS_df(3, 3)\nfoo['constant'] = [0.0]*len(foo)\nfoo\n\n\n\n\n\n\n\n\n0\n1\n2\nconstant\n\n\n\n\n2023-12-15 11:09:56.121985\n0.147379\n1.027159\n-0.151897\n0.0\n\n\n2023-12-15 11:09:57.121985\n1.236241\n0.293195\n-0.086113\n0.0\n\n\n2023-12-15 11:09:58.121985\n1.279446\n0.929147\n0.393791\n0.0\n\n\n\n\n\n\n\n\nbar = remove_constant_columns(foo)\nbar\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n2023-12-15 11:09:56.121985\n0.147379\n1.027159\n-0.151897\n\n\n2023-12-15 11:09:57.121985\n1.236241\n0.293195\n-0.086113\n\n\n2023-12-15 11:09:58.121985\n1.279446\n0.929147\n0.393791\n\n\n\n\n\n\n\n\ncolumn_diff = set(foo.columns) - set(bar.columns)\ntest_eq_type(column_diff, set(['constant']))"
  },
  {
    "objectID": "utils.html#create-wandb-artifact-containing-just-the-reference-to-an-object-pass-as-argument",
    "href": "utils.html#create-wandb-artifact-containing-just-the-reference-to-an-object-pass-as-argument",
    "title": "Utils",
    "section": "Create wandb artifact containing just the reference to an object pass as argument",
    "text": "Create wandb artifact containing just the reference to an object pass as argument\n\n\nReferenceArtifact\n\n ReferenceArtifact (obj, name, type='object', folder=None,\n                    description:Optional[str]=None,\n                    metadata:Optional[dict]=None,\n                    incremental:Optional[bool]=None,\n                    use_as:Optional[str]=None)\n\nFlexible and lightweight building block for dataset and model versioning.\nConstructs an empty artifact whose contents can be populated using its add family of functions. Once the artifact has all the desired files, you can call wandb.log_artifact() to log it.\nArguments: name: (str) A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in use_artifact calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project. type: (str) The type of the artifact, which is used to organize and differentiate artifacts. Common types include dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots. description: (str, optional) Free text that offers a description of the artifact. The description is markdown rendered in the UI, so this is a good place to place tables, links, etc. metadata: (dict, optional) Structured data associated with the artifact, for example class distribution of a dataset. This will eventually be queryable and plottable in the UI. There is a hard limit of 100 total keys.\nExamples: Basic usage ``` wandb.init()\nartifact = wandb.Artifact('mnist', type='dataset')\nartifact.add_dir('mnist/')\nwandb.log_artifact(artifact)\n```\nReturns: An Artifact object.\n\nfoo = np.arange(10)\nbar = ReferenceArtifact(obj=foo, name='foo', folder='.')\nbar_path = Path(f'./{bar.metadata[\"ref\"][\"hash\"]}')\ntest_eq(bar_path.exists(), True)\ntest_eq(bar.metadata['ref']['type'], \"&lt;class 'numpy.ndarray'&gt;\")\n\nValueError: Path \"file://./-1139414644047310040\" must be a valid file or directory path\n\n\nWhen a reference artifact is used by one wandb run, we should have a method to get the original object from it\n\n\n\nArtifact.to_obj\n\n Artifact.to_obj ()\n\nDownload the files of a saved ReferenceArtifact and get the referenced object. The artifact must come from a call to run.use_artifact with a proper wandb run.\nTest with Reference artifact from a df\n\nfoo = generate_TS_df(3, 3)\nbar = ReferenceArtifact(obj=foo, name='test_reference_artifact')\nbar.manifest.entries.values()\n\ndict_values([ArtifactManifestEntry(path='6609970771346233684', digest='iRL0+EafUTCkH4Fb+jrmqg==', ref='file:///home/macu/data/wandb_artifacts/6609970771346233684', birth_artifact_id=None, size=1005, extra={}, local_path=None)])\n\n\n\ntest_eq(bar.name, 'test_reference_artifact')\n\n\ntest_eq(bar.metadata['ref']['type'], str(type(foo)))\n\nTODO: Test method to_obj\nReferenceArtifact with a numpy array\n\nfoo = np.random.randn(5)\nbar = ReferenceArtifact(obj=foo, name='test_reference_artifact')\nbar.manifest.entries.values()\n\ndict_values([ArtifactManifestEntry(path='1329328052175697719', digest='c2NqJgrLix/V9gGUK3eTWQ==', ref='file:///home/macu/data/wandb_artifacts/1329328052175697719', birth_artifact_id=None, size=187, extra={}, local_path=None)])\n\n\n\ntest_eq(bar.metadata['ref']['type'], str(type(foo)))\n\n\n\n\nPrintLayer\n\n PrintLayer ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nLearner.export_and_get\n\n Learner.export_and_get (keep_exported_file=False)\n\nExport the learner into an auxiliary file, load it and return it back.\n\n\nget_wandb_artifacts\n\n\n\nget_wandb_artifacts\n\n get_wandb_artifacts (project_path, type=None, name=None,\n                      last_version=True)\n\nGet the artifacts logged in a wandb project. Input: - project_path (str): entity/project_name - type (str): whether to return only one type of artifacts - name (str): Leave none to have all artifact names - last_version: whether to return only the last version of each artifact or not\nOutput: List of artifacts\n\nfoo = get_wandb_artifacts('wandb/artifacts-example', type='model')\ntest_eq(len(foo), 2)\nfoo = get_wandb_artifacts('wandb/artifacts-example', type='model', name='convnet')\ntest_eq(len(foo), 1)\nfoo = get_wandb_artifacts('wandb/artifacts-example', type='model', name='convnet', last_version=False)\ntest_eq(len(foo), 2)\n\n\n\nget_pickle_artifact\n\n\n\nget_pickle_artifact\n\n get_pickle_artifact (filename)"
  },
  {
    "objectID": "utils.html#exec-from-feather",
    "href": "utils.html#exec-from-feather",
    "title": "Utils",
    "section": "Exec from feather",
    "text": "Exec from feather\n\n\nexec_with_feather\n\n exec_with_feather (function, path=None, print_flag=False, *args,\n                    **kwargs)\n\n\n\n\npy_function\n\n py_function (module_name, function_name, print_flag=False)\n\n\n\n\nexec_with_feather_k_output\n\n exec_with_feather_k_output (function_name, module_name='main', path=None,\n                             k_output=0, print_flag=False,\n                             time_flag=False, *args, **kwargs)\n\n\n\n\nexec_with_and_feather_k_output\n\n exec_with_and_feather_k_output (function_name, module_name='main',\n                                 path_input=None, path_output=None,\n                                 k_output=0, print_flag=False,\n                                 time_flag=False, *args, **kwargs)"
  },
  {
    "objectID": "load.html",
    "href": "load.html",
    "title": "Load",
    "section": "",
    "text": "base_path = Path.home()\nbase_path\n\nPath('/home/macu')"
  },
  {
    "objectID": "load.html#time-series-artifacts-to-be-used-with-weights-and-biases",
    "href": "load.html#time-series-artifacts-to-be-used-with-weights-and-biases",
    "title": "Load",
    "section": "Time series artifacts (to be used with weights and biases)",
    "text": "Time series artifacts (to be used with weights and biases)\nThis class is meant to extend wandb.Artifact for logging/using files with time series data.\n\n\nTSArtifact\n\n TSArtifact (name, sd:pandas._libs.tslibs.timestamps.Timestamp,\n             ed:pandas._libs.tslibs.timestamps.Timestamp,\n             description:Optional[str]=None, metadata:Optional[dict]=None,\n             incremental:Optional[bool]=None, use_as:Optional[str]=None)\n\nFlexible and lightweight building block for dataset and model versioning.\nConstructs an empty artifact whose contents can be populated using its add family of functions. Once the artifact has all the desired files, you can call wandb.log_artifact() to log it.\nArguments: name: (str) A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in use_artifact calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project. type: (str) The type of the artifact, which is used to organize and differentiate artifacts. Common types include dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots. description: (str, optional) Free text that offers a description of the artifact. The description is markdown rendered in the UI, so this is a good place to place tables, links, etc. metadata: (dict, optional) Structured data associated with the artifact, for example class distribution of a dataset. This will eventually be queryable and plottable in the UI. There is a hard limit of 100 total keys.\nExamples: Basic usage ``` wandb.init()\nartifact = wandb.Artifact('mnist', type='dataset')\nartifact.add_dir('mnist/')\nwandb.log_artifact(artifact)\n```\nReturns: An Artifact object.\n\n# TSArtifact class TEST\n\n# resampling frequency\nresampling_freq = '5s'\n# handle missing values technique\nmissing_values_technique='overall_median'\n\n# testing dataframe\ndf_test = pd.util.testing.makeMissingDataframe()\ndf_test.index = pd.date_range(start='2021-01-01', periods=len(df_test), freq='s')\n\nartifact = TSArtifact.from_df(df_test, \n                              name='JNK', \n                              missing_values_technique=missing_values_technique,\n                              resampling_freq=resampling_freq, \n                              normalize=True)\nartifact.metadata\n\nAbout to write df to  /home/macu/data/wandb_artifacts/7736998280633728827\n\n\n{'TS': {'sd': '2021-01-01 00:00:00',\n  'ed': '2021-01-01 00:00:29',\n  'created': 'from-df',\n  'n_vars': 4,\n  'handle_missing_values_technique': 'overall_median',\n  'has_missing_values': 'False',\n  'n_samples': 6,\n  'freq': '&lt;5 * Seconds&gt;',\n  'vars': ['A', 'B', 'C', 'D'],\n  'normalization': {'means': {'A': -0.029637609663132728,\n    'B': -0.40026973314124753,\n    'C': -0.02134916042814836,\n    'D': 0.04137270661806195},\n   'stds': {'A': 0.39932602098066644,\n    'B': 0.34830011744118783,\n    'C': 0.36764279679757034,\n    'D': 0.25373970381817534}},\n  'hash': '7736998280633728827'}}\n\n\n\nhash = artifact.metadata['TS']['hash']\npath = \"../../data/wandb_artifacts/\"+hash\nprint(path)\nf = ft.read_feather(path)\nprint(f)\n\n../../data/wandb_artifacts/7736998280633728827\n                            A         B         C         D\n2021-01-01 00:00:00  0.382090  1.138692  0.910023  0.048650\n2021-01-01 00:00:05 -1.609279 -0.059981 -1.187677 -0.445947\n2021-01-01 00:00:10 -0.785577 -1.696949  1.317957 -0.902181\n2021-01-01 00:00:15  1.083399  0.066801 -0.017103  1.428413\n2021-01-01 00:00:20  0.320671 -0.300320 -1.011853  0.919442\n2021-01-01 00:00:25  0.608696  0.851757 -0.011347 -1.048376\n\n\n\ndf = ft.read_feather(\"/home/macu/data/wandb_artifacts/-2535364569820284064\")\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\nAt the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a wandb.apis.public.Artifact object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original TSArtifact.\n\n\n\nArtifact.to_df\n\n Artifact.to_df ()\n\nDownload the files of a saved wandb artifact and process them as a single dataframe. The artifact must come from a call to run.use_artifact with a proper wandb run.\nFor convenience, we can write a method to cast a downloaded wandb artifact (instance from wandb.apis.public,Artifact) to a TSArtifact\n\n\n\nArtifact.to_tsartifact\n\n Artifact.to_tsartifact ()\n\nCast an artifact as a TS artifact. The artifact must have been created from one of the class creation methods of the class TSArtifact. This is useful to go back to a TSArtifact after downloading an artifact through the wand API"
  },
  {
    "objectID": "load.html#inject-or-infer-frequencies-in-a-dataframe",
    "href": "load.html#inject-or-infer-frequencies-in-a-dataframe",
    "title": "Load",
    "section": "Inject or infer frequencies in a dataframe",
    "text": "Inject or infer frequencies in a dataframe\n/home/macu/env/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\n/home/macu/env/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n  else: warn(msg)\n/home/macu/env/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n/home/macu/env/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n\n\ninfer_or_inject_freq\n\n infer_or_inject_freq (df, injected_freq='1s', start_date=None,\n                       errors:DateTimeErrorChoices='raise',\n                       dayfirst:bool=False, yearfirst:bool=False,\n                       utc:bool|None=None, format:str|None=None,\n                       exact:bool=True, unit:str|None=None,\n                       infer_datetime_format:bool=False, origin='unix',\n                       cache:bool=True)\n\nInfer index frequency. If there’s not a proper time index, create fake timestamps, keeping the desired injected_freq. If that is None, set a default one of 1 second. start_date: the first date of the index (int or string).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\n\n\n\ninjected_freq\nstr\n1s\n\n\n\nstart_date\nNoneType\nNone\n\n\n\nerrors\nDateTimeErrorChoices\nraise\n- If :const:'raise', then invalid parsing will raise an exception.- If :const:'coerce', then invalid parsing will be set as :const:NaT.- If :const:'ignore', then invalid parsing will return the input.\n\n\ndayfirst\nbool\nFalse\nSpecify a date parse order if arg is str or is list-like.If :const:True, parses dates with the day first, e.g. :const:\"10/11/12\"is parsed as :const:2012-11-10... warning:: dayfirst=True is not strict, but will prefer to parse with day first. If a delimited date string cannot be parsed in accordance with the given dayfirst option, e.g. to_datetime(['31-12-2021']), then a warning will be shown.\n\n\nyearfirst\nbool\nFalse\nSpecify a date parse order if arg is str or is list-like.- If :const:True parses dates with the year first, e.g. :const:\"10/11/12\" is parsed as :const:2010-11-12.- If both dayfirst and yearfirst are :const:True, yearfirst is preceded (same as :mod:dateutil)... warning:: yearfirst=True is not strict, but will prefer to parse with year first.\n\n\nutc\nbool | None\nNone\nControl timezone-related parsing, localization and conversion.- If :const:True, the function always returns a timezone-aware UTC-localized :class:Timestamp, :class:Series or :class:DatetimeIndex. To do this, timezone-naive inputs are localized as UTC, while timezone-aware inputs are converted to UTC.- If :const:False (default), inputs will not be coerced to UTC. Timezone-naive inputs will remain naive, while timezone-aware ones will keep their time offsets. Limitations exist for mixed offsets (typically, daylight savings), see :ref:Examples&lt;br&gt;  &lt;to_datetime_tz_examples&gt; section for details.See also: pandas general documentation about timezone conversion and&lt;br&gt;localization&lt;br&gt;&lt;https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html&lt;br&gt;#time-zone-handling&gt;_.\n\n\nformat\nstr | None\nNone\nThe strftime to parse time, e.g. :const:\"%d/%m/%Y\". Note that:const:\"%f\" will parse all the way up to nanoseconds. Seestrftime documentation&lt;br&gt;&lt;https://docs.python.org/3/library/datetime.html&lt;br&gt;#strftime-and-strptime-behavior&gt;_ for more information on choices.\n\n\nexact\nbool\nTrue\nControl how format is used:- If :const:True, require an exact format match.- If :const:False, allow the format to match anywhere in the target string.\n\n\nunit\nstr | None\nNone\nThe unit of the arg (D,s,ms,us,ns) denote the unit, which is aninteger or float number. This will be based off the origin.Example, with unit='ms' and origin='unix', this would calculatethe number of milliseconds to the unix epoch start.\n\n\ninfer_datetime_format\nbool\nFalse\nIf :const:True and no format is given, attempt to infer the formatof the datetime strings based on the first non-NaN element,and if it can be inferred, switch to a faster method of parsing them.In some cases this can increase the parsing speed by ~5-10x.\n\n\norigin\nstr\nunix\nDefine the reference date. The numeric values would be parsed as numberof units (defined by unit) since this reference date.- If :const:'unix' (or POSIX) time; origin is set to 1970-01-01.- If :const:'julian', unit must be :const:'D', and origin is set to beginning of Julian Calendar. Julian day number :const:0 is assigned to the day starting at noon on January 1, 4713 BC.- If Timestamp convertible, origin is set to Timestamp identified by origin.\n\n\ncache\nbool\nTrue\nIf :const:True, use a cache of unique, converted dates to apply thedatetime conversion. May produce significant speed-up when parsingduplicate date strings, especially ones with timezone offsets. The cacheis only used when there are at least 50 values. The presence ofout-of-bounds values will render the cache unusable and may slow downparsing... versionchanged:: 0.25.0 changed default value from :const:False to :const:True.\n\n\nReturns\nDatetimeIndex | Series | DatetimeScalar | NaTType | None\n\nIf parsing succeeded.Return type depends on input (types in parenthesis correspond tofallback in case of unsuccessful timezone or out-of-range timestampparsing):- scalar: :class:Timestamp (or :class:datetime.datetime)- array-like: :class:DatetimeIndex (or :class:Series with :class:object dtype containing :class:datetime.datetime)- Series: :class:Series of :class:datetime64 dtype (or :class:Series of :class:object dtype containing :class:datetime.datetime)- DataFrame: :class:Series of :class:datetime64 dtype (or :class:Series of :class:object dtype containing :class:datetime.datetime)\n\n\n\n\nfoo = pd.DataFrame([1, 2, 3])\nbar = pd.DataFrame([1, 2, 3])\nfoo = infer_or_inject_freq(foo)\nbar = infer_or_inject_freq(bar, injected_freq='2s')\ntest_eq(foo.index.freq, '1s')\ntest_eq(bar.index.freq, '2s')\nfoo, bar\n\n(                     0\n 1970-01-01 00:00:00  1\n 1970-01-01 00:00:01  2\n 1970-01-01 00:00:02  3,\n                      0\n 1970-01-01 00:00:00  1\n 1970-01-01 00:00:02  2\n 1970-01-01 00:00:04  3)\n\n\n\nfoo = pd.DataFrame([1, 2, 3])\nbar = infer_or_inject_freq(foo, injected_freq='1W', start_date='01/01/2020')\nbaz = infer_or_inject_freq(foo, injected_freq='1W', start_date='2020-01-01', format = '%Y-%m-%d')\ntest_eq(bar, baz)"
  },
  {
    "objectID": "visualization.html#plot-mask",
    "href": "visualization.html#plot-mask",
    "title": "Visualization",
    "section": "Plot mask",
    "text": "Plot mask\n\n\nplot_mask\n\n plot_mask (mask, i=0, fig_size=(10, 10), title_str='Mask',\n            return_fig=False)\n\nPlot the mask passed as argument. The mask is a 3D boolean tensor. The first dimension is the window number (or item index), the second is the variable, and the third is the time step. Input: mask: 3D boolean tensor i: index of the window to plot fig_size: size of the figure title_str: title of the plot return_fig: if True, returns the figure Output: if return_fig is True, returns the figure, otherwise, it does not return anything\n\n# Creates a mask (3d boolean tensor) with random values masked and call the previous function to plot it\nmask = torch.rand(3,10,5) &gt; 0.9\ntest_eq(mask.dtype, torch.bool)\nplot_mask(mask, 0, fig_size=(10,5), return_fig=False)\n\n\n\n\n\n# Test the parameter return_fig\nm = plot_mask(mask, 0, fig_size=(10,5), return_fig=True)\nm"
  },
  {
    "objectID": "encoder.html",
    "href": "encoder.html",
    "title": "Encoder",
    "section": "",
    "text": "gpu_memory_status_\n\n gpu_memory_status_ (device=0)\n\n\n\n\ncreate_bar\n\n create_bar (percentage, color_code, length=20)\n\n\n\n\ncolor_for_percentage\n\n color_for_percentage (percentage)\n\n\n\n\nget_gpu_memory_\n\n get_gpu_memory_ (device=0)\n\n\n\nArchitectures\n\n\n\nDCAE_torch\n\n DCAE_torch (c_in, seq_len, delta, nfs=[64, 32, 12], kss=[10, 5, 5],\n             pool_szs=[2, 2, 3], output_fsz=10)\n\nSame as nn.Module, but no need for subclasses to call super().__init__\n\n\nDictionary to get the default backbone modules to get the embeddings from\n\n\nGetting the embeddings (activations) from the encoder\n\n\n\nget_enc_embs\n\n get_enc_embs (X, enc_learn, module=None, cpu=False, average_seq_dim=True,\n               to_numpy=True)\n\nGet the embeddings of X from an encoder, passed in enc_learn as a fastai learner. By default, the embeddings are obtained from the last layer before the model head, although any layer can be passed tomodel. Input -cpu: Whether to do the model inference in cpu of gpu (GPU recommended) -average_seq_dim: Whether to aggregate the embeddings in the sequence dimensions -to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)\n\n\n\nget_enc_embs_set_stride_set_batch_size\n\n get_enc_embs_set_stride_set_batch_size (X, enc_learn, stride, batch_size,\n                                         module=None, cpu=False,\n                                         average_seq_dim=True,\n                                         to_numpy=True, print_flag=False,\n                                         time_flag=False, chunk_size=0,\n                                         check_memory_usage=False)\n\nGet the embeddings of X from an encoder, passed in enc_learn as a fastai learner. By default, the embeddings are obtained from the last layer before the model head, although any layer can be passed tomodel. Input -cpu: Whether to do the model inference in cpu of gpu (GPU recommended) -average_seq_dim: Whether to aggregate the embeddings in the sequence dimensions -to_numpy`: Whether to return the result as a numpy array (if false returns a tensor)"
  },
  {
    "objectID": "dr.html#get-projections-umap-t-snet-pca",
    "href": "dr.html#get-projections-umap-t-snet-pca",
    "title": "Dimensionality reduction",
    "section": "Get projections (UMAP, T-SNET, PCA)",
    "text": "Get projections (UMAP, T-SNET, PCA)\n\n\nget_UMAP_prjs\n\n get_UMAP_prjs (input_data, cpu=True, print_flag=False,\n                check_memory_usage=True, n_neighbors=15, n_components=2,\n                metric='euclidean', metric_kwds=None, n_epochs=None,\n                learning_rate=1.0, min_dist=0.1, spread=1.0,\n                set_op_mix_ratio=1.0, local_connectivity=1.0,\n                repulsion_strength=1.0, negative_sample_rate=5,\n                transform_queue_size=4.0, init='spectral', a=None, b=None,\n                target_n_neighbors=-1, target_weight=0.5,\n                target_metric='categorical', hash_input=False,\n                random_state=None, precomputed_knn=None, callback=None,\n                handle=None, verbose=False, output_type=None)\n\nCompute the projections of input_data using UMAP, with a configuration contained in **kwargs.\n\nfoo = np.random.rand(5, 10)\nbar = get_UMAP_prjs(\n    foo, \n    cpu=False, \n    print_flag = True,\n    check_memory_usage = True,\n    random_state = 1234, #822569775\n    n_neighbors=3, \n    min_dist=0.1\n)\ntest_eq(bar.shape, (foo.shape[0], 2))\n\n--&gt; get_UMAP_prjs\nkwargs:  {'random_state': 1234, 'n_neighbors': 3, 'min_dist': 0.1}\n1511f9e3c0ce668bd54ce23099714673\nUsed mem: 2454\nUsed mem: 24576\nMemory Usage: [██------------------] 10%\n1234\nRandomState(MT19937)\n-- cuml.UMAP -- False\n------- reducer --------\nUMAP()\n{'handle': &lt;pylibraft.common.handle.Handle object&gt;, 'verbose': 4, 'output_type': 'input', 'n_neighbors': 3, 'n_components': 2, 'n_epochs': None, 'learning_rate': 1.0, 'min_dist': 0.1, 'spread': 1.0, 'set_op_mix_ratio': 1.0, 'local_connectivity': 1.0, 'repulsion_strength': 1.0, 'negative_sample_rate': 5, 'transform_queue_size': 4.0, 'init': 'spectral', 'a': 1.5769434601962196, 'b': 0.8950608779914887, 'target_n_neighbors': -1, 'target_weight': 0.5, 'target_metric': 'categorical', 'hash_input': False, 'random_state': 822569775, 'callback': None, 'metric': 'euclidean', 'metric_kwds': None, 'precomputed_knn': None}\n------- reducer --------\nUsed mem: 2462\nUsed mem: 24576\nMemory Usage: [██------------------] 10%\nprjs checksum  51038866661e8b8943d9153570a0567f\nget_UMAP_prjs --&gt;\n\n\n\nfoo = np.random.rand(5, 10)\nbar = get_UMAP_prjs(\n    foo, \n    cpu=True, \n    print_flag = True,\n    check_memory_usage = True\n    n_neighbors=3, \n    min_dist=0.1\n)\ntest_eq(bar.shape, (foo.shape[0], 2))\n\n--&gt; get_UMAP_prjs\nkwargs:  {'n_neighbors': 3, 'min_dist': 0.1}\n175a6ecb96ef4de7bcbf326ae624d879\nUsed mem: 2428\nUsed mem: 24576\nMemory Usage: [██------------------] 10%\n-- cuml.UMAP -- False\n------- reducer --------\nUMAP()\n{'handle': &lt;pylibraft.common.handle.Handle object&gt;, 'verbose': 4, 'output_type': 'input', 'n_neighbors': 3, 'n_components': 2, 'n_epochs': None, 'learning_rate': 1.0, 'min_dist': 0.1, 'spread': 1.0, 'set_op_mix_ratio': 1.0, 'local_connectivity': 1.0, 'repulsion_strength': 1.0, 'negative_sample_rate': 5, 'transform_queue_size': 4.0, 'init': 'spectral', 'a': 1.5769434601962196, 'b': 0.8950608779914887, 'target_n_neighbors': -1, 'target_weight': 0.5, 'target_metric': 'categorical', 'hash_input': False, 'random_state': 2997075094, 'callback': None, 'metric': 'euclidean', 'metric_kwds': None, 'precomputed_knn': None}\n------- reducer --------\nUsed mem: 2438\nUsed mem: 24576\nMemory Usage: [██------------------] 10%\nprjs checksum  28f08a32beaf2872fd87d57b643a166d\nget_UMAP_prjs --&gt;\n\n\nIf you want to have consistent results across executions, use random_state\n\n\n\nget_PCA_prjs\n\n get_PCA_prjs (X, cpu=False, copy=True, handle=None, iterated_power=15,\n               n_components=None, random_state=None, svd_solver='auto',\n               tol=1e-07, verbose=False, whiten=False, output_type=None)\n\nComputes PCA projections of X\n\n\n\nget_TSNE_prjs\n\n get_TSNE_prjs (X, cpu=False, n_components=2, perplexity=30.0,\n                early_exaggeration=12.0, late_exaggeration=1.0,\n                learning_rate=200.0, n_iter=1000,\n                n_iter_without_progress=300, min_grad_norm=1e-07,\n                metric='euclidean', metric_params=None, init='random',\n                verbose=False, random_state=None, method='fft', angle=0.5,\n                learning_rate_method='adaptive', n_neighbors=90,\n                perplexity_max_iter=100, exaggeration_iter=250,\n                pre_momentum=0.5, post_momentum=0.8,\n                square_distances=True, precomputed_knn=None, handle=None,\n                output_type=None)\n\nComputes TSNE projections of X\n\nfoo = np.random.rand(90, 10)\nbar = get_TSNE_prjs(foo, cpu=False)\n\n/home/macu/env/lib/python3.10/site-packages/cuml/internals/api_decorators.py:342: UserWarning: Starting from version 22.04, the default method of TSNE is 'fft'.\n  return func(**kwargs)"
  },
  {
    "objectID": "dr.html#export",
    "href": "dr.html#export",
    "title": "Dimensionality reduction",
    "section": "Export",
    "text": "Export\n\n\ncluster_score\n\n cluster_score (prjs, clusters_labels, print_flag)"
  }
]