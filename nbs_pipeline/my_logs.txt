--> LR_find
n_epoch: 3
cb: LRFinder
--> Fit
--> Create opt
---> opt_func 
self.model: ORELM_torch(
  (inputAE): FOSELM_torch()
  (hiddenAE): FOSELM_torch()
)
self.splitter(self.model): []
Learning rate: 0.001
OPT func: <function Adam at 0x7f2b524d5870>
Adam optimizer
Get cbs
decouple_wd: True
weight_decay <function weight_decay at 0x7f2b524d5240>
l2_reg <function l2_reg at 0x7f2b524d52d0>
Cbs: [<function weight_decay at 0x7f2b524d5240>]
Add to cbs
average_sqr_grad<function average_sqr_grad at 0x7f2b524d53f0>
step_stat<function step_stat at 0x7f2b524d56c0>
adam_step<function adam_step at 0x7f2b524d57e0>
partial(average_grad, dampening=True): functools.partial(<function average_grad at 0x7f2b524d5360>, dampening=True)
About to get optimizer
Get params
Get cbs
Get defaults
Get param list from params: []
Get hyperes
Set hypers
Set frozen idx
Return optimizer
-- opt_func -> 
Estoy aqui
--> bn_bias_state
--> norm bias params
Getting res
--> norm bias params
Getting res
Okay got res
About to return
--> norm bias params
Getting res
Okay got res
About to return
Okay got res
About to return
mapping var [tensor([[-0.0201, -0.9106, -0.3460,  0.2690,  0.3306,  0.2629, -0.7413,  0.0400,
         -0.3430,  0.8464,  0.8662,  0.6521,  0.9422, -0.9131, -0.4958, -0.0518,
         -0.8982,  0.5048,  0.5389, -0.8018, -0.0117, -0.3826,  0.3216, -0.0617,
          0.2516]], grad_fn=<SubBackward0>), tensor([[ 0.7470, -0.7139,  0.0408,  0.0437, -0.5655, -0.0709, -0.9183,  0.9098,
          0.0901, -0.0172, -0.3812, -0.9559,  0.0599,  0.5100,  0.4138,  0.4670,
          0.2941,  0.9557,  0.1157, -0.6437,  0.7851, -0.4201,  0.6883, -0.2191,
         -0.3569]], grad_fn=<SubBackward0>), tensor([[-0.6570, -0.9197,  0.1740, -0.3054, -0.8998, -0.7519, -0.5452, -0.6246,
         -0.3882,  0.6820, -0.2313,  0.0673,  0.4960, -0.8112, -0.1339,  0.5075,
         -0.1981,  0.3401, -0.0552, -0.8474, -0.6391, -0.0958, -0.7247, -0.1527,
          0.4273]], grad_fn=<SubBackward0>)]
Returning
Got norm bias state
--> bn_bias_state
--> norm bias params
Getting res
--> norm bias params
Getting res
Okay got res
About to return
--> norm bias params
Getting res
Okay got res
About to return
Okay got res
About to return
mapping var []
Returning
Create opt -->
About to fit
Event: fit
Function: <bound method Learner._do_fit of <fastai.learner.Learner object at 0x7f2a20991630>>
--> _do_fit
epoch 0:
Event: epoch
Function: <bound method Learner._do_epoch of <fastai.learner.Learner object at 0x7f2a20991630>>
--> Do epoch
Event: train
Function: <bound method Learner.all_batches of <fastai.learner.Learner object at 0x7f2a20991630>>
Set the model to training mode
... Enabling Vs Code execution ...
--> Al batches
Event: batch
Function: <bound method Learner._do_one_batch of <fastai.learner.Learner object at 0x7f2a20991630>>
--> Do one batch
--> Forward
features ~ (num_samples, num_vars, num_steps) = torch.Size([64, 1, 25])
Case: anomalous - 3D shape
--> Input AE
--> Foselm: Train
targets shape torch.Size([1, 25])
FOSELM:TRAIN:2SHAPED
Dimensions ok
Features ~ torch.Size([1, 25])
Targets ~ torch.Size([1, 25])
Inputs ~ 25
1 1
--> Train func (single)
Foselm - Calculate Hidden layer activation
Features: torch.Size([1, 25])
weights: torch.Size([25, 25])
Foselm - Layer normalizatison
--> SigmoidActFunc V ~ torch.Size([1, 25])
SigmoidActFunc -->
FOSELM: Calculate hidden layer activation -->
non RLS
Train func (single) -->
FOSELM:Train:END -->
Input AE -->
--> Hidden AE
--> Foselm: Train
targets shape torch.Size([1, 25])
FOSELM:TRAIN:2SHAPED
Dimensions ok
Features ~ torch.Size([1, 25])
Targets ~ torch.Size([1, 25])
Inputs ~ 25
1 1
--> Train func (single)
Foselm - Calculate Hidden layer activation
Features: torch.Size([1, 25])
weights: torch.Size([25, 25])
Foselm - Layer normalizatison
--> SigmoidActFunc V ~ torch.Size([1, 25])
SigmoidActFunc -->
FOSELM: Calculate hidden layer activation -->
non RLS
Train func (single) -->
FOSELM:Train:END -->
Hidden AE -->
Before LR 3: torch.Size([1, 25])
---> Linear_recurrent
numSamples: 1
numInputs: 25
numHiddenNeuron: 25
Features = (samples, inputs): torch.Size([1, 25])
NumInputs = (hidden, inputs): torch.Size([25, 25])
Linear_recurrent --->
--> SigmoidActFunc V ~ torch.Size([1, 25])
SigmoidActFunc -->
ORELM: calculate hidden layer activation 3-->
Forward --> result ~ torch.Size([1, 25])
Do1b after pred
Loss_func MSELoss()
Loss function mse_loss
input -> input | Target -> target
Has torch function variadic
Check sizes
Este warning es por el target ~ y el input~
Check size_average
Broadcast tensors
Before broadcast: 
input ~ torch.Size([1, 25])
target ~ torch.Size([64, 1, 25])
About to apply mse_loss
input: expanded input ~ torch.Size([64, 1, 25])
target: expanded target ~ torch.Size([64, 1, 25])
self.loss_Grad.backward: tensor(1.3847, grad_fn=<MseLossBackward0>)
Loss clone
Do1b after loss
Before backward
Event: backward
Function: <bound method Learner._backward of <fastai.learner.Learner object at 0x7f2a20991630>>
--> _Backward 
loss_grad: tensor(1.3847, grad_fn=<MseLossBackward0>)
loss_grad: <class 'torch.Tensor'>
Backward: <bound method Tensor.backward of tensor(1.3847, grad_fn=<MseLossBackward0>)> | 
Self: <fastai.learner.Learner object at 0x7f2a20991630>
Backward.requires_grad True
--> Backward 
backward: Tensors: tensor(1.3847, grad_fn=<MseLossBackward0>)
Grad variables: None
Checking line Var <class 'torch.autograd.variable.Variable'> type <class 'torch.autograd.variable.VariableMeta'>
_Backward -->
Before Step
Event: step
Function: <bound method Learner._step of <fastai.learner.Learner object at 0x7f2a20991630>>
Do one batch -->
Traceback (most recent call last):
  File "/tmp/ipykernel_1125118/1119649501.py", line 16, in <module>
    lr_valley, lr_steep = learn.lr_find(suggest_funcs=[valley, steep])
  File "/home/macu/env/lib/python3.10/site-packages/fastai/callback/schedule.py", line 309, in lr_find
    with self.no_logging(): self.fit(n_epoch, cbs=cb)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 316, in fit
    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 214, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 302, in _do_fit
    self._with_events(self._do_epoch, 'epoch', CancelEpochException)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 214, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 289, in _do_epoch
    self._do_epoch_train()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 280, in _do_epoch_train
    self._with_events(self.all_batches, 'train', CancelTrainException)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 214, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 221, in all_batches
    for o in enumerate(self.dl): self.one_batch(*o)
  File "/home/macu/lib/tsai/tsai/learner.py", line 40, in one_batch
    self._with_events(self._do_one_batch, 'batch', CancelBatchException)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 216, in _with_events
    self(f'after_{event_type}');  final()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 172, in __call__
    def __call__(self, event_name): L(event_name).map(self._call_one)
  File "/home/macu/env/lib/python3.10/site-packages/fastcore/foundation.py", line 156, in map
    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))
  File "/home/macu/env/lib/python3.10/site-packages/fastcore/basics.py", line 840, in map_ex
    return list(res)
  File "/home/macu/env/lib/python3.10/site-packages/fastcore/basics.py", line 825, in __call__
    return self.func(*fargs, **kwargs)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 176, in _call_one
    for cb in self.cbs.sorted('order'): cb(event_name)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/callback/core.py", line 62, in __call__
    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\n\t{e.args[0]}', replace=True)
  File "/home/macu/env/lib/python3.10/site-packages/fastai/callback/core.py", line 60, in __call__
    try: res = getcallable(self, event_name)()
  File "/home/macu/env/lib/python3.10/site-packages/fastai/learner.py", line 617, in after_batch
    self.lrs.append(self.opt.hypers[-1]['lr'])
  File "/home/macu/env/lib/python3.10/site-packages/fastcore/foundation.py", line 112, in __getitem__
    def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None)
  File "/home/macu/env/lib/python3.10/site-packages/fastcore/foundation.py", line 116, in _get
    if is_indexer(i) or isinstance(i,slice): return getattr(self.items,'iloc',self.items)[i]
IndexError: Exception occured in `Recorder` when calling event `after_batch`:
	list index out of range

