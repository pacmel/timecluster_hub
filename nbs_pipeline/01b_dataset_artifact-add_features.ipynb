{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create artifact from time series dataframe\n",
    "Gets a .tsf or .csv with a time serie, convert int to np.dataframe and loads it to weights and biases (W&B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Initial notebook setup and specific debugging and pre-configured cases selection\n",
    "### VsCode update patch\n",
    "Initial notebook setup when using VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '--vscode' in sys.argv:\n",
    "    print(\"Executing inside vscode\")\n",
    "    import nbs_pipeline.utils.vscode  as vs\n",
    "    vs.DisplayHandle.update = vs.update_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging variables\n",
    "- `print_flag`. If `True` it adds debbuging messages in those functions that allows so (eg. `get_enc_embeddings`)\n",
    "- `reset_kernel`. If `True` it resets the kernel by the end of the execution. Use only in case that memory management is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_flag = True\n",
    "reset_kernel=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preconfigurated cases selection\n",
    "- `pre_configured_case`. If `True`, a preconfigured case will be selected, forcing the artifact to get the expected configuration based on the information in `config\\*.yml` and `utils\\config.py`.\n",
    "- `case_id`. If `preconfigured_case` is `True`, it forces to select the configuration of the `case_id` preconfigured samples. The available preconfigured samples are shown in the next cell.\n",
    "- `frequency_factor`. If `pre_configured_case` is `True`, frequency will be resampled by `config.freq*frequency_factor`\n",
    "  `frequency_factor_change_alias`. If `pre_configured_case` is `True` and `frequency_factor != 1` then the dataset alias will be modified for adding the new frequency as suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.config as cfg_\n",
    "cfg_.show_available_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_configured_case = True\n",
    "case_id = 8\n",
    "frequency_factor = 1\n",
    "frequency_factor_change_alias = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from dvats.load import TSArtifact, infer_or_inject_freq\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tsai.data.external import convert_tsf_to_dataframe\n",
    "from tsai.utils import stack_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path and Artiffact configurattions\n",
    "This notebook gets configuration from `config\\base.yaml` and `config\\01-dataset_artifact.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg_.get_artifact_config_sd2a(print_flag = False)\n",
    "if pre_configured_case: \n",
    "    cfg_.force_artifact_config_sd2a(\n",
    "        config = config, \n",
    "        id = case_id, \n",
    "        print_flag = print_flag, \n",
    "        both = print_flag, \n",
    "        frequency_factor = frequency_factor, \n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is assumed to come as a dataframe, either as a binarized  picke file or\n",
    "as a csv file. It can also come as a `.tsf` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check file content (if wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_flag:\n",
    "    fpath=os.path.expanduser(config.data_fpath)\n",
    "    print(fpath)\n",
    "    try: \n",
    "        with open(fpath, 'r') as file:\n",
    "            for _ in range(13):\n",
    "                line = file.readline()\n",
    "                print(line, end='')\n",
    "        data, _, _, _, _ = convert_tsf_to_dataframe(fpath)\n",
    "        print(\"Timestamp\", data.start_timestamp)\n",
    "    except Exception as e:\n",
    "        print(\"Error while converting file. Maybe not a tsf: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = str(config.data_fpath).split('.')[-1]\n",
    "\n",
    "if ext == 'pickle':\n",
    "    df = pd.read_pickle(config.data_fpath)\n",
    "    \n",
    "elif ext in ['csv','txt']:\n",
    "    df = pd.read_csv(config.data_fpath, **config.csv_config)\n",
    "    \n",
    "elif ext == 'tsf':\n",
    "    data, _, _, _, _ = convert_tsf_to_dataframe(os.path.expanduser(config.data_fpath))\n",
    "    config.update({'start_date': data.start_timestamp[0]}, allow_val_change=True)\n",
    "    date_format = config.date_format\n",
    "    df = pd.DataFrame(stack_pad(data.series_value).T)\n",
    "    \n",
    "else:\n",
    "    raise Exception('The data file path has an unsupported extension')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_flag:\n",
    "    print(f'File loaded successfully')\n",
    "    print(df.shape)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the time column (if any) as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.time_col is not None:\n",
    "    if print_flag: print(\"time_col: \"+str(config.time_col))\n",
    "    \n",
    "    if isinstance(config.time_col, int): \n",
    "        if print_flag: print(\"Op 1: time_col int\")\n",
    "        datetime = df.iloc[:, config.time_col]\n",
    "    \n",
    "    elif isinstance(config.time_col, list): \n",
    "        if print_flag: print(\"Op 2: time_col list\")\n",
    "        datetime = df.iloc[:, config.time_col].apply(lambda x: x.astype(str).str.cat(sep='-'), axis=1)\n",
    "    \n",
    "    index = pd.DatetimeIndex(datetime)\n",
    "    \n",
    "    if config.date_offset:\n",
    "        index += config.date_offset\n",
    "    \n",
    "    df = df.set_index(index, drop=False)   \n",
    "    \n",
    "    #Delete Timestamp col\n",
    "    col_name = df.columns[config.time_col]\n",
    "    \n",
    "    if print_flag: print(\"... drop Timestamp col \" + str(col_name))\n",
    "    \n",
    "    df = df.drop(col_name, axis=1)\n",
    "    \n",
    "if print_flag: display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set dataframe frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = infer_or_inject_freq(\n",
    "    df, \n",
    "    injected_freq=config.freq, \n",
    "    start_date=config.start_date, \n",
    "    format=config.date_format\n",
    ")\n",
    "if print_flag: print(df.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only the needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of variables\n",
    "if config.data_cols:\n",
    "    if print_flag: print(\"data_cols: \", config.data_cols)\n",
    "    df = df.iloc[:, config.data_cols]\n",
    "\n",
    "if print_flag: print(f'Num. variables: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicated rows\n",
    "if print_flag: print(\"df shape before dropping duplicates\", df.shape)\n",
    "df.drop_duplicates()\n",
    "if print_flag: print(\"df shape after dropping duplicates\", df.shape)\n",
    "# Verificar si hay duplicados en el Ã­ndice del dataframe\n",
    "if df.index.duplicated().any():\n",
    "    raise ValueError(\"Duplicated index names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the default missing values by np.NaN\n",
    "if config.missing_values_constant:\n",
    "    df.replace(config.missing_values_constant, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_time_serie_flag = True\n",
    "if show_time_serie_flag:\n",
    "    # Show time series plot\n",
    "    fig, ax = plt.subplots(1, figsize=(15,5), )\n",
    "    cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "    #df.plot(color=cmap(0.05), ax=ax) # or use colormap=cmap\n",
    "    df.plot(colormap=cmap, ax=ax) # or use colormap=cmap\n",
    "    # rect = Rectangle((5000, -4.2), 3000, 8.4, facecolor='lightgrey', alpha=0.5)\n",
    "    # ax.add_patch(rect)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.core import TSTensor\n",
    "from tsai.data.preprocessing  import TSRollingMean\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se quieren aÃ±adir simplemente N pasos hacia atrÃ¡s/Hacia delante\n",
    "def add_lagged_features(df, total_extra_features = 12, save_index = True):\n",
    "    # reframe as supervised learning\n",
    "    if save_index:\n",
    "        df_lag = pd.DataFrame(index=df.index)\n",
    "    else:\n",
    "        df_lag = pd.DataFrame()\n",
    "    for i in range(total_extra_features,0,-1):\n",
    "     df_lag['t-'+str(i)] = df.shift(i).values[:,0]\n",
    "    df_lag['t'] = df.values[:,0]\n",
    "    return df_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFrame2TSTensor(df, print_flag = False):\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(df.index.name) if df.index.name else None\n",
    "    arr = np.vstack([df[col].values for col in cols])\n",
    "    if print_flag: \n",
    "        print(arr.shape)\n",
    "    df_tensor = TSTensor(arr)\n",
    "    df_tensor = np.expand_dims(arr, axis=0)\n",
    "    df_tensor = TSTensor(df_tensor)\n",
    "    if print_flag:\n",
    "        print(df_tensor.shape)\n",
    "        print(\"(1, nvars, length) = (1,\", df.shape[1], \",\", df.shape[0],\")\")\n",
    "    return df_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Check que efectivamente se estÃ© creando shape (1, num_vars, df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando rolling means de diferentes tamaÃ±os \n",
    "def add_rolling_means(\n",
    "    df, \n",
    "    config,\n",
    "    window_init = 1, #First window mean size\n",
    "    window_end = 2, #Last window mean size\n",
    "    window_step = 1, #Windows will be window_init + k*step until window_end\n",
    "    replace = False, #True for replacing original variables, False for adding extra variable\n",
    "    print_flag = False,\n",
    "    save_index = True\n",
    "):\n",
    "    vars = list(range(df.shape[1])) if config.data_cols == [] else config.data_cols\n",
    "\n",
    "        \n",
    "    if print_flag:\n",
    "        print(\"-----> Convert df to tensor <-----\")\n",
    "    \n",
    "    if replace:\n",
    "        if save_index:\n",
    "            df_roll = pd.DataFrame(index=df.index)\n",
    "        else:\n",
    "            df_roll = pd.DataFrame()\n",
    "    else:\n",
    "        df_roll = df.copy()\n",
    "        if print_flag: print(\"df_roll starts as original ~\",  df_roll.shape)\n",
    "\n",
    "    t = dataFrame2TSTensor(df, print_flag)\n",
    "\n",
    "    \n",
    "    \n",
    "    if print_flag:\n",
    "        print(\"-----> Apply rolling means to tensor <-----\", t.shape)\n",
    "\n",
    "\n",
    "    concatenated_columns = []\n",
    "    for i in range(window_init,window_end, +window_step):\n",
    "        if print_flag:  print(\"\\t ---> Window\", i, \"<----\")\n",
    "        \n",
    "        t_nvars = t.shape[1]\n",
    "\n",
    "        if print_flag: print(\"\\t Vars: \", vars)\n",
    "            \n",
    "        t_roll_i = TSRollingMean(sel_vars=vars, window=i)(t)\n",
    "\n",
    "        \n",
    "        count = t_nvars       \n",
    "\n",
    "            \n",
    "        for var in vars: \n",
    "            columnname = 'RollingMean - var ' + df.columns[var] + \" - window - \" + str(i)\n",
    "            if print_flag: \n",
    "                print(\"\\t\\t---> Saving column | \", columnname, \" | <----\")\n",
    "                print(\"\\t\\t t_roll_i ~\", t_roll_i.shape)\n",
    "                print(\"\\t\\t count\", count)\n",
    "                print(\"\\t\\t t_roll_i[:,count] ~\", t_roll_i[:,count].shape)\n",
    "                print(\"\\t\\t df_roll length\", df_roll.shape[0])\n",
    "            #df_roll[columnname]  = t_roll_i[:,count][0]\n",
    "            concatenated_columns.append(pd.Series(t_roll_i[:, count][0], name=columnname))\n",
    "            count += 1\n",
    "        \n",
    "    if print_flag: \n",
    "        print(\"-----> Save the result <-----\")\n",
    "        \n",
    "    df_concat = pd.concat(concatenated_columns, axis=1)\n",
    "    \n",
    "    if print_flag: \n",
    "        print(\"-----> df concat <-----\")\n",
    "        display(df_concat.head())\n",
    "    \n",
    "    df_roll[df_concat.columns] = df_concat.values\n",
    "        \n",
    "    if print_flag: \n",
    "        print(\"-----> df roll <-----\")\n",
    "        print(df_roll.shape)\n",
    "        display(df_roll.head())\n",
    "        \n",
    "    return df_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod = add_rolling_means(\n",
    "    df, config, \n",
    "    window_init = 2, \n",
    "    window_end = 4, \n",
    "    window_step = 1, \n",
    "    replace = False, \n",
    "    print_flag = False, \n",
    "    save_index = True\n",
    ")\n",
    "display(df_mod.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pyscamp as scamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mps(ts, mp, m, print_flag = False):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    gs = GridSpec(2, 1, height_ratios=[1, 4])\n",
    "    # Serie temporal\n",
    "    if print_flag:\n",
    "        print(ts)\n",
    "        print(mp.shape)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.plot(ts, label=\"Serie Temporal\")\n",
    "    ax1.set_title(\"Serie Temporal\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # MPlot\n",
    "    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "    mp_values = mp.astype(float) # Extraer solo los valores del perfil de similitud\n",
    "    ax2.imshow(mp_values.reshape(-1, 1).T, aspect='auto', origin='lower', cmap='hot', extent=(0, len(ts), 0, m))\n",
    "    ax2.set_title(\"Matrix profile plot\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyscamp as scamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando rolling means de diferentes tamaÃ±os \n",
    "def add_selfjoin_profile(\n",
    "    df, \n",
    "    config,\n",
    "    m, #Should be inside config\n",
    "    replace = False, #True for replacing original variables, False for adding extra variable\n",
    "    print_flag = False,\n",
    "    save_index = True\n",
    "):\n",
    "    vars = list(range(df.shape[1])) if config.data_cols == [] else config.data_cols\n",
    "\n",
    "    has_gpu_support = scamp.gpu_supported()\n",
    "    if print_flag: \n",
    "        has_gpu_support\n",
    "        \n",
    "    if replace:\n",
    "        if save_index:\n",
    "            df_mp = pd.DataFrame(index=df.index)\n",
    "        else:\n",
    "            df_mp = pd.DataFrame()\n",
    "    else:\n",
    "        df_mp = df.copy()\n",
    "        if print_flag: print(\"df_roll starts as original ~\",  df_mp.shape)\n",
    "\n",
    "    concatenated_columns = []\n",
    "    for var in vars: \n",
    "        ts = df.iloc[:,var].values\n",
    "        if print_flag:\n",
    "            print(\"var: \", var)\n",
    "            print(\"ts.len: \", len(ts))\n",
    "            print(\"m: \", m)\n",
    "        mp_scamp, _ = scamp.selfjoin(ts, m)\n",
    "        mp_scamp = np.pad(mp_scamp, (0, df.shape[0] - len(mp_scamp)), 'constant', constant_values=(0))\n",
    "    \n",
    "        columnname = 'Matrix Profile - var ' + df.columns[var]\n",
    "        concatenated_columns.append(pd.Series(\n",
    "            mp_scamp, name=columnname)\n",
    "        )\n",
    "        \n",
    "        if print_flag: \n",
    "            print(\"---> MP plot var \" + df.columns[var] + \" <---\")\n",
    "            plot_mps(ts, mp_scamp, m, print_flag = print_flag)\n",
    "            print(\"\\t\\t---> Saving column | \", columnname, \" | <----\")\n",
    "            print(\"\\t\\t mp_var ~\", mp_scamp.shape)\n",
    "    \n",
    "    df_concat = pd.concat(concatenated_columns, axis=1)\n",
    "    df_mp[df_concat.columns] = df_concat.values\n",
    "    \n",
    "    return df_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_ = add_selfjoin_profile(\n",
    "    df, config, \n",
    "    m = 2000,\n",
    "    replace = True, \n",
    "    print_flag = True, \n",
    "    save_index = True\n",
    ")\n",
    "df_mod = pd.concat([df_mod, df_mod_], axis=1)\n",
    "display(df_mod_.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handle Missing Values, Resample and Normalize__\n",
    "\n",
    "> In this second part, Time Series Artifact (TSArtifact) object can be created and missing values handling techniques, resampling and normalization can be applied.\n",
    "> \n",
    "> This techniques should be applied on the three subsets that must be previously created: training, validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_df(config, df, print_flag = False): \n",
    "    rg = config.range_training\n",
    "    if isinstance(rg, list):\n",
    "        rg_training = rg\n",
    "    \n",
    "    elif isinstance(rg, dict):\n",
    "        rg_training = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "    \n",
    "    elif config.test_split:\n",
    "        rg_training = df.index[:math.ceil(len(df) * (1-config.test_split))]\n",
    "\n",
    "    else:\n",
    "        rg_training = None\n",
    "        \n",
    "    df_training = df[df.index.isin(rg_training)] if rg_training is not None else df\n",
    "    return df_training, rg_training    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training, rg_training = build_train_df(config, df_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build training artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_artifact = TSArtifact.from_df(\n",
    "    df_training, \n",
    "    name=config.artifact_name, \n",
    "    missing_values_technique=config.missing_values_technique,\n",
    "    resampling_freq=config.resampling_freq, \n",
    "    normalize=config.normalize_training, \n",
    "    path=str(Path.home()/config.wandb_artifacts_path)\n",
    ")\n",
    "if print_flag: display(training_artifact.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging \n",
    "if df_training.index.duplicated().any():\n",
    "    raise ValueError(\"Duplicated index names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe & artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_df(config, df, print_flag = False): \n",
    "    # Testing data\n",
    "    rg = config.range_testing\n",
    "    df_testing = pd.DataFrame()\n",
    "    if rg or config.test_split:\n",
    "    \n",
    "        if isinstance(rg, list):\n",
    "            rg_testing = rg\n",
    "\n",
    "        elif isinstance(rg, dict):\n",
    "            rg_testing = pd.date_range(rg['start'], rg['end'], freq=rg['freq'])\n",
    "\n",
    "        elif config.test_split:\n",
    "            rg_testing = df.index[math.ceil(len(df) * (1 - config.test_split)):]\n",
    "        else:\n",
    "            rg_testing = None\n",
    "        df_testing = df[df.index.isin(rg_testing)]\n",
    "        testing_artifact = TSArtifact.from_df(\n",
    "            df_testing,\n",
    "            name=config.artifact_name, \n",
    "            missing_values_technique=config.missing_values_technique,\n",
    "            resampling_freq=config.resampling_freq, \n",
    "            normalize=False,\n",
    "            path=str(Path.home()/config.wandb_artifacts_path)\n",
    "        )\n",
    "        display(testing_artifact.metadata)\n",
    "        if df_testing.index.duplicated().any():\n",
    "            print(\"Duplicated values in dataframe index.\")\n",
    "        else:\n",
    "            print(\"No duplicated values in dataframe index\")\n",
    "    else:\n",
    "        if print_flag: print(\"rg \"+ str(rg) + \" | test_split \"+ str(config.test_split))\n",
    "        testing_artifact = None\n",
    "    return df_testing, testing_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing, testing_artifact = build_test_df(config, df_mod, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training + Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build dataframe & artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training + Testing data\n",
    "if(config.joining_train_test):\n",
    "    print(\"joining_train_test: \"+ str(config.joining_train_test))\n",
    "    df_train_test = pd.concat([df_training, df_testing])\n",
    "    train_test_artifact = TSArtifact.from_df(\n",
    "        df_train_test,\n",
    "        name=config.artifact_name, \n",
    "        missing_values_technique=config.missing_values_technique,\n",
    "        resampling_freq=config.resampling_freq, \n",
    "        normalize=False,\n",
    "        path=str(Path.home()/config.wandb_artifacts_path)\n",
    "    )\n",
    "    if df_train_test.index.duplicated().any():\n",
    "        print(\"Duplicated values in dataframe index.\")\n",
    "    else:\n",
    "        print(\"No duplicated values in dataframe index.\")\n",
    "    display(train_test_artifact.metadata)\n",
    "else:\n",
    "    train_test_artifact = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "> \n",
    "Before running this notebook part, make sure you have the `$WANDB_API_KEY`, `$WANDB_ENTITY` and `$WANDB_PROJECT` environment varibales defined with your API_KEY and your ENTITY and PROJECT names (run in a terminal `echo $WANDB_API_KEY` to see it, same with the other variables). If not, run in a terminal `wandb login [API_KEY]` to set the first one. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account. Run in a terminal `export WANDB_ENTITY=entity_name` and/or `export WANDB_PROJECT=project_name` to set the other two\n",
    "> \n",
    "> <span style=\"color:red\"> TODO: Modify config.ipynb so it gets wandb config from base.yml </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"01_dataset_artifact\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "print(\"runname: \"+runname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'online' if config.use_wandb else 'disabled'\n",
    "\n",
    "# Make the run that will produce the artifact\n",
    "with wandb.init(job_type='create_dataset', resume=True, mode=mode, config=config, name=runname) as run:\n",
    "    if testing_artifact: \n",
    "        run.log_artifact(training_artifact, aliases=['train'])\n",
    "        run.log_artifact(testing_artifact, aliases=['test'])\n",
    "        \n",
    "        if train_test_artifact:\n",
    "            run.log_artifact(train_test_artifact, aliases=['all'])\n",
    "    \n",
    "    else:\n",
    "        run.log_artifact(training_artifact, aliases=['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dvats.imports import beep\n",
    "print(\"Execution ended\")\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reset_kernel:\n",
    "    import os\n",
    "    os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
