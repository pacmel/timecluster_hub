{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9447285-810f-443c-b673-158fc7b757da",
   "metadata": {},
   "source": [
    "# Lag-Llama tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e329c1-bb5f-47b2-ba41-b538d23b632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6dde98b-7e6a-4124-8be8-79ba11ca91a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (/home/macu/.local/lib/python3.10/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdates\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmdates\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_evaluation_predictions, Evaluator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgluonts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepository\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/__init__.py:1694\u001b[0m\n\u001b[1;32m   1691\u001b[0m py_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;66;03m# Shared memory manager needs to know the exact location of manager executable\u001b[39;00m\n\u001b[0;32m-> 1694\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initExtension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_manager_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _manager_path\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;66;03m# Appease the type checker: it can't deal with direct setting of globals().\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;66;03m# Note that we will see \"too many\" functions when reexporting this way; there\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;66;03m# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;66;03m# so that this import is good enough\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/cuda/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dummy_type, _LazySeedTracker, classproperty\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_device_index\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     CUDAGraph,\n\u001b[1;32m     33\u001b[0m     graph,\n\u001b[1;32m     34\u001b[0m     graph_pool_handle,\n\u001b[1;32m     35\u001b[0m     is_current_stream_capturing,\n\u001b[1;32m     36\u001b[0m     make_graphed_callables,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Event, ExternalStream, Stream\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/cuda/graphs.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pytree\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dummy_type\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_CudaStreamBase\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Define dummy base classes\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/utils/_pytree.py:51\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict, deque, namedtuple, OrderedDict\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     Any,\n\u001b[1;32m     29\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     Union,\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     54\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreespec_pprint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m ]\n\u001b[1;32m     89\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (/home/macu/.local/lib/python3.10/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f6488-11fa-4e80-a425-3e5bb0d3749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, device, context_length=32, use_rope_scaling=False, num_samples=100):\n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device) # Uses GPU since in this Colab we use a GPU.\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    rope_scaling_arguments = {\n",
    "        \"type\": \"linear\",\n",
    "        \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "    }\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length, # Lag-Llama was trained with a context length of 32, but can work with any context length\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "        rope_scaling=rope_scaling_arguments if use_rope_scaling else None,\n",
    "\n",
    "        batch_size=1,\n",
    "        num_parallel_samples=100,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e90e21-97f4-4681-8e1e-ebffe689c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "url = (\n",
    "    \"https://gist.githubusercontent.com/rsnirwan/a8b424085c9f44ef2598da74ce43e7a3\"\n",
    "    \"/raw/b6fdef21fe1f654787fa0493846c546b7f9c4df2/ts_long.csv\"\n",
    ")\n",
    "df = pd.read_csv(url, index_col=0, parse_dates=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0160a9-3f48-4237-9b0b-cd5816d7591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set numerical columns as float32\n",
    "for col in df.columns:\n",
    "    # Check if column is not of string type\n",
    "    if df[col].dtype != 'object' and pd.api.types.is_string_dtype(df[col]) == False:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "# Create the Pandas\n",
    "dataset = PandasDataset.from_long_dataframe(df, target=\"target\", item_id=\"item_id\")\n",
    "\n",
    "backtest_dataset = dataset\n",
    "prediction_length = 24  # Define your prediction length. We use 24 here since the data is of hourly frequency\n",
    "num_samples = 100 # number of samples sampled from the probability distribution for each timestep\n",
    "device = torch.device(\"cuda:0\") # You can switch this to CPU or other GPUs if you'd like, depending on your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e7a38-4b8f-4861-8b53-5892c96016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, tss = get_lag_llama_predictions(backtest_dataset, prediction_length, device, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525dc1b-d730-4069-ae5c-e4af87780794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
