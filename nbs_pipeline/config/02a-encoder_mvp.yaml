include: !include "base.yaml"

configuration:
  job_type: 'encoder_MVP'
  alias: *alias
  wandb:
    mode: *wdb_mode
    # Whether to group this run in a wandb group (for sweeps)
    group: null
  specifications:
    batch_size: 32
    # Number of epochs to train for
    n_epoch: 100
    mask:
      # Mask future samplesx
      future: false
      # True: mask stateful samples, False: mask individual time steps
      stateful: true
      # (only for multivariate ts) mask all variables at once
      sync: false
    mvp:
      # Tuple (min_w, max_x) to train MVP with adaptable window sizes. Usually max_w = config.w
      # Set to null to train MVP with fixed window size
      ws1: 10
      ws2: 30
      # probability of masking in MVP
      r: 0.7
      valid_size: 0.2
      normalize:
        by_sample: false
        # Whether to use a single batch or not for the normalization (TSStandardize)
        use_single_batch: false
    sliding_windows:
      # n datapoints the window is moved ahead along the sequence in the sliding window
      stride: 1
      # This will set the percentage of items that go to val 
      # window size for the sliding window (taxi=48, steamflow=640)
      #w
      size: 30