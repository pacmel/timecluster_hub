{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "1) Probar con M-Toy y comprobar que, efectivamente, salga como más relevantes las dos primeras de las tres variables (0,1)\n",
    "2) Se puede probar con la serie temporal de BTC, pero no da valores para ve rlas dos relevantes, indica para diferentes periodos de tiempo cuáles son las más importantes (se puede añadir una celda para una tabla similar, no sería complicado, aunque habría que arreglar el acceso a los Artiffact antes para poder sacarlas o hacerlo a mano y buscar las 10 variables más importantes para cada forecasting horizon) -> Ver tabla 3 [BTCPaper](https://link.springer.com/article/10.1007/s00521-020-05129-6)\n",
    "3) Seguir buscando más artículos de ste tipo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer-wise Relevance Propagation\n",
    "\n",
    "> This notebooks has five main parts:\n",
    "> - Introduction to Layer-wise Relevance Propagation\n",
    "> - Dimensionality reduction\n",
    ">\n",
    ">   Following the steps of 04_dr notebooks.\n",
    ">   - Gets the embeddings (or latent space) from a vultivariate time series given by an encoder (e.g. autoencoder) \n",
    ">   - Uses the obtained embeddings as input for a dimensionality reduction algorithm, to generate projections of the embeddings. (As 04.. does)\n",
    ">\n",
    "> - Compute Clusters\n",
    ">   Following the steps of 04_dr notebooks. ¿Maybe it should also be in another notebook?\n",
    ">   - The projections are clustered via hdbscan \n",
    "> - Anomalies simple detector\n",
    ">   - Use basic statistics to obtain an anomaly score to visualize annomalies using a dynamic plot. Just for clarity.\n",
    "> - Layer-wise relevance propagation\n",
    ">   - Different implementations ti apply LRP to check the importance of each feature in the embeddings obtainment.\n",
    ">   - Same check for selecting some points in the projections plot (assuming random selection). Checks the importance for each feature in the obtainment of the associated part of the embeddings.\n",
    "\n",
    "> <span style=\"color:red; display:block;\">\n",
    ">  TODO: Save in 04_... an Artifact as in the previous nbs_pipeline notebooks and divide this notebooks in two (one for clustering and other for layer propagation. ¿Deberíamos separar también 04 en dos notebooks?\n",
    "> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Layer-wise Relevance Propagation\n",
    "Layer-wise Relevance Propagation is a XAI technique introduced in 2015 by [Bach et all](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140&ref=blog.paperspace.com) for vision computing deep learning models that has been extensively used in different DL domains for better explainability. \n",
    "\n",
    "This method belongs to the attribution methods cattegory. According to [Towards Better Understanding Attribution Methods](https://openaccess.thecvf.com/content/CVPR2022/html/Rao_Towards_Better_Understanding_Attribution_Methods_CVPR_2022_paper.html) and [Oportunities and Challengues in Explainable Artificial Intelligence (XAI): A Survey](https://arxiv.org/pdf/2006.11371.pdf), it can be classified into three main groups: backpropagation based methods, activation based methods and perturbation-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Mejorar/revisar\n",
    "la clasificación con https://arxiv.org/pdf/2006.11371.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation-based or Gradient-based methods\n",
    "These methods use backpropagation to calculate the relevance of input features, based on each feature's contribution to the model's output. Tipically rely on the gradients:\n",
    "- with respect to the input\n",
    "  - [DeepLift: Propagating Activation Differences](https://arxiv.org/abs/1704.02685). Decomposes the oitput prediction of a neural network on a especific input by backpropagating the activation of each neuron to its ```reference activation``` and assigns coontribution scores according to the difference.\n",
    "  - [Guided BackPropagation/Guided saliency](https://arxiv.org/abs/1412.6806). \"Variant of the [deconvolution approach](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) for visualizing features learned by CNNs, which can also be applied to a broad range of network structures. Under this approach, the use of max-pooling in convolutional neural networks for small images is questioned and the replacement of max-pooling layers by a convolutional layer with increased stride is proposed, resulting in no loss of accuracy on several image recognition benchmarks.\" \n",
    "- with respect to intermediate layers\n",
    "  - Saliency Maps. \"Visualizing gradients, neural activation of individual layers using DeConv nets, guided backpropagation, etc. as images.\"\n",
    "     - [NormGrad](https://openaccess.thecvf.com/content_CVPR_2020/html/Rebuffi_There_and_Back_Again_Revisiting_Backpropagation_Saliency_Methods_CVPR_2020_paper.html). \"Based on the spatial contribution of gradients of convolutional weights\"\n",
    "       - Saliency maps combination at different layers to test the ability of saliency methods to extract complementary information at different network levels\n",
    "        - Class-sensitivity metric and meta-learning inspired paradigm applicable to any saliency method for improving sensitivity to the output class being explained\n",
    "     - [Compute gradient of the class score with respect to the input case](https://arxiv.org/pdf/1312.6034.pdf). Visualisation techniques for Convolutional Networks used for classification.\n",
    "     - [Salient Deconvolutional Networks](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_8).\n",
    "  -  [Salient Relevance Maps](https://sciencedirect.com/science/article/pii/S0262885619300149)\n",
    "  - [Excitation Backprop](https://link.springer.com/article/10.1007/s11263-017-1059-x). Top-down attention of a CNN classifier for generating task-specific attention map.\n",
    "  \n",
    "  - [FullGrad: Full-Gradient Representation for NN Visualization](https://proceedings.neurips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html). Decomposes the NN response into input sensitivity and per-neuron sensitivity components. \n",
    "- With respect to the last layer\n",
    "  - [GradCam](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), [GradCam++](https://ieeexplore.ieee.org/abstract/document/8354201)\n",
    "- With respect to different layers\n",
    "  - [LayerCAM: Hierarchical Class Activation Maps](https://ieeexplore.ieee.org/abstract/document/9462463)\n",
    "  \n",
    "Also, LRP can be included as a backpropagation-based methods. But it focuses on propagating relevances with different rules instead on focusing on the gradients. \n",
    "- [LRP: Layer-wise Relevance Propagation](https://iphome.hhi.de/samek/pdf/MonXAI19.pdf). \"Operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation-based methods\n",
    "\n",
    "Weigh activation maps to assign importance of the final convolutional layer:\n",
    "  - weighted by their gradients:\n",
    "    - [GradCam](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), [GradCam++](https://ieeexplore.ieee.org/abstract/document/8354201)\n",
    "    - [LayerCAM: Hierarchical Class Activation Maps](https://ieeexplore.ieee.org/abstract/document/9462463)\n",
    "    - [Gap CAM: Using Global Average Pooling in CNNs for generating class activation maps](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html)\n",
    "  - Estimating their importance to the classification score \n",
    "    - [Ablation-cam](https://scholar.google.com/scholar?hl=es&as_sdt=0%2C5&q=Saurabh+Desai+and+Harish+G.+Ramaswamy.+Ablation-CAM%3A+Visual+Explanations+for+Deep+Convolutional+Network+via+Gradient-free+Localization.+In+WACV%2C+pages+983%E2%80%93991%2C+2020.&btnG=). Visual explanations for deep CNN via gradient-free localization. \"Uses Ablation analysis to determine the importance (weights) of individual feature map units wrt class. Hilights the important regions in the image for predicting the concept. ...\"\n",
    "    - [Flow restiction](https://arxiv.org/abs/2001.00396), Adds noise to intermediate feature maps to restrict the flow of information and quantify how much information image regions provide.\n",
    "<span style=\"color:red\">TODO: quizá en TS largas pueda tener sentido algo del estilo aplicar algoritmos menos pesados, comprobar las secciones más relevantes y reducir la serie temporal seleccionando sólo las partes más importantes. </span>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation-based methods\n",
    "\n",
    "This methods treat the network as a black-box and assign importance by observing the change in output on perturbing the input. The explanation is generated by iteratively probing a trained ML model with those variations of the input. This can be done using different techniques:\n",
    "\n",
    "- Occluding parts of the image\n",
    "  - [Rise: Randomised Input Sampling for Explanation of black-box models](https://arxiv.org/abs/1806.07421). \"Generates an importance map indicating how salient each pixel is for the model's prediction.\" \"Estimates importance empirically by proving the model with randomly masked versions of the input image and obtaining the corresponding outputs.\"\n",
    "  -  [LIME: Learning and Interpretable Model locally around the prediction](https://dl.acm.org/doi/abs/10.1145/2939672.2939778). \"Explains the prediction of any classifier in an interpretable and faithful manner\"\n",
    "  - [Loss derivative back-propagation](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). Classification of labelled images. \"We train these models using a large set of N labeled images {x,y}, where label y_i is a discrete variable indicating the true class. A cross-entropy loss function, suoitable for image classification, is used to compare ~y_i and u_i. The parameters of the networks are trained by back-propagating the derivative of the loss with respect to the parameters throghout the network, and updating the parameters via stochastic gradient descent\"\n",
    "  - [DeconvNet: DeConvolution networks for convolution visualizations](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). Activate the neourons of individual layers by occluding input instance and visualizing using DeConv\n",
    "Nets.\n",
    "- Optimising for a mask that maximizes/minimizes class confidence\n",
    "  - [Real Time Image Saliency for black box classifiers](https://proceedings.neurips.cc/paper_files/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html). Saliency detection method. \n",
    "  - [Interpretable explanation of black boxes by Meaningful Perturbation](https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html). Goal: find the part of an image most responsible for a classifier decision. Model-agnostic and testable. Interpretable image perturbations.\n",
    "\n",
    "- Selecting specific features\n",
    "   - [SHAP: A Unified Approach to Interpreting Model Predictions\n",
    "](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html) proves feature correlations by removing features in a game theoretic framework.\n",
    "   - [Prediction Difference Analysis](https://arxiv.org/abs/1702.04595). Remove individual fetures  and finds the positive and negative correlation of individual features towards the output\n",
    "- Features replacements. [Interpreting Black Box Models via Hypothesis Testing\n",
    "](https://dl.acm.org/doi/abs/10.1145/3412815.3416889). \"Conterfactual replacements of features to study feature importance. \n",
    "> TODO: REVISAR el artículo principal. Propone un modo de evaluar algoritmos de atribución. ¿Merece la pena revisarlo? Creo que se sale del scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Generate projectsion of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight & Biases\n",
    "import wandb\n",
    "\n",
    "#Yaml\n",
    "from yaml import load, FullLoader\n",
    "\n",
    "#Embeddings\n",
    "from dvats.all import *\n",
    "from tsai.data.preparation import prepare_forecasting_data\n",
    "from tsai.data.validation import get_forecasting_splits\n",
    "from fastcore.all import *\n",
    "\n",
    "#Dimensionality reduction\n",
    "from tsai.imports import *\n",
    "\n",
    "#Clustering\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory_usage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 1\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m0%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage:\n",
    "    import nbs_pipeline.utils.memory as mem\n",
    "    import torch \n",
    "    gpu_device = torch.cuda.current_device()\n",
    "    mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get W&B API\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put here everything that could be needed if this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Change to config.py & tml version when fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buscando una combinación buena\n",
    "cpu_flag = False\n",
    "\n",
    "if cpu_flag:\n",
    "    n_neighbors = 15\n",
    "    min_dist = 0.1\n",
    "else: \n",
    "    n_neighbors = 15 #200\n",
    "    min_dist = 0.1 #0.0001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    use_wandb = False, # Whether to use or not wandb for experiment tracking\n",
    "    wandb_group = None, # Whether to group this run in a wandb group\n",
    "    wandb_entity = os.environ['WANDB_ENTITY'], # The entity to use for wandb,\n",
    "    wandb_project = os.environ['WANDB_PROJECT'], # The project to use for wandb,\n",
    "    dr_artifact_name = None, # * Set to None for using the default one (encoder validation set)\n",
    "    enc_artifact = 'mi-santamaria/deepvats/mvp-SWV:latest', # Name:version of the encoder artifact\n",
    "    n_neighbors = n_neighbors, #15, #UMAP\n",
    "    min_dist = min_dist, #0.1, #UMAP,\n",
    "    random_state = int(1234), # UMAP\n",
    "    metric = 'euclidean',\n",
    "    cpu = cpu_flag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model needs to restore the encoder model fitted in the notebook `02x`, as well as the data and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"04_dimensionality_reduction\"\n",
    "runname = name\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /home/macu/work/nbs_pipeline/04_dimensionality_reduction.ipynb.\n"
     ]
    }
   ],
   "source": [
    "run_dr = wandb.init(\n",
    "    entity=config.wandb_entity,\n",
    "    project=config.wandb_project if config.use_wandb else 'work-nbs', \n",
    "    group=config.wandb_group,\n",
    "    allow_val_change=True, \n",
    "    job_type='dimensionality_reduction', \n",
    "    mode='online' if config.use_wandb else 'disabled',\n",
    "    anonymous = 'never' if config.use_wandb else 'must',\n",
    "    config=config,\n",
    "    resume = 'allow',\n",
    "    name = runname\n",
    "    #resume=False\n",
    ")\n",
    "config_dr = wandb.config # Object for storing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botch to use artifacts offline\n",
    "artifacts_gettr = run.use_artifact if config_dr.use_wandb else api.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the encoder model and its associated configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 0.71,\n",
       " 'w': 1008,\n",
       " 'MVP': {'r': 0.71,\n",
       "  'lm': 3.0,\n",
       "  'crit': None,\n",
       "  'sync': False,\n",
       "  'fname': 'encoder_MVP',\n",
       "  'dropout': 0.1,\n",
       "  'verbose': False,\n",
       "  'stateful': True,\n",
       "  'save_best': True,\n",
       "  'nan_to_num': 0,\n",
       "  'custom_mask': None,\n",
       "  'future_mask': False,\n",
       "  'weights_path': None,\n",
       "  'variable_mask': False,\n",
       "  'subsequence_mask': True},\n",
       " 'ref': {'hash': '854438143855504315',\n",
       "  'type': \"<class 'fastai.learner.Learner'>\"},\n",
       " 'freq': '1s',\n",
       " 'alias': 'toy',\n",
       " 'n_inp': 1,\n",
       " 'device': 'cuda',\n",
       " 'epochs': 100,\n",
       " 'frozen': False,\n",
       " 'mvp_ws': [10, 1008],\n",
       " 'stride': 1,\n",
       " 'Learner': {'lr': 0.001,\n",
       "  'wd': None,\n",
       "  'arch': 'tsai.models.InceptionTimePlus.InceptionTimePlus',\n",
       "  'moms': [0.95, 0.85, 0.95],\n",
       "  'path': '.',\n",
       "  '_name': '<fastai.learner.Learner object at 0x7f092d6356c0>',\n",
       "  'metrics': None,\n",
       "  'opt_func': 'fastai.optimizer.Adam',\n",
       "  'splitter': 'tsai.models.utils.ts_splitter',\n",
       "  'train_bn': True,\n",
       "  'loss_func': {'axis': -1,\n",
       "   '_name': {'axis': -1,\n",
       "    '_name': 'FlattenedLoss of MSELoss()',\n",
       "    'is_2d': False,\n",
       "    'flatten': True,\n",
       "    'floatify': True},\n",
       "   'is_2d': False,\n",
       "   'flatten': True,\n",
       "   'floatify': True},\n",
       "  'model_dir': 'models',\n",
       "  'wd_bn_bias': False,\n",
       "  'default_cbs': True},\n",
       " 'Recorder': {'add_time': True, 'train_metrics': False, 'valid_metrics': True},\n",
       " 'time_col': None,\n",
       " 'ShowGraph': {'perc': 0.5, 'final_losses': True, 'plot_metrics': True},\n",
       " 'data_cols': [],\n",
       " 'mask_sync': False,\n",
       " 'use_wandb': True,\n",
       " 'batch size': 512,\n",
       " 'batch_size': 512,\n",
       " 'csv_config': {},\n",
       " 'frozen idx': 0,\n",
       " 'valid_size': 0.2,\n",
       " 'mask_future': False,\n",
       " 'wandb_group': None,\n",
       " 'CastToTensor': True,\n",
       " 'dataset.tfms': '[ToFloat:\\nencodes: (Tensor,object) -> encodes\\n(object,object) -> encodes\\ndecodes: (object,object) -> decodes\\n, None]',\n",
       " 'WandbCallback': {'log': None,\n",
       "  'seed': 12345,\n",
       "  'n_preds': 36,\n",
       "  'reorder': True,\n",
       "  'valid_dl': None,\n",
       "  'log_model': False,\n",
       "  'log_preds': False,\n",
       "  'model_name': None,\n",
       "  'log_dataset': False,\n",
       "  'dataset_name': None,\n",
       "  'log_preds_every_epoch': False},\n",
       " 'analysis_mode': 'online',\n",
       " 'artifact_name': 'toy',\n",
       " 'input 1 dim 1': 512,\n",
       " 'input 1 dim 2': 5,\n",
       " 'input 1 dim 3': 1008,\n",
       " 'mask_stateful': True,\n",
       " 'ParamScheduler': True,\n",
       " 'dls.after_item': 'Pipeline: ',\n",
       " 'norm_by_sample': False,\n",
       " 'train_artifact': 'mi-santamaria/deepvats/toy:latest',\n",
       " 'valid_artifact': None,\n",
       " 'batch per epoch': 361,\n",
       " 'dls.after_batch': 'Pipeline: TSStandardize',\n",
       " 'ProgressCallback': True,\n",
       " 'dls.before_batch': 'Pipeline: ',\n",
       " 'model parameters': 456261,\n",
       " 'TrainEvalCallback': True,\n",
       " 'EarlyStoppingCallback': True,\n",
       " 'norm_use_single_batch': False,\n",
       " 'norm_use_by_single_batch': [False]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_artifact = artifacts_gettr(config.enc_artifact, type='learner')\n",
    "enc_artifact.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mvp-SWV:v55'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_artifact.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "# TODO: This only works when you run it two timeS! WTF?\n",
    "try:\n",
    "    enc_learner = enc_artifact.to_obj()\n",
    "except:\n",
    "    enc_learner = enc_artifact.to_obj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the dataset artifact used for training the encoder. Even if we do not compute the dimensionality reduction over this dataset, we need to know the metadata of the encoder training set, to check that \n",
    "it matches with the dataset that we want to reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_artifact_train:  toy:v1\n"
     ]
    }
   ],
   "source": [
    "enc_logger = enc_artifact.logged_by()\n",
    "enc_artifact_train = artifacts_gettr(enc_logger.config['train_artifact'], type='dataset')\n",
    "if enc_logger.config['valid_artifact'] is not None:\n",
    "    enc_artifact_valid = artifacts_gettr(enc_logger.config['valid_artifact'], type='dataset')\n",
    "    print(\"enc_artifact_valid:\", enc_artifact_valid.name)\n",
    "print(\"enc_artifact_train: \", enc_artifact_train.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the dataset artifact that we want to use for the reduction. If no artifact is defined, the artifact to reduce will be the one used for validate the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy:v1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config_dr.dr_artifact_name is not None:\n",
    "    dr_artifact = artifacts_gettr(config_dr.dr_artifact_name)\n",
    "else:\n",
    "    dr_artifact = enc_artifact_train\n",
    "dr_artifact.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to check whether the artifact that is going to be used fort the dimensionality reduction matches the artifact used to train the encoder. Matching means having the same number of variables, the same window size and stride, and the same frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-01 00:00:00</th>\n",
       "      <td>5714.045004</td>\n",
       "      <td>3535.867064</td>\n",
       "      <td>3382.041342</td>\n",
       "      <td>1191.078014</td>\n",
       "      <td>315.915504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 00:00:01</th>\n",
       "      <td>5360.189078</td>\n",
       "      <td>3383.499028</td>\n",
       "      <td>3288.315794</td>\n",
       "      <td>1219.589472</td>\n",
       "      <td>306.245864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 00:00:02</th>\n",
       "      <td>5014.835118</td>\n",
       "      <td>3655.527552</td>\n",
       "      <td>3172.329022</td>\n",
       "      <td>1119.173498</td>\n",
       "      <td>305.762576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 00:00:03</th>\n",
       "      <td>4602.755516</td>\n",
       "      <td>3510.446636</td>\n",
       "      <td>3020.312986</td>\n",
       "      <td>1016.407248</td>\n",
       "      <td>295.602196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-01 00:00:04</th>\n",
       "      <td>4285.179828</td>\n",
       "      <td>3294.697156</td>\n",
       "      <td>2918.082882</td>\n",
       "      <td>923.499578</td>\n",
       "      <td>290.447070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0            1            2            3  \\\n",
       "2002-01-01 00:00:00  5714.045004  3535.867064  3382.041342  1191.078014   \n",
       "2002-01-01 00:00:01  5360.189078  3383.499028  3288.315794  1219.589472   \n",
       "2002-01-01 00:00:02  5014.835118  3655.527552  3172.329022  1119.173498   \n",
       "2002-01-01 00:00:03  4602.755516  3510.446636  3020.312986  1016.407248   \n",
       "2002-01-01 00:00:04  4285.179828  3294.697156  2918.082882   923.499578   \n",
       "\n",
       "                              4  \n",
       "2002-01-01 00:00:00  315.915504  \n",
       "2002-01-01 00:00:01  306.245864  \n",
       "2002-01-01 00:00:02  305.762576  \n",
       "2002-01-01 00:00:03  295.602196  \n",
       "2002-01-01 00:00:04  290.447070  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dr_artifact.to_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232272, 5)\n",
      "0.2\n",
      "232.3\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(np.round(df.shape[0]/ 1e6, 1))\n",
    "print(np.round(df.shape[0]/ 1e3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_time_series_flag = False\n",
    "if show_time_series_flag:\n",
    "    # Show time series plot\n",
    "    fig, ax = plt.subplots(1, figsize=(15,5), )\n",
    "    cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "    df.plot(color=cmap(0.05), ax=ax) # or use colormap=cmap\n",
    "    # rect = Rectangle((5000, -4.2), 3000, 8.4, facecolor='lightgrey', alpha=0.5)\n",
    "    # ax.add_patch(rect)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 1\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m0%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW start |  1706259496.6843097  | end  1706259496.6850264 total (secs):  0.0007166862487792969\n",
      "(231264, 5, 1008)\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "enc_input, _ = prepare_forecasting_data(df, fcst_history = w)\n",
    "t_end = time.time()\n",
    "t = t_end - t_start\n",
    "print(\"SW start | \" , t_start, \" | end \", t_end, \"total (secs): \", t)\n",
    "print(enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 1\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m0%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = enc_logger.config['stride']\n",
    "batch_size = enc_logger.config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(stride)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231264, 5, 1008)\n",
      "mvp-SWV:v55\n"
     ]
    }
   ],
   "source": [
    "print(enc_input.shape)\n",
    "print(enc_artifact.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 1\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m0%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_max = 10000000\n",
    "shape = enc_input.shape\n",
    "chunk_size_ = min(shape[1]*shape[2],chunk_max/(shape[1]*shape[2]))\n",
    "N = max(3200,np.floor(chunk_size_/32))\n",
    "chunk_size = N*32\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'torch.cuda.OutOfMemoryError'>",
     "evalue": "CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 23.69 GiB total capacity; 23.02 GiB already allocated; 29.88 MiB free; 23.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m embs \u001b[38;5;241m=\u001b[39m get_enc_embs_set_stride_set_batch_size(\n\u001b[1;32m      3\u001b[0m     enc_input, \n\u001b[1;32m      4\u001b[0m     enc_learner, \n\u001b[1;32m      5\u001b[0m     stride \u001b[38;5;241m=\u001b[39m stride,\n\u001b[1;32m      6\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[1;32m      7\u001b[0m     cpu\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcpu,\n\u001b[1;32m      8\u001b[0m     to_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m     print_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     time_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m chunk_size\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m t_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m t_end\u001b[38;5;241m-\u001b[39mt_start\n",
      "File \u001b[0;32m~/work/dvats/encoder.py:259\u001b[0m, in \u001b[0;36mget_enc_embs_set_stride_set_batch_size\u001b[0;34m(X, enc_learn, stride, batch_size, module, cpu, average_seq_dim, to_numpy, print_flag, time_flag, chunk_size, check_memory_usage)\u001b[0m\n\u001b[1;32m    257\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    258\u001b[0m chunk \u001b[38;5;241m=\u001b[39m [batch \u001b[38;5;28;01mfor\u001b[39;00m (n, batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aux_dl) \u001b[38;5;28;01mif\u001b[39;00m (chunk_size\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n  \u001b[38;5;129;01mand\u001b[39;00m chunk_size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m n) ]\n\u001b[0;32m--> 259\u001b[0m chunk_embs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    260\u001b[0m     get_acts_and_grads(\n\u001b[1;32m    261\u001b[0m         model\u001b[38;5;241m=\u001b[39menc_learn\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    262\u001b[0m         modules\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    263\u001b[0m         x\u001b[38;5;241m=\u001b[39mxb[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m    264\u001b[0m         cpu\u001b[38;5;241m=\u001b[39mcpu\n\u001b[1;32m    265\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[1;32m    267\u001b[0m ]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Mueve los embeddings del bloque a la CPU\u001b[39;00m\n\u001b[1;32m    269\u001b[0m chunk_embs \u001b[38;5;241m=\u001b[39m [emb\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m chunk_embs]\n",
      "File \u001b[0;32m~/work/dvats/encoder.py:260\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    258\u001b[0m chunk \u001b[38;5;241m=\u001b[39m [batch \u001b[38;5;28;01mfor\u001b[39;00m (n, batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aux_dl) \u001b[38;5;28;01mif\u001b[39;00m (chunk_size\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n  \u001b[38;5;129;01mand\u001b[39;00m chunk_size\u001b[38;5;241m*\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m n) ]\n\u001b[1;32m    259\u001b[0m chunk_embs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mget_acts_and_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_learn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[1;32m    267\u001b[0m ]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Mueve los embeddings del bloque a la CPU\u001b[39;00m\n\u001b[1;32m    269\u001b[0m chunk_embs \u001b[38;5;241m=\u001b[39m [emb\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m chunk_embs]\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/tsai/models/explainability.py:26\u001b[0m, in \u001b[0;36mget_acts_and_grads\u001b[0;34m(model, modules, x, y, detach, cpu)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hook_outputs(modules, grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detach\u001b[38;5;241m=\u001b[39mdetach, cpu\u001b[38;5;241m=\u001b[39mcpu) \u001b[38;5;28;01mas\u001b[39;00m h_grad:\n\u001b[1;32m     25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()(x)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     28\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1530\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__str__\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): types \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[38;5;241m=\u001b[39m _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res\u001b[38;5;241m.\u001b[39mset_meta(dict_objs[\u001b[38;5;241m0\u001b[39m],as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/_tensor.py:1278\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1278\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 23.69 GiB total capacity; 23.02 GiB already allocated; 29.88 MiB free; 23.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "embs = get_enc_embs_set_stride_set_batch_size(\n",
    "    enc_input, \n",
    "    enc_learner, \n",
    "    stride = stride,\n",
    "    batch_size = batch_size,\n",
    "    cpu=config.cpu,\n",
    "    to_numpy = True, \n",
    "    print_flag = False,\n",
    "    time_flag = True,\n",
    "    chunk_size = chunk_size\n",
    ")\n",
    "t_end = time.time()\n",
    "t = t_end-t_start\n",
    "print(\"GE start | \" , t_start, \" | end \", t_end, \"total (secs): \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[0,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = time.time()\n",
    "t = t_end-t_start\n",
    "print(\"GE start | \" , t_start, \" | end (next cell) \", t_end, \"total (secs): \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dr.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel (Debugging code 4 analysing where can app be failing. Expecting to be related to GPU mem ussage)\n",
    "#os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embs.shape)\n",
    "print(enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensions check\n",
    "num_inputs = np.ceil(enc_input.shape[0]/stride)\n",
    "num_embs = embs.shape[0]\n",
    "test_eq(num_inputs, num_embs )\n",
    "print(num_inputs, num_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average embeddings in the time dimension, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use DR techniques to provide an alternative view for users to visually analyze and explore the time-series data. The algorithm UMAP shows its high competitiveness compared to t-SNE. t-SNE suffers from some limitations such as loss of large-scale information (the inter-cluster relationships). UMAP has a faster runtime and provides better scaling which helps to gain a meaningful organization of clusters, outliers and the preservation of continuums compared to t-SNE\n",
    "\n",
    "For this part of the implementation, the package [umap-learn](https://github.com/lmcinnes/umap) is used. The input of the algoritm is the $n \\times \\delta$ that contains, for each slice of the time series, the corresponding $\\delta$ latent embeddings given by the encoder.\n",
    "\n",
    "The hyperparameters of UMAP are given values by default here. If the value has been already set previously, that means this notebook is being called from a wandb sweep, and we must use the value that the sweep is bringing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cpu_flag:\n",
    "    umap_params = {\n",
    "        'n_neighbors' : config_dr.n_neighbors,\n",
    "        'min_dist' : config_dr.min_dist,\n",
    "        'random_state': np.uint64(1234), \n",
    "        'metric': config_dr.metric,\n",
    "        'a': 1.5769434601962196,\n",
    "        'b': 0.8950608779914887,\n",
    "        #'metric_kwds': {'p': 2}, #No debería ser necesario, just in case\n",
    "        'output_metric': 'euclidean',\n",
    "        'verbose': 4,\n",
    "        'n_epochs': 200\n",
    "    }\n",
    "else:\n",
    "    umap_params = {\n",
    "        'n_neighbors' : config_dr.n_neighbors,\n",
    "        'min_dist' : config_dr.min_dist,\n",
    "        'random_state': np.uint64(1234), \n",
    "        'metric': config_dr.metric,\n",
    "        'a': 1.5769434601962196,\n",
    "        'b': 0.8950608779914887,\n",
    "        'target_metric': 'euclidean',\n",
    "        'target_n_neighbors': config_dr.n_neighbors,\n",
    "        'verbose': 6,\n",
    "        'n_epochs': 200\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "check_reducer = True\n",
    "import utils.config as cfg_\n",
    "if check_reducer:\n",
    "    import umap\n",
    "    import cuml\n",
    "    print(\"-- umap.UMAP --\", cpu_flag)\n",
    "    reducer1 = umap.UMAP(force_approximation_algorithm = True, **umap_params)\n",
    "    print(\"-- cuml.UMAP --\", cpu_flag)\n",
    "    reducer2 = cuml.UMAP(**umap_params)\n",
    "    print(\"------- reducer1 --------\")\n",
    "    print(reducer1)\n",
    "    params1=AttrDict(reducer1.get_params())\n",
    "    print(\"------- reducer2 --------\")\n",
    "    print(reducer2)\n",
    "    params2=AttrDict(reducer2.get_params())\n",
    "    print(\"------- reducer2 --------\")\n",
    "    cfg_.diff_attrdict(dict_original=params2, dict_modified=params1, both=True)\n",
    "    beep(0.10)\n",
    "\n",
    "#if not cpu_flag:\n",
    "    #umap_params['force_approximation_algorithm'] = True # Este no parece ser el influencer\n",
    "    #umap_params['angular_rp_forest'] = True #No es el influencer\n",
    "\n",
    "#if params2['random_state'] != 1234:\n",
    " #   raise Exception(\"Wrong random_state params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure no nan ((Intento de Macu. La celda de comentada abajo es la original. Pero falla por Nan con sunspot))\n",
    "embs_no_nan = embs[~np.isnan(embs).any(axis=1)]\n",
    "embs_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "umap_params_cpu = {\n",
    "        'n_neighbors' : config_dr.n_neighbors,\n",
    "        'min_dist' : config_dr.min_dist,\n",
    "        'random_state': np.uint64(1234), \n",
    "        'metric': config_dr.metric,\n",
    "        #'a': 1.5769434601962196,\n",
    "        #'b': 0.8950608779914887,\n",
    "        #'metric_kwds': {'p': 2}, #No debería ser necesario, just in case\n",
    "        #'output_metric': 'euclidean',\n",
    "        'verbose': 4,\n",
    "        #'n_epochs': 200\n",
    "    }\n",
    "umap_params_gpu = {\n",
    "        'n_neighbors' : config_dr.n_neighbors,\n",
    "        'min_dist' : config_dr.min_dist,\n",
    "        'random_state': np.uint64(1234), \n",
    "        'metric': config_dr.metric,\n",
    "        'a': 1.5769434601962196,\n",
    "        'b': 0.8950608779914887,\n",
    "        'target_metric': 'euclidean',\n",
    "        'target_n_neighbors': config_dr.n_neighbors,\n",
    "        'verbose': 4, #6, #CUML_LEVEL_TRACE\n",
    "        'n_epochs': 200*3*2,\n",
    "        'init': 'random',\n",
    "        'hash_input': True\n",
    "    }\n",
    "\n",
    "\n",
    "if cpu_flag:\n",
    "    umap_params = umap_params_cpu\n",
    "else:\n",
    "    umap_params = umap_params_gpu\n",
    "\n",
    "umap_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prjs_pca = get_PCA_prjs(\n",
    "    X = embs_no_nan, \n",
    "    cpu = cpu_flag, #config_dr.cpu, \n",
    "    print_flag = True, \n",
    "    #target_weight=1,\n",
    "    **umap_params\n",
    ")\n",
    "prjs_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prjs_umap = get_UMAP_prjs(\n",
    "    input_data = prjs_pca, \n",
    "    cpu = cpu_flag, #config_dr.cpu, \n",
    "    print_flag = True, \n",
    "    #target_weight=1,\n",
    "    **umap_params\n",
    ")\n",
    "prjs_umap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(0.15)\n",
    "beep(0.15)\n",
    "beep(0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prjs_umap[0:10] # En R head(res[1,],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prjs = get_UMAP_prjs(embs, cpu=False, **umap_params)\n",
    "#prjs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the projections as an artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_wandb: \n",
    "    run.log_artifact(ReferenceArtifact(prjs, 'projections', type='projections', \n",
    "metadata=dict(run_dr.config)), aliases=f'run-{run.project}-{run.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Precomputed Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate precomputed clusters into the embedding space, it's necessary to log artifacts that include the labels of the newly created clusters. \n",
    "\n",
    "The cluster creation process is presented below. This creation procedure can be modified according to specific needs. However, the structure of the new artifact must be preserved (it must be a numpy.ndarray and the number of elements must be equal to the number of points in the embedding space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'HDBSCAN supported metrics: {list(hdbscan.dist_metrics.METRIC_MAPPING.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HDBSCAN parameters\n",
    "hdbscan_kwargs = {\n",
    "    'min_cluster_size' : 7, #100, #100,\n",
    "    'min_samples' : 3,\n",
    "    'cluster_selection_epsilon' : 0.0001,\n",
    "}\n",
    "metric_kwargs = {\n",
    "    'metric' : 'euclidean' #'jaccard'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters using HDBSCAN\n",
    "clusters = hdbscan.HDBSCAN(**hdbscan_kwargs, **metric_kwargs).fit(prjs_umap)\n",
    "clusters_labels = clusters.labels_\n",
    "list(Counter(clusters_labels).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cluster score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cluster_score(prjs_umap, clusters_labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing artifact structure \n",
    "test_eq_type(type(clusters_labels), np.ndarray)\n",
    "test_eq(clusters_labels.size, prjs_umap.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and log 'clusters_labels' artifact\n",
    "clusters_ar = ReferenceArtifact(obj=clusters_labels, name='clusters_labels')\n",
    "clusters_ar.metadata, clusters_ar.manifest.entries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dr.log_artifact(clusters_ar, aliases=['hdbscan_jaccard'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the connected scatter plot is a simple visualization technique, it has very specific functions in our approach. Every sliding window is represented as a dot in the plot after the projection process (Fig. 4C, D of the paper). Before labeling, all points have the same color and transparency, and when they are concentrated in one area, the densities are accumulated. Lines are used to connect consecutive points preserving the temporal ordering of the data and allowing the user to see temporal connections (Fig. 4B of the paper). Thus, the point is linked to the previous point (inner) and to the posterior point (outer) as an indication of the flow of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_projections(prjs, umap_params, fig_size = (25,25)):\n",
    "    \"Plot 2D projections thorugh a connected scatter plot\"\n",
    "    df_prjs = pd.DataFrame(prjs, columns = ['x1', 'x2'])\n",
    "    fig = plt.figure(figsize=(fig_size[0],fig_size[1]))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(df_prjs['x1'], df_prjs['x2'], marker='o', facecolors='none', edgecolors='b', alpha=0.1)\n",
    "    ax.plot(df_prjs['x1'], df_prjs['x2'], alpha=0.5, picker=1)\n",
    "    plt.title('DR params -  n_neighbors:{:d} min_dist:{:f}'.format(\n",
    "        umap_params['n_neighbors'],umap_params['min_dist']))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_projections_clusters(prjs, clusters_labels, umap_params, fig_size = (25,25)):\n",
    "    \"Plot 2D projections thorugh a connected scatter plot\"\n",
    "    df_prjs = pd.DataFrame(prjs, columns = ['x1', 'x2'])\n",
    "    df_prjs['cluster'] = clusters_labels\n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_size[0],fig_size[1]))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Create a scatter plot for each cluster with different colors\n",
    "    unique_labels = df_prjs['cluster'].unique()\n",
    "    print(unique_labels)\n",
    "    for label in unique_labels:\n",
    "        cluster_data = df_prjs[df_prjs['cluster'] == label]\n",
    "        ax.scatter(cluster_data['x1'], cluster_data['x2'], label=f'Cluster {label}')\n",
    "        #ax.scatter(df_prjs['x1'], df_prjs['x2'], marker='o', facecolors='none', edgecolors='b', alpha=0.1)\n",
    "    \n",
    "    #ax.plot(df_prjs['x1'], df_prjs['x2'], alpha=0.5, picker=1)\n",
    "    plt.title('DR params -  n_neighbors:{:d} min_dist:{:f}'.format(\n",
    "        umap_params['n_neighbors'],umap_params['min_dist']))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prjs_plt = plot_projections_clusters(prjs_umap, clusters_labels, umap_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(0.25)\n",
    "beep(0.25)\n",
    "beep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prjs_plt = plot_projections(prjs_umap, umap_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log this plot as part of the current wandb run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# Get the figure of the embedding plot, and save it on thea wandb run.\n",
    "run_dr.log({\"img\": [wandb.Image(prjs_plt.get_figure(), caption=\"dr_projections_plot\")]})\n",
    "\n",
    "#run_dr.log({'embeddings_plot': embeddings_plot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "run_dr.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomalies simple detector: dynamic plot for determining whether a window of time series is anomalous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters using HDBSCAN\n",
    "clusters = hdbscan.HDBSCAN(**hdbscan_kwargs, **metric_kwargs).fit(prjs_umap)\n",
    "clusters_labels = clusters.labels_\n",
    "list(Counter(clusters_labels).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_stats(data, labels):\n",
    "    \"\"\"Computes the media and the standard deviation for every cluster.\"\"\"\n",
    "    cluster_stats = {}\n",
    "    for label in np.unique(labels):\n",
    "        #members = data[labels == label]\n",
    "        members = data\n",
    "        mean = np.mean(members, axis = 0)\n",
    "        std = np.std(members, axis = 0)\n",
    "        cluster_stats[label] = (mean, std)\n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_score(point, cluster_stats, label):\n",
    "    \"\"\"Computes an anomaly score for each point.\"\"\"\n",
    "    mean, std = cluster_stats[label]\n",
    "    return np.linalg.norm((point - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector(data, labels):\n",
    "    \"\"\"Anomaly detection function.\"\"\"\n",
    "    cluster_stats = calculate_cluster_stats(data, labels)\n",
    "    scores = []\n",
    "    for point, label in zip(data, labels):\n",
    "        score = anomaly_score(point, cluster_stats, label)\n",
    "        scores.append(score)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = detector(prjs_umap, clusters_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check anomaly scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_scores_distribution(anomaly_scores):\n",
    "    \"Plot the distribution of anomaly scores to check for normality\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(anomaly_scores, kde=True, bins=30)\n",
    "    plt.title(\"Distribución de Anomaly Scores\")\n",
    "    plt.xlabel(\"Anomaly Score\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plot_anomaly_scores_distribution(anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_scores.shape)\n",
    "print(\"min \", np.min(anomaly_scores))\n",
    "print(\"max \", np.max(anomaly_scores))\n",
    "anomaly_scores_mean = np.mean(anomaly_scores)\n",
    "print(\"media \", anomaly_scores_mean)\n",
    "anomaly_scores_std = np.std(anomaly_scores)\n",
    "print(\"std \", anomaly_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_std = 2\n",
    "threshold = anomaly_scores_mean + num_std * anomaly_scores_std\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_clusters_with_anomalies(prjs, clusters_labels, anomaly_scores, threshold, fig_size=(25, 25)):\n",
    "    \"Plot 2D projections of clusters and superimpose anomalies\"\n",
    "    df_prjs = pd.DataFrame(prjs, columns=['x1', 'x2'])\n",
    "    df_prjs['cluster'] = clusters_labels\n",
    "    df_prjs['anomaly'] = anomaly_scores > threshold\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_size[0], fig_size[1]))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot each cluster with different colors\n",
    "    unique_labels = df_prjs['cluster'].unique()\n",
    "    for label in unique_labels:\n",
    "        cluster_data = df_prjs[df_prjs['cluster'] == label]\n",
    "        ax.scatter(cluster_data['x1'], cluster_data['x2'], label=f'Cluster {label}', alpha=0.7)\n",
    "\n",
    "    # Superimpose anomalies\n",
    "    anomalies = df_prjs[df_prjs['anomaly']]\n",
    "    ax.scatter(anomalies['x1'], anomalies['x2'], color='red', label='Anomalies', edgecolor='k', s=50)\n",
    "\n",
    "    plt.title('Clusters and anomalies')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "fig_size = (7,7)\n",
    "def update_plot(threshold):\n",
    "    plot_clusters_with_anomalies(prjs_umap, clusters_labels, anomaly_scores, threshold, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_slider = widgets.FloatSlider(value=2, min=1, max=3, step=0.01, description='Threshold')\n",
    "widgets.interactive(update_plot, threshold=threshold_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using quartiles for the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_using_iqr(scores):\n",
    "    # First and third quartiles\n",
    "    Q1 = np.percentile(scores, 25)\n",
    "    Q3 = np.percentile(scores, 75)\n",
    "    # IQR range\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    #Limits for anomalies\n",
    "    factor = 1.5\n",
    "    lower_bound = Q1 - (factor * IQR)\n",
    "    upper_bound = Q3 + (factor * IQR)\n",
    "    \n",
    "    # Get anomalies\n",
    "    anomalies = (scores < lower_bound) | (scores > upper_bound)\n",
    "    \n",
    "    return anomalies, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies, lower_bound, upper_bound = detect_anomalies_using_iqr(anomaly_scores)\n",
    "\n",
    "print(\"Lower Bound for Anomalies:\", lower_bound)\n",
    "print(\"Upper Bound for Anomalies:\", upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_anomalies(data, anomalies):\n",
    "    plt.scatter(data[:, 0], data[:, 1], color='blue', label='Normal Data')\n",
    "    plt.scatter(data[anomalies, 0], data[anomalies, 1], color='red', label='Anomalies')\n",
    "    plt.title('Data with Anomalies highlighted')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_with_anomalies(prjs_umap, anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRP is a technique of explainable artifficial intelligence (XAI). It is used to explain the predictions of models that are structured as neural networks. It operates by backpropagating the prediction in the neural network using a set of designed local ```propagation rules```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP structure\n",
    "\n",
    "- Define the Neural Network (NN) model. It will be composed by different layers, each one wich its own neurons.\n",
    "- Backpropagation\n",
    "  - Propagation starts at the top layer (usually, the output layer of the NN)\n",
    "  - Local Propagation Rules\n",
    "\n",
    "    In each neuron, specific local ```propagation rules``` are applied to calculate how much ```relevance``` or importance should be passed to the next layer. The ```relevance``` is a real number defined via the ```propagation rule```.\n",
    "  - Propagation\n",
    "  \n",
    "    The relevances calculated in the layer are transmitted downwards to the next layer.\n",
    "\n",
    "      - Conservation property: the total amount of relevance received by each neuron is equally redistributed to the neurons in the layer bellow.\n",
    "  - Repeat layer by layer. The propagation continues until it reaches the input features of the NN. At this point, each input feature has received a relevance score that reflects its contribution to the NN prediction. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with other techniques\n",
    "LRP distinguishes itself from other explainability techniques in two ways:\n",
    "- Other techniques are often more computationally expensive, as many of them involve multiple neural network evaluations.\n",
    "- Some alternative techniques replace the gradient with a coarser estimate of effect. That involves optimising some local surrogate model or the explanation itself.\n",
    "\n",
    "In contrast, LRP leverages the graph structure of deep neural networks to compute explanations quickly and reliably​​."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "The main limitation of LRP is the way it handles features' contributions. The method for dealing with positive and negative contributions during the propagation phase may limit how much these relevances can grow. \n",
    "\n",
    "This may result on less detailed representation of how the input features may affect to the NN output. However, this aids in providing more stable explanations. \n",
    "\n",
    "Also, this mean that LRP is more focused on global trend or most influence features instead of granulated details or minor variations... \n",
    "\n",
    "...**and that is exactly what we are looking for here!** \n",
    "\n",
    "Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP implementations\n",
    "In order to decide a library for implementing LRP into deepvats, ```GitHub``` and ```PyPI``` available libraries have been checked.\n",
    "ctions) |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Library | GitHub URL | Base Framework | Supported Data Types | Associated Paper |\n",
    "|---------|------------|----------------|----------------------|------------------|\n",
    "| iNNvestigate | [GitHub](https://github.com/albermax/innvestigate) [PyPI](https://pypi.org/project/innvestigate/)| Various (Keras, TensorFlow, PyTorch - added in 2019) | Not specified. The paper talks about pixels and classifiers. Assumed to be used for vision classification models | [Paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140&ref=blog.paperspace.com) |\n",
    "| Zennit | [PyPI](https://pypi.org/project/zennit/) | PyTorch | Not specified (Adaptable to different data types) | [Paper](https://arxiv.org/abs/2106.13200) |\n",
    "| PyTorchRelevancePropagation | [GitHub](https://github.com/kaifishr/PyTorchRelevancePropagation) | PyTorch | Not specified | Not available |\n",
    "| TorchLRP | [GitHub](https://github.com/fhvilshoj/TorchLRP) | PyTorch | Not specified | Not available, not in PyPI => Not relevant for this study|\n",
    "| Layerwise-Relevance-Propagation-for-LSTMs | [GitHub](https://github.com/alewarne/Layerwise-Relevance-Propagation-for-LSTMs) | TensorFlow | Time Series (Specifically for LSTMs) | Not available |\n",
    "| LRP Toolbox | [GitHub](https://github.com/sebastian-lapuschkin/lrp_toolbox) | Matlab, Python (no PyTorch in requirements), Caffe | Not specified. Examples show images and text. | Not available |\n",
    "| lrp-pf-auc | [PyPI](https://pypi.org/project/lrp-pf-auc/) [Zenodo](https://zenodo.org/records/6821295) | Python (no PyTorch in requirements) | Not specified | Not available |\n",
    "| keras-explain | [PyPI](https://pypi.org/project/keras-explain/) | Keras | Not specified | Not available |\n",
    "| captum | [PyPI]() [GitHub]() | PyTorch | Not specified | [Paper](https://arxiv.org/abs/2009.07896) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are two options that can potentially be used for training our models and have associated papers: ```iNNvestigate```, ```Zennit``` and ```captum```. For simplicity, as we are focused on ```Pytorch``` and ```LRP```, ```zennit``` has been tested for integrating LRP into ```DeepVATS```. Basic use is shown in the next section going through its [tutorial](https://zennit.readthedocs.io/en/latest/getting-started.html), \n",
    "\n",
    "> Aquí intentos en Google Collab de usar captum y zennit con datos tabulares: [GCollab](https://colab.research.google.com/drive/1Bt-csfh1M-ttU2ww6akY_BwRJUA8uND5?authuser=0#scrollTo=lqgM6bIzHb6I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; display:inline-block;\">\n",
    "\n",
    "> TODO: Posible análisis para el paper/otro paper: analizar si iNNvestigate y captum se pueden incluir en DeepVATS y si merece la pena para poder visualizar otros modelos en el futuro o no. De primeras, Zennit me ha parecido el más sencillo de seguir y permite una selección clara de reglas usando tres clases sencillas para manejar el modelo\n",
    "</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zennit: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zennit](https://zennit.readthedocs.io/en/latest/getting-started.html) is a library that implements **propagation-based attribution methods** by *overwriting the gradient of PyTorch modules* in PyTorch's auto-differentiation engine (the part associated to the automatic gradient calculus of 'complex' functions).  Zennit uses this engine for modifying the way gradients are computed within the attribution process, allowing to apply attribution methods based on propagation. \n",
    "\n",
    "*Zennit will only work on models which are strictly implemented using Pytorch modules. Including activation functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "\n",
    "\n",
    "##### Attribution process\n",
    "The \"attribution process\" in the context of neural networks and machine learning refers to the technique of determining how different parts of the input to a model contribute to its output. \n",
    "    \n",
    "The goal is to explain a model's decisions or predictions by identifying which input features are responsible for the final prediction and how much these features influence it. This process is crucial for understanding, interpreting, and trusting machine learning models, especially those that are complex and opaque, such as deep neural networks.\n",
    "\n",
    "Some key considerations about the attribution process include:\n",
    "    \n",
    "- **Interpretability:** Provides a clear insight into why a model makes certain decisions, which is especially important in fields where decisions need to be explainable and justifiable, such as in medicine or banking.\n",
    "\n",
    "- **Identification of Important Features:** Helps understand which features are most influential for the model's predictions, which can be useful for feature engineering or gaining a better understanding of the problem under study.\n",
    "\n",
    "- **Attribution Techniques:** There are different methods for conducting the attribution process, such as Layer-wise Relevance Propagation (LRP), Shapley Decomposition, Grad-CAM, and others. Each of these methods has its own advantages, limitations, and suitable use cases.\n",
    "\n",
    "- **Applications in Various Fields:** The attribution process is applied in a variety of fields, from image recognition and natural language processing to disease prediction and financial decision-making.\n",
    "\n",
    "  In summary, the attribution process is a fundamental part of analyzing machine learning models, providing transparency and understanding in how models make predictions or decisions based on input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Propagation-based attribution methods\n",
    "Explainable-AI techniques that propagate the  contribution of output neurons back to the input layers. Essentially, these methods attempt to explain how the input features of a model contribute to its final prediction. In the context of Zennit, these methods modify the gradients of the PyTorch modules during the autodifferentiation process to compute these contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Zennit structures\n",
    "The most important high-level structures in Zennit are ```composites```, ```Attributors```, and ```Canonizers```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composites\n",
    "Structures that map ```Rules``` to modules (torch.nn, MVP) based on their properties and context to modify their gradient. The most common composites for ```LRP``` are implemented in ```zennit.composites```.\n",
    "That is: \n",
    "\n",
    "- Map ```Rules``` to modules. Each module (convolutional layers, ReLU activation layers, ...) may need specifics ways for computing their contribution to the module output. ```Composites``` are used to assign different rules to those modules to define how to get those contributions.\n",
    "- Based on their propierties and context. The assignation of this rules is not random and depends on the charasteristics of each module and its context insithe the module.\n",
    "- This mapping/assignation changes the way gradient is computed within retropropagation.\n",
    "\n",
    "\n",
    "[Predefined composites](https://zennit.readthedocs.io/en/latest/reference/zennit.composites.html#module-zennit.composites): \n",
    "<span style=\"color:red; display:block;\">\n",
    "La relevancia se la he pedido a ChatGPT entendiendola como: demasiado metida en Imagen (esperará 3D), Aparentemente útil, Seguro que se puede usar. Hay que revisarlo investigando un poco más de cada una de ellas a la hora de fijarlas y de escribir el artículo. Tener en cuenta que nos interesa que sea algo especialmente genérico. O proponer las que sean úties y que el destinatario decida cuál quiere usar.\n",
    "</span>\n",
    "| Composite Name            | Description                                                        | Reference                                                                                     | Relevance for Time Series (MVP) |\n",
    "|---------------------------|--------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| BetaSmooth                | Explicit composite to modify ReLU gradients to smooth softplus gradients [Dombrowski et al., 2019]. | Dombrowski, A.-K., Alber, M., Anders, C. J., Ackermann, M., Müller, K.-R., & Kessel, P. (2019). Explanations can be manipulated and geometry is to blame. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 13567–13578. [Link](https://proceedings.neurips.cc/paper/2019/hash/bb836c01cdc9120a9c984c525e4b1a4a-Abstract.html) | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| DeconvNet                 | Modifying gradients of all ReLUs according to DeconvNet [Zeiler and Fergus, 2014]. | Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I, volume 8689 of Lecture Notes in Computer Science, 818–833. Springer. [Link](https://doi.org/10.1007/978-3-319-10590-1_53) | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant |\n",
    "| EpsilonAlpha2Beta1        | Alpha2-beta1 rule for convolutional and epsilon rule for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonAlpha2Beta1Flat    | Flat rule for first linear layer, alpha2-beta1 for convolutional, epsilon for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonGammaBox           | ZBox rule for first convolutional layer, gamma for following convolutional, epsilon for fully connected layers. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| EpsilonPlus               | Zplus rule for convolutional layers and epsilon rule for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonPlusFlat           | Flat rule for any first linear layer, zplus for other convolutional, epsilon for other fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| ExcitationBackprop        | Implementing ExcitationBackprop [Zhang et al., 2016]. | Zhang, J., Lin, Z. L., Brandt, J., Shen, X., & Sclaroff, S. (2016). Top-down neural attention by excitation backprop. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, 543–559. Springer. [Link](https://doi.org/10.1007/978-3-319-46493-0_33) | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant |\n",
    "| GuidedBackprop            | Modifying gradients of all ReLUs according to GuidedBackprop [Springenberg et al., 2015]. | Springenberg, J. T., Dosovitskiy, A., Brox, T., & Riedmiller, M. A. (2015). Striving for simplicity: the all convolutional net. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings. [Link](http://arxiv.org/abs/1412.6806) | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| LayerMapComposite         | A Composite for which hooks are specified by a mapping from module types to hooks. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| MixedComposite            | A Composite for which hooks are specified by a list of composites.  | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| NameLayerMapComposite     | A Composite for which hooks are specified by both a mapping from module names and module types to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| NameMapComposite          | A Composite for which hooks are specified by a mapping from module names to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| SpecialFirstLayerMapComposite | A Composite for which hooks are specified by a mapping from module types to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Predefined rules](https://zennit.readthedocs.io/en/latest/how-to/use-rules-composites-and-canonizers.html#rules)\n",
    "| Rule Name              | Description                                                        | Relevance for Time Series (MVP) | Advantages                                      | Disadvantages                                 |\r\n",
    "|------------------------|--------------------------------------------------------------------|---------------------------------|-------------------------------------------------|-----------------------------------------------|\r\n",
    "| AlphaBeta              | Adaptable rule for different layer types.                          | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Adaptable to different layers.                   | Increases in complexity with layer depth.      |\r\n",
    "| Epsilon                | A stable rule, often used as a default for many layers.            | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful    | Simple and stable.                               | Might not capture all relevant features.      |\r\n",
    "| Flat                   | Suitable for input layers; provides a basic relevance mapping.     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Beneficial for input layers.                     | Less informative for deeper layers.            |\r\n",
    "| Gamma                  | Balances positive and negative contributions in layers.            | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Balances positive and negative contributions.    | Sensitive to hyperparameter tuning.            |\r\n",
    "| ZBox                   | Specific to input normalisation processes.                         | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Specific to input normalization.                | Limited general applicability.                 |\r\n",
    "| ZPlus                  | Focuses on positive contributions from the layers.                 | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Focuses on positive contributions.               | Neglects negative contributions.               |\r\n",
    "| ZB                     | A balanced approach to attributing relevance.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Provides a balanced attribution.                | Requires careful calibration and tuning.       |\r\n",
    "| WSquare                | Emphasises the importance of weights in the network.              | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Highlights weight significance.                  | Not universally suitable for all networks.     |\r\n",
    "| WSquareFlat            | A combination of WSquare and Flat rules.                           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Combines features of WSquare and Flat rules.     | Limited in scope and application.              |\r\n",
    "| GuidedBackpropReLU     | Alters ReLU gradients for visualisation in convolutional networks. | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Focused on visualisation in CNNs.                | Specific to CNNs and similar architectures.    |\r\n",
    "| PatternAttribution     | Considers layer-wise patterns for attribution.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Accounts for layer-specific patterns.            | Complexity due to need for precomputed patterns. |\r\n",
    "| PatternNet             | Provides detailed layer-wise analysis based on patterns.           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Detailed layer analysis.                         | Requires extensive pre-computation.            |\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta los comentarios de ChatGPT y que queremos explicar las atribuciones del recorrido completo, usando varias capas, la tabla quedaría más bien así\n",
    "| Rule Name              | Description                                                        | Relevance for Time Series (MVP) | Advantages                                      | Disadvantages                                 |\n",
    "|------------------------|--------------------------------------------------------------------|---------------------------------|-------------------------------------------------|-----------------------------------------------|\n",
    "| AlphaBeta              | Adaptable rule for different layer types.                          | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Adaptable to different layers.                   | Increases in complexity with layer depth.      |\n",
    "| Epsilon                | A stable rule, often used as a default for many layers.            | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful    | Simple and stable.                               | Might not capture all relevant features.      |\n",
    "| Flat                   | Suitable for input layers; provides a basic relevance mapping.     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Beneficial for input layers.                     | Less informative for deeper layers.            |\n",
    "| Gamma                  | Balances positive and negative contributions in layers.            | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Balances positive and negative contributions.    | Sensitive to hyperparameter tuning.            |\n",
    "| ZBox                   | Specific to input normalisation processes.                         | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Specific to input normalization.                | Limited general applicability.                 |\n",
    "| ZPlus                  | Focuses on positive contributions from the layers.                 | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Focuses on positive contributions.               | Neglects negative contributions.               |\n",
    "| ZB                     | A balanced approach to attributing relevance.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Provides a balanced attribution.                | Requires careful calibration and tuning.       |\n",
    "| WSquare                | Emphasises the importance of weights in the network.              | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Highlights weight significance.                  | Not universally suitable for all networks.     |\n",
    "| WSquareFlat            | A combination of WSquare and Flat rules.                           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Combines features of WSquare and Flat rules.     | Limited in scope and application.              |\n",
    "| GuidedBackpropReLU     | Alters ReLU gradients for visualisation in convolutional networks. | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Focused on visualisation in CNNs.                | Specific to CNNs and similar architectures.    |\n",
    "| PatternAttribution     | Considers layer-wise patterns for attribution.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Accounts for layer-specific patterns.            | Complexity due to need for precomputed patterns. |\n",
    "| PatternNet             | Provides detailed layer-wise analysis based on patterns.           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Detailed layer analysis.                         | Requires extensive pre-computation.            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributors\n",
    "\n",
    "[Attributtors](https://zennit.readthedocs.io/en/latest/how-to/write-custom-attributors.html) provide an additional layer of abstraction over the context of Composites. \n",
    "\n",
    "They are used to directly produce attributions, which may or may not be computed with modified gradients, if they are used, from Composites. \n",
    "More information on Attributors, examples and their use can be found in [Using Attributors](https://zennit.readthedocs.io/en/latest/how-to/use-attributors.html).\r\n",
    "\r\n",
    "Attributors can be used to implement non-layer-wise or only partly layer-wise attribution methods. For this, it is enough to define a subclass of zennit.attribution.Attributor and implement its forward() and optionally its __init__() methods\n",
    "We are focused on Layer-wise-retropropagation. However, in order to check the tool. The example attributor based in Gradient method will be implemented..: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Canonizers(https://zennit.readthedocs.io/en/latest/how-to/use-rules-composites-and-canonizers.html)\n",
    "\n",
    "Zennit implements propagation-based attribution methods by overwriting the gradient of PyTorch modules within PyTorch’s auto-differentiation engine. There are three building blocks in Zennit to achieve attributions: Rules, Composites and Canonizers. In short, Rules specify how to overwrite the gradient, Composites map rules to modules, and Canonizers transform some module types and configurations to a canonical form, necessary in some cases.\n",
    "\n",
    "For some modules and operations, Layer-wise Relevance Propagation (LRP) is not implementation-invariant, eg. ```BatchNorm -> Dense -> ReLU``` will be attributed differently than ```Dense -> BatchNorm -> ReLU```. \n",
    "\n",
    "Therefore, LRP needs a canonical form of the model, which is implemented in Canonizers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute LRP relevance \n",
    "> Following the Startting Guide & using relevat composites, attributtors and canonizers according to chatGPT. Well, attributtors should not at first be used, but let's check how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install zennit TODO: pasar al conda\n",
    "! pip install zennit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submodules summary for rules decision\n",
    "\n",
    "Before computing the LRP relevance, let's check the layer in the model so we can better decide the rules that we should use according to their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc_learner.modules # Too long to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner_module_leave(learner):\n",
    "    modules = list(learner.modules())[0]  # Obtener el módulo raíz\n",
    "    rows = []\n",
    "\n",
    "    def find_leave_modules(module, path=[]):\n",
    "        for name, sub_module in module.named_children():\n",
    "            current_path = path + [f\"{type(sub_module).__name__}\"]\n",
    "            if not list(sub_module.children()):\n",
    "                leave_name = ' -> '.join(current_path)\n",
    "                leave_params = str(sub_module).strip()  \n",
    "                rows.append([\n",
    "                    leave_name,\n",
    "                    f\"{type(sub_module).__name__}\",\n",
    "                    name,\n",
    "                    leave_params\n",
    "                ]\n",
    "                )\n",
    "\n",
    "            find_leave_modules(sub_module, current_path)\n",
    "\n",
    "    find_leave_modules(modules)\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=['Path', 'Module_type', 'Module_name', 'Module'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner_module_leave(enc_learner)\n",
    "md = learner_module_leave(enc_learner).drop(\n",
    "        'Path', axis = 1\n",
    "    ).sort_values(\n",
    "        by = 'Module_type'\n",
    "    )\n",
    "print(\"The layers are of this types:\")\n",
    "md_types = pd.DataFrame(md['Module_type'].drop_duplicates())\n",
    "display(md_types)\n",
    "print(\"And they are called with this parameters:\")\n",
    "md_modules = pd.DataFrame(md['Module'].drop_duplicates())\n",
    "display(md_modules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data to be analysed\n",
    "> TODO: Addapt so only a specific range of TS is used according to a selected ProjectionPoints plot section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.core import get_ts_dls\n",
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input = enc_input #[::enc_artifact.metadata['stride']]\n",
    "lrp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = get_forecasting_splits(\n",
    "        df = df, \n",
    "        fcst_history = enc_artifact.metadata['w'],\n",
    "        fcst_horizon = 1,\n",
    "        stride = enc_artifact.metadata['stride'], \n",
    "        test_size = 0.2,\n",
    "        show_plot = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [ToFloat(), None]\n",
    "batch_tfms = [\n",
    "    TSStandardize(\n",
    "        by_sample=enc_artifact.metadata['norm_by_sample'],\n",
    "        use_single_batch=enc_artifact.metadata['norm_use_single_batch']\n",
    "    )\n",
    "]\n",
    "dls = get_ts_dls(\n",
    "    lrp_input, \n",
    "    splits=splits, \n",
    "    tfms=tfms, \n",
    "    bs=enc_artifact.metadata['batch_size'], \n",
    "    batch_tfms=batch_tfms\n",
    ")\n",
    "dls.show_at(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input = enc_input\n",
    "print(lrp_input.shape)\n",
    "lrp_input_strided = lrp_input[::enc_artifact.metadata['stride']]\n",
    "print(lrp_input_strided.shape)\n",
    "print(type(lrp_input_strided))\n",
    "rows_with_nan = np.isnan(lrp_input_strided).any(axis=(1,2))\n",
    "lrp_input_strided = lrp_input_strided[~rows_with_nan]\n",
    "lrp_input_strided.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the input requires a gradient\n",
    "lrp_input_torch = torch.cuda.FloatTensor(lrp_input_strided)\n",
    "lrp_input_torch.requires_grad = True\n",
    "# Ensure input to be in GPU\n",
    "lrp_input_torch.to('cuda')\n",
    "lrp_input_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window_size = enc_artifact.metadata['w']\n",
    "#window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = enc_artifact.metadata['batch_size']\n",
    "#batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model to be in GPU\n",
    "model = enc_learner.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: composite (basic version)\n",
    "\n",
    "> Using ```EpsilonPlusFlat``` composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a composite instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.composites import EpsilonPlusFlat\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite instance\n",
    "composite = EpsilonPlusFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output and gradient within the composite's context\n",
    "def compute_output_and_gradient(model, input_data):\n",
    "    model.eval()\n",
    "    # Just in case to check if the difference in the relevance is cause of no-reversion (not should as it is inside with)\n",
    "    original_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    with composite.context(model) as modified_model:\n",
    "        output = modified_model(input_data)\n",
    "        output.backward(\n",
    "            gradient=torch.ones_like(input_data),\n",
    "            retain_graph = True\n",
    "        ) \n",
    "        relevance = torch.autograd.grad(\n",
    "            output, input_data, \n",
    "            torch.ones_like(input_data),\n",
    "            retain_graph=True\n",
    "        )\n",
    "    model.load_state_dict(original_state_dict)\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Revisar por qué salen NaNs\n",
    "output, relevance = compute_output_and_gradient(model, lrp_input_torch)\n",
    "relevance = relevance[0]\n",
    "relevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce relevance dimensions to get an array with a relevance per each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(relevance):\n",
    "    #Get the mean per each feature\n",
    "    importances = relevance.nanmean(axis=2)\n",
    "    print(importances.shape)\n",
    "    #Get a global mean instead of one per sample\n",
    "    importances_mean = importances.nanmean(dim=0).detach().cpu().numpy()\n",
    "    print(importances_mean.shape)\n",
    "    #print(importances_mean)\n",
    "    #Take care of negative values\n",
    "    min_importance = min(importances_mean)\n",
    "    if  min_importance < 0:\n",
    "        importances_mean = importances_mean - min_importance\n",
    "    print(min(importances_mean))\n",
    "    #See as percentage\n",
    "    importances_sum = np.nansum(importances_mean)\n",
    "    importances_percentage = (importances_mean/importances_sum)\n",
    "    print(importances_percentage)\n",
    "    return importances_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = reduce_dimensions(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = list(df.columns)\n",
    "features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create auxiliar function to check a diagram bar\n",
    "> TODO: Think how a heatmap would be sawn as and implement if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_importance(title, features_names, importances_percentage):\n",
    "    # Crear diagrama de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(features_names, importances_percentage)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(title + '| Features importance (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance ('LRP', features_names, importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the two most meaningful features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meaningful_ids(importances, features_names):\n",
    "    meaningful_ids = np.argsort(importances)[-2:]\n",
    "    meaningful_features = [(i,features_names[i], importances[i]) for i in meaningful_ids]\n",
    "    return meaningful_features, meaningful_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features, meaningful_ids = get_meaningful_ids(importances, features_names)\n",
    "meaningful_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: attributtors \n",
    "> Using ```SmoothGrad``` attributor as in the example\n",
    ">\n",
    "> Allows to use other attribution-based XAI techniques\n",
    "> \n",
    "> Not relevant as we want to use RLP. But tried for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.attribution import SmoothGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributor = SmoothGrad(model, noise_level=0.1, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_and_relevance_atributor(model, attributor, input):\n",
    "    # we do not need a composite to compute vanilla SmoothGrad\n",
    "    model.eval()\n",
    "    #with atributor_ as attributor:\n",
    "    with attributor:\n",
    "         # gradient/ relevance\n",
    "         output, relevance = attributor(\n",
    "             input, \n",
    "             torch.ones_like(input)\n",
    "        )\n",
    "    print('Attributor:', relevance[:2], relevance.shape)\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_smooth_grad = compute_output_and_relevance_atributor(model, attributor, lrp_input_torch)\n",
    "print(relevance_smooth_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_attributor = reduce_dimensions(relevance_smooth_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Attributor Smooth Grad', features_names, importances_attributor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_attributors, meaningful_ids_attributors = get_meaningful_ids(importances_attributor, features_names)\n",
    "meaningful_features_attributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to compare\n",
    "meaningful_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Canonizers & Attributor & Rule\n",
    "> Using ```SequentialMergeBatchNorm``` canonizer\n",
    "\n",
    "> and ```Gradient``` attributor\n",
    "\n",
    "> and ```EpsilonGammaBox``` as rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.attribution import Gradient\n",
    "from zennit.composites import EpsilonGammaBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizers = [SequentialMergeBatchNorm()]\n",
    "composite = EpsilonGammaBox(low=-3., high=3., canonizers=canonizers)\n",
    "attributor = Gradient(model, composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_and_relevance_canonizer_atributor(model, composite, attributor, input):\n",
    "    composite.register(model)\n",
    "    with attributor:\n",
    "        output, relevance = attributor(\n",
    "             input,\n",
    "             torch.ones_like(input)\n",
    "        )\n",
    "    composite.remove()\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_canonizer_atributor_rule = compute_output_and_relevance_canonizer_atributor(model, composite, attributor, lrp_input_torch)\n",
    "importances_canonizer_atributor_rule = reduce_dimensions(relevance_canonizer_atributor_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_canonizer_atributor_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_canonizer_attributor_rule, meaningful_ids_canonizer_attributor_rule = get_meaningful_ids(importances_canonizer_atributor_rule, features_names)\n",
    "meaningful_features_canonizer_attributor_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumarise\")\n",
    "print(\"Composite meaningful features: \", meaningful_features)\n",
    "print(\"Attributor meaningful features: \", meaningful_features_attributors)\n",
    "print(\"Canonizer & Atributor & Rule meaningful features: \", meaningful_features_canonizer_attributor_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 4: Canonizer & Rule\n",
    "> The best option for our goal. Next step: select specific rules for our model (MPV)\n",
    "\n",
    "> Using ```SequentialMergeBatchNorm``` composite\n",
    "\n",
    "> and ```EpsilonGammaBox``` as rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.composites import EpsilonGammaBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizer2 = SequentialMergeBatchNorm()\n",
    "composite2 = EpsilonGammaBox(low=-3., high=3., canonizers=canonizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output and gradient within the composite's context\n",
    "def compute_output_and_gradient_canonizer_rule(model, composite2, input_data):\n",
    "    model.eval()\n",
    "    \n",
    "    composite.register(model)\n",
    "\n",
    "    #Do something with the model\n",
    "    output = model(input_data)\n",
    "    \n",
    "    relevance, = torch.autograd.grad(\n",
    "        output, input_data, \n",
    "        torch.ones_like(input_data),\n",
    "        retain_graph=True\n",
    "    )\n",
    "    \n",
    "    composite.remove()\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_canonizer_rule = compute_output_and_gradient_canonizer_rule(model, composite2, lrp_input_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_canonizer_rule = reduce_dimensions(relevance_canonizer_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_canonizer_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_canonizer_rule, meaningful_ids_canonizer_rule = get_meaningful_ids(importances_canonizer_rule, features_names)\n",
    "meaningful_features_canonizer_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumarise\")\n",
    "print(\"Composite meaningful features: \", meaningful_features)\n",
    "print(\"Attributor meaningful features: \", meaningful_features_attributors)\n",
    "print(\"Canonizer & Atributor & Rule meaningful features: \", meaningful_features_canonizer_attributor_rule)\n",
    "print(\"Canonizer & Rule meaningful features: \", meaningful_features_canonizer_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MVP, the modules types are \n",
    "\n",
    "- Add\n",
    "- BatchNorm1d\n",
    "- Concat\n",
    "- Conv1d\n",
    "- Dropout\n",
    "- MaxPool1d\n",
    "- ReLU\n",
    "\n",
    "The main for LRP analysis are the following, sumarised to check the rules and composites that may be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer Type  | Rule Recommendation | Composite Recommendation | Canonizer Recommendation | Notes |\n",
    "|-------------|---------------------|--------------------------|-------------------------|-------|\n",
    "| BatchNorm1d | - | - | SequentialMergeBatchNorm() for ensuring correct execution order | Ideal for normalizing batch layers. |\n",
    "| Conv1d      | Epsilon rule | Epsilon - * | - | Further investigation needed to select the specific composite. |\n",
    "| MaxPool1d   | - | Epsilon | - | Epsilon is stable, no change needed. |\n",
    "| ReLU        | - | BetaSmooth; DeconvNet | - | DeconvNet may be more suited to visual computing. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, final implementation should look like ```compute_output_and_gradient_canonizer_rule``` version. The next step is to select the parameters for epsilon composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you must select a subset of the plot. By default, some random indices will be selected at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U plotly\n",
    "#! pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#! mamba install canvas -c conda-forge | No conseguido, usar pip install canvas si hace falta | En revisión\n",
    "#! mamba install -y -c conda-forge ipympl==0.9.3 #0.5.1\n",
    "#! mamba update -y -c conda-forge nbdime\n",
    "#! conda install -y -c conda-forge ipywidgets\n",
    "#! jupyter nbextension enable --py --sys-prefix ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output, HTML as IPHTML\n",
    "from ipywidgets import Button, Output, VBox, HBox, HTML, Layout\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_prjs_points_total = min(prjs_umap.shape[0], 10)\n",
    "selected_indices = np.random.permutation(lrp_input_torch.size(0))[:selected_prjs_points_total]\n",
    "print(selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save(fig):\n",
    "    image_bytes = pio.to_image(fig, format='png')\n",
    "    with open(f\"../imgs/w={w}.png\", 'wb') as f:\n",
    "        f.write(image_bytes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projections_clusters_interactive(\n",
    "    prjs,\n",
    "    cluster_labels, \n",
    "    anomaly_scores,\n",
    "    threshold = 0.5,\n",
    "    fig_size = (7,7),\n",
    "    umap_params,\n",
    "    anomalies_flag = False\n",
    "):\n",
    "    \n",
    "    global selected_indices\n",
    "    selected_indices_tmp = selected_indices\n",
    "    py.init_notebook_mode()\n",
    "    \n",
    "    prjs_df = pd.DataFrame(prjs, columns = ['x1', 'x2'])\n",
    "    prjs_df['cluster'] = cluster_labels\n",
    "    if anomalies_flag:\n",
    "        prjs_df['anomaly'] = anomaly_scores > threshold\n",
    "\n",
    "    fig = go.FigureWidget(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=prjs_df['x1'], y=prjs_df['x2'], \n",
    "                mode=\"markers\", \n",
    "                marker=dict(color=prjs_df['cluster'].unique())\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if anomalies_flag:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = anomalies['x1'],\n",
    "            y = anomalies['x2'],\n",
    "            mode = 'markers',\n",
    "            marker=dict(size=8, color='red', opacity=0.7, line=dict(color='black', width=1)),\n",
    "            name = 'Anomalies'\n",
    "        ))\n",
    "\n",
    "    \n",
    "    sca = fig.data[0]\n",
    "    \n",
    "    fig.update_layout(\n",
    "        dragmode='lasso',\n",
    "        width=700, \n",
    "        height=500,\n",
    "        title={\n",
    "            'text': '<span style=\"font-weight:bold\">DR params - n_neighbors:{:d} min_dist:{:f}</span>'.format(\n",
    "                     umap_params['n_neighbors'], umap_params['min_dist']),\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='#f0f0f0',\n",
    "        xaxis=dict(gridcolor='lightgray', zerolinecolor='black', title = 'x'), \n",
    "        yaxis=dict(gridcolor='lightgray', zerolinecolor='black', title = 'y'),\n",
    "        margin=dict(l=10, r=20, t=30, b=10)\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    output_tmp = Output()\n",
    "    output_button = Output()\n",
    "\n",
    "    \n",
    "    def select_action(trace, points, selector):\n",
    "        global selected_indices_tmp\n",
    "        selected_indices_tmp = points.point_inds\n",
    "        with output_tmp:\n",
    "            output_tmp.clear_output(wait=True)\n",
    "            print(\"Selected indices tmp:\", selected_indices_tmp)\n",
    "        \n",
    "    def button_action(b):\n",
    "        global selected_indices\n",
    "        global selected_indices_tmp\n",
    "        selected_indices = selected_indices_tmp \n",
    "        with output_button: \n",
    "            output_button.clear_output(wait = True)\n",
    "            print(\"Selected indices:\", selected_indices)\n",
    "    \n",
    "    sca.on_selection(select_action)\n",
    "\n",
    "    layout = widgets.Layout(width='auto', height='40px')\n",
    "    button = Button(\n",
    "        description=\"Update selected_indices\",\n",
    "        style = {'button_color': 'lightblue'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    \n",
    "    button.on_click(button_action)\n",
    "\n",
    "\n",
    "    ##### Reactivity buttons\n",
    "    pause_button = Button(\n",
    "        description = \"Pause interactiveness\",\n",
    "        style = {'button_color': 'pink'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    resume_button = Button(\n",
    "        description = \"Resume interactiveness\",\n",
    "        style = {'button_color': 'lightgreen'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    interaction_enabled = True\n",
    "    def pause_interaction(b):\n",
    "        global interaction_enabled\n",
    "        interaction_enabled = False\n",
    "        fig.update_layout(dragmode='pan')\n",
    "    \n",
    "    def resume_interaction(b):\n",
    "        global interaction_enabled\n",
    "        interaction_enabled = True\n",
    "        fig.update_layout(dragmode='lasso')\n",
    "\n",
    "    pause_button.on_click(pause_interaction)\n",
    "    resume_button.on_click(resume_interaction)\n",
    "    #####\n",
    "    space = HTML(\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\") \n",
    "    \n",
    "    vbox = VBox((output_tmp, output_button, fig))\n",
    "    hbox = HBox((space, button, space, pause_button, space, resume_button))\n",
    "    # Centrar las dos cajas horizontalmente en el VBox\n",
    "\n",
    "    box_layout = widgets.Layout(display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='center',\n",
    "                width='100%')\n",
    "    \n",
    "    box = VBox((hbox,vbox), layout = box_layout)\n",
    "    box.add_class(\"layout\")\n",
    "    plot_save(fig)\n",
    "    \n",
    "    display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections_clusters_interactive(\n",
    "    prjs_umap, \n",
    "    clusters_labels, \n",
    "    umap_params, \n",
    "    anomalies_flag = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain relevance scores for selected projection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input_subset_torch = lrp_input_torch[selected_indices]\n",
    "lrp_input_subset_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizer_subset = SequentialMergeBatchNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_subset = compute_output_and_gradient_canonizer_rule(model, canonizer_subset, lrp_input_subset_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_subset = reduce_dimensions(relevance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_subset, meaningful_features_subset_ids = get_meaningful_ids(importances_canonizer_rule, features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dr_artifact.to_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable1 = df[meaningful_features_subset[0][1]]\n",
    "variable2 = df[meaningful_features_subset[1][1]]\n",
    "time = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "\n",
    "# Meaningful features selection\n",
    "df.plot(y=meaningful_features_subset_ids, colormap=cmap, ax=ax)\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Meaningful Features Subset Time Series Plot')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indices = []\n",
    "prjs_ids = selected_indices\n",
    "prjs_ids = [id * enc_artifact.metadata['stride'] for id in prjs_ids]\n",
    "\n",
    "for start_idx in splits[0]:\n",
    "    min = start_idx\n",
    "    max = start_idx +  enc_artifact.metadata['w'] + 1\n",
    "    sequence_indices = range(min,max)\n",
    "    if any(idx in range(min, max) for idx in prjs_ids):\n",
    "        df_indices.extend(sequence_indices)\n",
    "df_indices = sorted(set(df_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subset\n",
    "max_points = 5000\n",
    "step = len(df) // max_points if len(df) > max_points else 1\n",
    "\n",
    "# Selección uniforme de puntos\n",
    "df_selected = df.iloc[df_indices]\n",
    "df_sampled = df_selected.iloc[::step, :]\n",
    "\n",
    "# Gráfico de series temporales\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "\n",
    "# Graficar las características significativas del subset reducido\n",
    "df_sampled.plot(y=meaningful_features_subset_ids, colormap=cmap, ax=ax)\n",
    "\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Meaningful Features Subset Time Series Plot | Projections points selected associated time windows')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(0.025)\n",
    "beep(0.025)\n",
    "beep(0.025)\n",
    "print(\"Execution ended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
