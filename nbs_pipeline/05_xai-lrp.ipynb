{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "----- > Motif "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer-wise Relevance Propagation\n",
    "\n",
    "> This notebooks has five main parts:\n",
    "> - Introduction to Layer-wise Relevance Propagation\n",
    "> - Dimensionality reduction\n",
    ">\n",
    ">   Following the steps of 04_dr notebooks.\n",
    ">   - Gets the embeddings (or latent space) from a vultivariate time series given by an encoder (e.g. autoencoder) \n",
    ">   - Uses the obtained embeddings as input for a dimensionality reduction algorithm, to generate projections of the embeddings. (As 04.. does)\n",
    ">\n",
    "> - Compute Clusters\n",
    ">   Following the steps of 04_dr notebooks. ¿Maybe it should also be in another notebook?\n",
    ">   - The projections are clustered via hdbscan \n",
    "> - Anomalies simple detector\n",
    ">   - Use basic statistics to obtain an anomaly score to visualize annomalies using a dynamic plot. Just for clarity.\n",
    "> - Layer-wise relevance propagation\n",
    ">   - Different implementations ti apply LRP to check the importance of each feature in the embeddings obtainment.\n",
    ">   - Same check for selecting some points in the projections plot (assuming random selection). Checks the importance for each feature in the obtainment of the associated part of the embeddings.\n",
    "\n",
    "> <span style=\"color:red; display:block;\">\n",
    ">  TODO: Save in 04_... an Artifact as in the previous nbs_pipeline notebooks and divide this notebooks in two (one for clustering and other for layer propagation. ¿Deberíamos separar también 04 en dos notebooks?\n",
    "> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Layer-wise Relevance Propagation\n",
    "Layer-wise Relevance Propagation is a XAI technique introduced in 2015 by [Bach et all](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140&ref=blog.paperspace.com) for vision computing deep learning models that has been extensively used in different DL domains for better explainability. \n",
    "\n",
    "This method belongs to the attribution methods cattegory. According to [Towards Better Understanding Attribution Methods](https://openaccess.thecvf.com/content/CVPR2022/html/Rao_Towards_Better_Understanding_Attribution_Methods_CVPR_2022_paper.html) and [Oportunities and Challengues in Explainable Artificial Intelligence (XAI): A Survey](https://arxiv.org/pdf/2006.11371.pdf), it can be classified into three main groups: backpropagation based methods, activation based methods and perturbation-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Mejorar/revisar\n",
    "la clasificación con https://arxiv.org/pdf/2006.11371.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation-based or Gradient-based methods\n",
    "These methods use backpropagation to calculate the relevance of input features, based on each feature's contribution to the model's output. Tipically rely on the gradients:\n",
    "- with respect to the input\n",
    "  - [DeepLift: Propagating Activation Differences](https://arxiv.org/abs/1704.02685). Decomposes the oitput prediction of a neural network on a especific input by backpropagating the activation of each neuron to its ```reference activation``` and assigns coontribution scores according to the difference.\n",
    "  - [Guided BackPropagation/Guided saliency](https://arxiv.org/abs/1412.6806). \"Variant of the [deconvolution approach](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) for visualizing features learned by CNNs, which can also be applied to a broad range of network structures. Under this approach, the use of max-pooling in convolutional neural networks for small images is questioned and the replacement of max-pooling layers by a convolutional layer with increased stride is proposed, resulting in no loss of accuracy on several image recognition benchmarks.\" \n",
    "- with respect to intermediate layers\n",
    "  - Saliency Maps. \"Visualizing gradients, neural activation of individual layers using DeConv nets, guided backpropagation, etc. as images.\"\n",
    "     - [NormGrad](https://openaccess.thecvf.com/content_CVPR_2020/html/Rebuffi_There_and_Back_Again_Revisiting_Backpropagation_Saliency_Methods_CVPR_2020_paper.html). \"Based on the spatial contribution of gradients of convolutional weights\"\n",
    "       - Saliency maps combination at different layers to test the ability of saliency methods to extract complementary information at different network levels\n",
    "        - Class-sensitivity metric and meta-learning inspired paradigm applicable to any saliency method for improving sensitivity to the output class being explained\n",
    "     - [Compute gradient of the class score with respect to the input case](https://arxiv.org/pdf/1312.6034.pdf). Visualisation techniques for Convolutional Networks used for classification.\n",
    "     - [Salient Deconvolutional Networks](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_8).\n",
    "  -  [Salient Relevance Maps](https://sciencedirect.com/science/article/pii/S0262885619300149)\n",
    "  - [Excitation Backprop](https://link.springer.com/article/10.1007/s11263-017-1059-x). Top-down attention of a CNN classifier for generating task-specific attention map.\n",
    "  \n",
    "  - [FullGrad: Full-Gradient Representation for NN Visualization](https://proceedings.neurips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html). Decomposes the NN response into input sensitivity and per-neuron sensitivity components. \n",
    "- With respect to the last layer\n",
    "  - [GradCam](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), [GradCam++](https://ieeexplore.ieee.org/abstract/document/8354201)\n",
    "- With respect to different layers\n",
    "  - [LayerCAM: Hierarchical Class Activation Maps](https://ieeexplore.ieee.org/abstract/document/9462463)\n",
    "  \n",
    "Also, LRP can be included as a backpropagation-based methods. But it focuses on propagating relevances with different rules instead on focusing on the gradients. \n",
    "- [LRP: Layer-wise Relevance Propagation](https://iphome.hhi.de/samek/pdf/MonXAI19.pdf). \"Operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation-based methods\n",
    "\n",
    "Weigh activation maps to assign importance of the final convolutional layer:\n",
    "  - weighted by their gradients:\n",
    "    - [GradCam](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), [GradCam++](https://ieeexplore.ieee.org/abstract/document/8354201)\n",
    "    - [LayerCAM: Hierarchical Class Activation Maps](https://ieeexplore.ieee.org/abstract/document/9462463)\n",
    "    - [Gap CAM: Using Global Average Pooling in CNNs for generating class activation maps](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html)\n",
    "  - Estimating their importance to the classification score \n",
    "    - [Ablation-cam](https://scholar.google.com/scholar?hl=es&as_sdt=0%2C5&q=Saurabh+Desai+and+Harish+G.+Ramaswamy.+Ablation-CAM%3A+Visual+Explanations+for+Deep+Convolutional+Network+via+Gradient-free+Localization.+In+WACV%2C+pages+983%E2%80%93991%2C+2020.&btnG=). Visual explanations for deep CNN via gradient-free localization. \"Uses Ablation analysis to determine the importance (weights) of individual feature map units wrt class. Hilights the important regions in the image for predicting the concept. ...\"\n",
    "    - [Flow restiction](https://arxiv.org/abs/2001.00396), Adds noise to intermediate feature maps to restrict the flow of information and quantify how much information image regions provide.\n",
    "<span style=\"color:red\">TODO: quizá en TS largas pueda tener sentido algo del estilo aplicar algoritmos menos pesados, comprobar las secciones más relevantes y reducir la serie temporal seleccionando sólo las partes más importantes. </span>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation-based methods\n",
    "\n",
    "This methods treat the network as a black-box and assign importance by observing the change in output on perturbing the input. The explanation is generated by iteratively probing a trained ML model with those variations of the input. This can be done using different techniques:\n",
    "\n",
    "- Occluding parts of the image\n",
    "  - [Rise: Randomised Input Sampling for Explanation of black-box models](https://arxiv.org/abs/1806.07421). \"Generates an importance map indicating how salient each pixel is for the model's prediction.\" \"Estimates importance empirically by proving the model with randomly masked versions of the input image and obtaining the corresponding outputs.\"\n",
    "  -  [LIME: Learning and Interpretable Model locally around the prediction](https://dl.acm.org/doi/abs/10.1145/2939672.2939778). \"Explains the prediction of any classifier in an interpretable and faithful manner\"\n",
    "  - [Loss derivative back-propagation](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). Classification of labelled images. \"We train these models using a large set of N labeled images {x,y}, where label y_i is a discrete variable indicating the true class. A cross-entropy loss function, suoitable for image classification, is used to compare ~y_i and u_i. The parameters of the networks are trained by back-propagating the derivative of the loss with respect to the parameters throghout the network, and updating the parameters via stochastic gradient descent\"\n",
    "  - [DeconvNet: DeConvolution networks for convolution visualizations](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). Activate the neourons of individual layers by occluding input instance and visualizing using DeConv\n",
    "Nets.\n",
    "- Optimising for a mask that maximizes/minimizes class confidence\n",
    "  - [Real Time Image Saliency for black box classifiers](https://proceedings.neurips.cc/paper_files/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html). Saliency detection method. \n",
    "  - [Interpretable explanation of black boxes by Meaningful Perturbation](https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html). Goal: find the part of an image most responsible for a classifier decision. Model-agnostic and testable. Interpretable image perturbations.\n",
    "\n",
    "- Selecting specific features\n",
    "   - [SHAP: A Unified Approach to Interpreting Model Predictions\n",
    "](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html) proves feature correlations by removing features in a game theoretic framework.\n",
    "   - [Prediction Difference Analysis](https://arxiv.org/abs/1702.04595). Remove individual fetures  and finds the positive and negative correlation of individual features towards the output\n",
    "- Features replacements. [Interpreting Black Box Models via Hypothesis Testing\n",
    "](https://dl.acm.org/doi/abs/10.1145/3412815.3416889). \"Conterfactual replacements of features to study feature importance. \n",
    "> TODO: REVISAR el artículo principal. Propone un modo de evaluar algoritmos de atribución. ¿Merece la pena revisarlo? Creo que se sale del scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Generate projectsion of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight & Biases\n",
    "import wandb\n",
    "\n",
    "#Yaml\n",
    "from yaml import load, FullLoader\n",
    "\n",
    "#Embeddings\n",
    "from dvats.all import *\n",
    "from tsai.data.preparation import prepare_forecasting_data\n",
    "from tsai.data.validation import get_forecasting_splits\n",
    "from fastcore.all import *\n",
    "\n",
    "#Dimensionality reduction\n",
    "from tsai.imports import *\n",
    "\n",
    "#Clustering\n",
    "import hdbscan\n",
    "import utils.config as cfg_\n",
    "import time\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory_usage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage:\n",
    "    import nbs_pipeline.utils.memory as mem\n",
    "    import torch \n",
    "    gpu_device = torch.cuda.current_device()\n",
    "    mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get W&B API\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model needs to restore the encoder model fitted in the notebook `02x`, as well as the data and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"05-xai-lrp\"\n",
    "runname = name\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg_.get_artifact_config_xai_lrp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_wandb: True\n",
      "wandb_group: None\n",
      "wandb_entity: mi-santamaria\n",
      "wandb_project: deepvats\n",
      "valid_artifact: None\n",
      "train_artifact: mi-santamaria/deepvats/mvp-SWV:latest\n",
      "enc_artifact: mi-santamaria/deepvats/mvp-SWV:latest\n",
      "n_neighbors: 15\n",
      "min_dist: 0.1\n",
      "random_state: 1234\n",
      "metric: euclidean\n",
      "cpu_flag: False\n",
      "job_type: layer_relevance_propagation\n",
      "allow_val_change: True\n",
      "dr_artifact: None\n"
     ]
    }
   ],
   "source": [
    "cfg_.show_attrdict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W&B initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /home/macu/work/nbs_pipeline/05-xai-lrp.ipynb.\n",
      "wandb: Currently logged in as: mi-santamaria. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macu/work/wandb/run-20240205_160633-62476q71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mi-santamaria/deepvats/runs/62476q71' target=\"_blank\">05-xai-lrp</a></strong> to <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mi-santamaria/deepvats/runs/62476q71' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats/runs/62476q71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'stream.Stream' object attribute 'write' is read-only\n"
     ]
    }
   ],
   "source": [
    "run_dr = wandb.init(\n",
    "    entity           = config.wandb_entity,\n",
    "    project          = config.wandb_project if config.use_wandb else 'work-nbs', \n",
    "    group            = config.wandb_group,\n",
    "    allow_val_change = config.allow_val_change, \n",
    "    job_type         = config.job_type, \n",
    "    mode             = 'online' if config.use_wandb else 'disabled',\n",
    "    anonymous        = 'never' if config.use_wandb else 'must',\n",
    "    config           =  config,\n",
    "    resume           = 'allow',\n",
    "    name = runname\n",
    ")\n",
    "config_dr = wandb.config # Object for storing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the encoder model and its associated configuration\n",
    "This model is neccesary for getting back the projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mvp-SWV:v66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_artifact_train:  toy:v2\n",
      "DR artifact train:  toy:v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00</th>\n",
       "      <td>0.741822</td>\n",
       "      <td>0.637180</td>\n",
       "      <td>0.565117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:01</th>\n",
       "      <td>0.739731</td>\n",
       "      <td>0.629415</td>\n",
       "      <td>0.493513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:02</th>\n",
       "      <td>0.718757</td>\n",
       "      <td>0.539220</td>\n",
       "      <td>0.469350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:03</th>\n",
       "      <td>0.730169</td>\n",
       "      <td>0.577670</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:04</th>\n",
       "      <td>0.752406</td>\n",
       "      <td>0.570180</td>\n",
       "      <td>0.373008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T3        T2        T1\n",
       "1970-01-01 00:00:00  0.741822  0.637180  0.565117\n",
       "1970-01-01 00:00:01  0.739731  0.629415  0.493513\n",
       "1970-01-01 00:00:02  0.718757  0.539220  0.469350\n",
       "1970-01-01 00:00:03  0.730169  0.577670  0.444100\n",
       "1970-01-01 00:00:04  0.752406  0.570180  0.373008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, df_config, enc_artifact, enc_learner = get_dataset(config, run_dr, config_dr, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_time_series_flag = False\n",
    "if show_time_series_flag:\n",
    "    # Show time series plot\n",
    "    fig, ax = plt.subplots(1, figsize=(15,5), )\n",
    "    cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "    df.plot(color=cmap(0.05), ax=ax) # or use colormap=cmap\n",
    "    # rect = Rectangle((5000, -4.2), 3000, 8.4, facecolor='lightgrey', alpha=0.5)\n",
    "    # ax.add_patch(rect)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW start |  1707149205.9310153  | end  1707149205.9317 total (secs):  0.0006847381591796875\n",
      "(520, 3, 30)\n"
     ]
    }
   ],
   "source": [
    "w = enc_artifact.metadata['w']\n",
    "t_start = time.time()\n",
    "enc_input, _ = prepare_forecasting_data(df, fcst_history = w)\n",
    "t_end = time.time()\n",
    "t = t_end - t_start\n",
    "print(\"SW start | \" , t_start, \" | end \", t_end, \"total (secs): \", t)\n",
    "print(enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 11680\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the embeddings (activations) from the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = df_config['stride']\n",
    "batch_size = df_config['batch_size']\n",
    "\n",
    "##### Check de toy ####\n",
    "stride = 5\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "None\n",
      "(520, 3, 30)\n",
      "mvp-SWV:v66\n"
     ]
    }
   ],
   "source": [
    "print(stride)\n",
    "print(batch_size)\n",
    "print(enc_input.shape)\n",
    "print(enc_artifact.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_max = 10000000\n",
    "shape = enc_input.shape\n",
    "chunk_size_ = min(shape[1]*shape[2],chunk_max/(shape[1]*shape[2]))\n",
    "N = max(3200,np.floor(chunk_size_/32))\n",
    "chunk_size = N*32\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_enc_embs_set_stride_set_batch_size 1.4458732604980469 seconds\n",
      "GE start |  1707149206.1617174  | end  1707149207.608371 total (secs):  1.4466536045074463\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "embs = get_enc_embs_set_stride_set_batch_size(\n",
    "    enc_input, \n",
    "    enc_learner, \n",
    "    stride     = stride,\n",
    "    batch_size = batch_size,\n",
    "    cpu        = config.cpu_flag,\n",
    "    to_numpy   = True, \n",
    "    print_flag = False,\n",
    "    time_flag  = True,\n",
    "    chunk_size = chunk_size\n",
    ")\n",
    "t_end = time.time()\n",
    "t = t_end-t_start\n",
    "print(\"GE start | \" , t_start, \" | end \", t_end, \"total (secs): \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,UklGRvQHAABXQVZFZm10IBAAAAABAAEAECcAACBOAAACABAAZGF0YdAHAAAAADoKZBRsHkMo2DEbO/1DcExmVNFbpmLaaGJuNXNMd596KX3nftR/8X87f7Z9YntFeGJ0wW9pamNkuV10VqNOUEaLPWA04CoaIR0X+wzDAoj4WO5F5F/at9Bcx1++zLWzrSGmIp/CmAuTBo68iTOGcoN9gVeAAoB/gM2B6YPQhn6K7I4UlOyZa6CHpzSvZbcNwB3JiNI93CzmRvB6+rYE6g4HGfoitSwmNj4/70cqUOJXCV+WZXtrsXAvdex443sQfm5/+3+3f6J+vXwMepN2WHJibblnZmFzWuxS3UpUQl45CzBoJoccdxJJCA7+1fOw6bDf5dVfzC3DX7oCsiaq1qIfnAuWpZD1iwOI1oRygtyAFoAigP+ArIImhWmIcYw1ka+W1Zyeo/6q6bJTuy3EaM331sngzur29DD/agmWE6EdfScXMWE6TEPIS8hTP1shYmJo+G3acv92Ynr9fMt+yX/2f1F/3H2Ze4x4uXQncN1q5GRGXg5XR0/+RkE+HjWlK+Mh6hfKDZQDWPkn7xHlJ9t50RjIEr93tlSutqarnz6ZeZNmjgyKdIajg56BZ4ABgG6AqoG2g46GLIqLjqSTb5nhn/Gmkq65tli/YcjF0XXbYOV376n55QMbDjoYMiLxK2g1iD5CR4dPSVd9XhZlCmtOcNp0p3iue+t9Wn/4f8R/wH7rfEp64Xa2cs5tM2jtYQZbi1OGSwZDGTrMMC8nUh1FExkJ3v6l9H7qeuCq1h7N5cMOu6iywapmo6KcgZYMkU6MTIgPhZuC9YAegBmA5oCCguyEIIgXjM2QOZZSnA6jYqpDsqO6dMOpzDLW/98A6ib0X/6aCMcS1hy2JlYwpzmaQh9LKlOsWpph6GeMbX1ysnYles98rn68f/p/Zn8Cfs970XgOdYtwT2tkZdNeplfqT6xH9z7cNWksrCK3GJoOZAQp+vbv3eXv2zzS1MjGvyK39a5MpzWgu5nok8aOXoq2htWDv4F4gAKAXYCKgYWDTIbbiSuONpPymFifW6byrQ62pL6lxwPRrdqU5Kju2fgUA0wNbRdoIS0rqjTSPZRG406wVvBdlmSXaulvhHRheHh7xX1Ef/N/0H/cfhh9h3oudxJzOW6raHJimFsoVC9MuEPSOo0x9icdHhQU6Qmv/3X1TOtF4XDX3s2dxL67T7Ndq/ajJp33lnWRqIyXiEqFxoIPgSiAEoDOgFqCs4TXh7+LZpDElc+bf6LIqZ2x9Lm9wurLbdU13zPpVvOO/ckH+RELHO8llC/sOOdBdkqLUhlaE2FuZyBtH3JjduZ5oHyPfq5//X96fyZ+A3wWeWJ17nDAa+NlX18+WI1QWEitP5k2LC11I4QZaQ81Bfn6xfCp5rfc/9KRyXvAzreXr+OnwKA5mliUKI+xivmGCITigYuABIBOgGqBVYMMhouJzI3IkneYz57HpVKtZLXxverGQdDl2cjj2u0J+EQCfAygFp8gaCrsMxs95kU+ThZWYl0UZCJqg28tdBl4QHuefS1/7X/af/d+RH3Denp3bHOibiNp92IqXMVU1kxpRIw7TTK8KOge4hS5Cn8ARfYa7BDiNtiezlfFb7z3s/qriKSrnW+X35EDjeOIhoXygiqBMoAMgLeAMoJ8hJCHaYsBkFCVTpvxoS6p+bBFuQbCLMuo1GveZuiH8r78+QYqET8bJyXSLjE4NEHMSetRhFmKYPJmsmzAcRN2pXlwfG9+n3/+f4x/SH42fFl5tHVPcTFsYWbpX9VYL1EESWJAVjfvLT0kUBo4EAUGyvuU8XbngN3D007KMcF7uDqwe6hMobiaypSLjwWLPoc9hAaCnoAHgEGATIEng86FPYlvjVyS/ZdInjOls6y7tD+9MMZ/zx7Z/eIL7Tj3cwGsC9IV1B+jKS0zYzw2RZhNe1XSXJFjrWkcb9Vz0HcHe3V9FX/lf+R/EX9uff56xHfGcwtvmWl7Y7pcYVV8TRlFRDwNM4Epsh+wFYkLUAEV9+js2+L92F/PEMYhvZ+0mKwapTGe6JdKkl+NMInDhR+DR4E/gAeAooANgkaESocTi5yP3ZTOmmShlahVsJi4T8FuyuTTot2Z57fx7fspBlsQcxpfJBAudTeAQCFJSlHuWAFgdmZDbF9xwnVkeT98Tn6Pf/5/nX9qfmh8mnkGdrBxoGzeZnNga1nQUa9JFkESOLIuBSUcGwcR1gaa/GTyQ+hJ3ofUDMvnwSi53bAUqdmhOJs9lfCPWouEh3OELIKzgAuANIAvgfmCkIXwiBKN8ZGEl8KdoKQVrBO0jbx2xb7OWNgy4j3saPaiANwKBBUKH90obTKrO4ZE8kzgVEJcDmM3abRufHOGd816S337ftx/638pf5d9N3sNeB50cm8Pav5jSl38VSJOyEX8PMwzRyp9IH0WWQwgAuX3t+2m48TZINDLxtO9SLU3ra6luJ5imLaSvI1+iQKGTYNlgUyABICOgOiBEYQFh7+KOY9rlE6a2KD9p7Kv67eawLDJINPZ3Mzm6PAc+1gFjA+mGZcjTS25Nss/dUioUFhYdl/4ZdNr/nBwdSF5DHwsfn1//X+sf4p+mHzbeVZ2D3INbVln/GAAWnBSWUrJQc04dC/NJegb1hGmB2v9M/MQ6RPfS9XKy57C1rmCsa6pZ6K5m7CVVZCxi8uHqoRTgg==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33592895, 0.        , 0.18003725, 0.        , 0.4662698 ,\n",
       "       0.696591  , 0.25043887, 0.49783897, 0.        , 0.        ,\n",
       "       0.        , 0.41448307, 0.        , 0.        , 0.12928401,\n",
       "       0.        , 0.06615641, 0.        , 0.08677851, 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GE start |  1707149206.1617174  | end (next cell)  1707149208.6571867 total (secs):  2.495469331741333\n"
     ]
    }
   ],
   "source": [
    "t_end = time.time()\n",
    "t = t_end-t_start\n",
    "print(\"GE start | \" , t_start, \" | end (next cell) \", t_end, \"total (secs): \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if check_memory_usage: mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel (Debugging code 4 analysing where can app be failing. Expecting to be related to GPU mem ussage)\n",
    "#os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 128)\n",
      "(520, 3, 30)\n"
     ]
    }
   ],
   "source": [
    "print(embs.shape)\n",
    "print(enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33592895, 0.        , 0.18003725, 0.        , 0.4662698 ,\n",
       "       0.696591  , 0.25043887, 0.49783897, 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.0 104\n"
     ]
    }
   ],
   "source": [
    "#Dimensions check\n",
    "num_inputs = np.ceil(enc_input.shape[0]/stride)\n",
    "num_embs = embs.shape[0]\n",
    "test_eq(num_inputs, num_embs )\n",
    "print(num_inputs, num_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average embeddings in the time dimension, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure no nan ((Intento de Macu. La celda de comentada abajo es la original. Pero falla por Nan con sunspot))\n",
    "embs_no_nan = embs[~np.isnan(embs).any(axis=1)]\n",
    "embs_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] [16:06:48.852153] Unused keyword parameter: print_flag during cuML estimator initialization\n",
      "[I] [16:06:48.852432] Unused keyword parameter: n_neighbors during cuML estimator initialization\n",
      "[I] [16:06:48.852602] Unused keyword parameter: min_dist during cuML estimator initialization\n",
      "[I] [16:06:48.852790] Unused keyword parameter: metric during cuML estimator initialization\n",
      "[I] [16:06:48.852976] Unused keyword parameter: a during cuML estimator initialization\n",
      "[I] [16:06:48.853160] Unused keyword parameter: b during cuML estimator initialization\n",
      "[I] [16:06:48.853342] Unused keyword parameter: target_metric during cuML estimator initialization\n",
      "[I] [16:06:48.853528] Unused keyword parameter: target_n_neighbors during cuML estimator initialization\n",
      "[I] [16:06:48.853710] Unused keyword parameter: n_epochs during cuML estimator initialization\n",
      "[I] [16:06:48.853894] Unused keyword parameter: init during cuML estimator initialization\n",
      "[I] [16:06:48.854074] Unused keyword parameter: hash_input during cuML estimator initialization\n",
      "[W] [16:06:48.855062] Warning(`fit`): As of v0.16, PCA invoked without an n_components argument defauts to using min(n_samples, n_features) rather than 1\n",
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n",
      "-- cuml.UMAP -- False\n",
      "GPU | Used mem: 11694\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[92m█████████-----------\u001b[0m] \u001b[92m48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prjs = get_prjs(embs_no_nan, config_dr, config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beep(0.15)\n",
    "beep(0.15)\n",
    "beep(0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.9885178 , -1.0625296 ],\n",
       "       [ 1.6740065 , -1.1075859 ],\n",
       "       [ 1.3234682 , -1.1890249 ],\n",
       "       [ 0.9527254 , -1.0872755 ],\n",
       "       [ 1.0939217 , -1.1267929 ],\n",
       "       [ 0.97213936, -1.277544  ],\n",
       "       [ 1.0439377 , -1.2342987 ],\n",
       "       [ 0.7059803 , -0.9702797 ],\n",
       "       [ 0.77603054, -0.8571005 ],\n",
       "       [-1.4581089 ,  1.531126  ]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prjs[0:10] # En R head(res[1,],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prjs = get_UMAP_prjs(embs, cpu=False, **umap_params)\n",
    "#prjs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the projections as an artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_wandb: \n",
    "    run_dr.log_artifact(ReferenceArtifact(prjs, 'projections', type='projections', \n",
    "metadata=dict(run_dr.config)), aliases=f'run-{run_dr.project}-{run_dr.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Precomputed Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integrate precomputed clusters into the embedding space, it's necessary to log artifacts that include the labels of the newly created clusters. \n",
    "\n",
    "The cluster creation process is presented below. This creation procedure can be modified according to specific needs. However, the structure of the new artifact must be preserved (it must be a numpy.ndarray and the number of elements must be equal to the number of points in the embedding space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN supported metrics: ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'wminkowski', 'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice', 'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'cosine', 'arccos', 'pyfunc']\n"
     ]
    }
   ],
   "source": [
    "print(f'HDBSCAN supported metrics: {list(hdbscan.dist_metrics.METRIC_MAPPING.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HDBSCAN parameters\n",
    "hdbscan_kwargs = {\n",
    "    'min_cluster_size' : 7, #100, #100,\n",
    "    'min_samples' : 3,\n",
    "    'cluster_selection_epsilon' : 0.0001,\n",
    "}\n",
    "metric_kwargs = {\n",
    "    'metric' : 'euclidean' #'jaccard'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 9), (3, 20), (1, 15), (2, 9), (-1, 2), (0, 22), (6, 13), (4, 14)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create clusters using HDBSCAN\n",
    "clusters = hdbscan.HDBSCAN(**hdbscan_kwargs, **metric_kwargs).fit(prjs)\n",
    "clusters_labels = clusters.labels_\n",
    "list(Counter(clusters_labels).items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cluster score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette_score: 0.5959915\n"
     ]
    }
   ],
   "source": [
    "score = cluster_score(prjs, clusters_labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing artifact structure \n",
    "test_eq_type(type(clusters_labels), np.ndarray)\n",
    "test_eq(clusters_labels.size, prjs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ref': {'hash': '-6700447374356751756', 'type': \"<class 'numpy.ndarray'>\"}},\n",
       " dict_values([ArtifactManifestEntry(path='-6700447374356751756', digest='IXr4cLYR5IigmGfuSyIFlA==', ref='file:///home/macu/data/wandb_artifacts/-6700447374356751756', birth_artifact_id=None, size=982, extra={}, local_path=None)]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and log 'clusters_labels' artifact\n",
    "clusters_ar = ReferenceArtifact(obj=clusters_labels, name='clusters_labels')\n",
    "clusters_ar.metadata, clusters_ar.manifest.entries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dvats.utils.ReferenceArtifact at 0x7fd011eb56c0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dr.log_artifact(clusters_ar, aliases=['hdbscan_jaccard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beep(0.25)\n",
    "beep(0.25)\n",
    "beep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic plot for determining whether a window of the time series is anomalous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 9), (3, 20), (1, 15), (2, 9), (-1, 2), (0, 22), (6, 13), (4, 14)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create clusters using HDBSCAN\n",
    "clusters = hdbscan.HDBSCAN(**hdbscan_kwargs, **metric_kwargs).fit(prjs)\n",
    "clusters_labels = clusters.labels_\n",
    "list(Counter(clusters_labels).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33185362, 0.28174041, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.21276976,\n",
       "       0.        , 0.48576635, 0.03813443, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07969174, 0.        , 0.1508215 , 0.10156325,\n",
       "       0.4552269 , 0.20566782, 0.00750608, 0.        , 0.41292634,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.41005463, 0.08938308, 0.37453604,\n",
       "       0.02822439, 0.        , 0.22747075, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23120446, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.53388751, 0.43686685, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.30574832, 0.15953104,\n",
       "       0.22189081, 0.        , 0.        , 0.29675403, 0.10964318,\n",
       "       0.41951309, 0.31016148, 0.3149579 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00199836, 0.36183124, 0.41951309, 0.59929514, 0.21276976,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02739199, 0.24976647,\n",
       "       0.24976647, 0.24976647, 0.24976647, 0.24976647])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#anomaly_scores = detector(prjs_umap, clusters_labels)\n",
    "anomaly_scores = clusters.outlier_scores_\n",
    "anomaly_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check anomaly scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plot_anomaly_scores_distribution(anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104,)\n",
      "min  0.0\n",
      "max  0.5992951420854997\n",
      "media  0.097164689013051\n",
      "std  0.15540863961965262\n"
     ]
    }
   ],
   "source": [
    "print(anomaly_scores.shape)\n",
    "print(\"min \", np.min(anomaly_scores))\n",
    "print(\"max \", np.max(anomaly_scores))\n",
    "anomaly_scores_mean = np.mean(anomaly_scores)\n",
    "print(\"media \", anomaly_scores_mean)\n",
    "anomaly_scores_std = np.std(anomaly_scores)\n",
    "print(\"std \", anomaly_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = pd.Series(clusters.outlier_scores_).quantile(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "fig_size = (7,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486ad404bb64e9d8d58e1cd38eaaf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='Threshold', max=3.0, min=1.0, step=0.01), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_clusters_with_anomalies_interactive_plot(threshold, prjs, clusters_labels, anomaly_scores, fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using quartiles for the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_using_iqr(scores):\n",
    "    # First and third quartiles\n",
    "    Q1 = np.percentile(scores, 25)\n",
    "    Q3 = np.percentile(scores, 75)\n",
    "    # IQR range\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    #Limits for anomalies\n",
    "    factor = 1.5\n",
    "    lower_bound = Q1 - (factor * IQR)\n",
    "    upper_bound = Q3 + (factor * IQR)\n",
    "    \n",
    "    # Get anomalies\n",
    "    anomalies = (scores < lower_bound) | (scores > upper_bound)\n",
    "    \n",
    "    return anomalies, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Bound for Anomalies: -0.31915464097817137\n",
      "Upper Bound for Anomalies: 0.5319244016302855\n"
     ]
    }
   ],
   "source": [
    "anomalies, lower_bound, upper_bound = detect_anomalies_using_iqr(anomaly_scores)\n",
    "\n",
    "print(\"Lower Bound for Anomalies:\", lower_bound)\n",
    "print(\"Upper Bound for Anomalies:\", upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_anomalies(data, anomalies):\n",
    "    plt.scatter(data[:, 0], data[:, 1], color='blue', label='Normal Data')\n",
    "    plt.scatter(data[anomalies, 0], data[anomalies, 1], color='red', label='Anomalies')\n",
    "    plt.title('Data with Anomalies highlighted')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'prjs_umap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_data_with_anomalies(\u001b[43mprjs_umap\u001b[49m, anomalies)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prjs_umap' is not defined"
     ]
    }
   ],
   "source": [
    "plot_data_with_anomalies(prjs_umap, anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRP is a technique of explainable artifficial intelligence (XAI). It is used to explain the predictions of models that are structured as neural networks. It operates by backpropagating the prediction in the neural network using a set of designed local ```propagation rules```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP structure\n",
    "\n",
    "- Define the Neural Network (NN) model. It will be composed by different layers, each one wich its own neurons.\n",
    "- Backpropagation\n",
    "  - Propagation starts at the top layer (usually, the output layer of the NN)\n",
    "  - Local Propagation Rules\n",
    "\n",
    "    In each neuron, specific local ```propagation rules``` are applied to calculate how much ```relevance``` or importance should be passed to the next layer. The ```relevance``` is a real number defined via the ```propagation rule```.\n",
    "  - Propagation\n",
    "  \n",
    "    The relevances calculated in the layer are transmitted downwards to the next layer.\n",
    "\n",
    "      - Conservation property: the total amount of relevance received by each neuron is equally redistributed to the neurons in the layer bellow.\n",
    "  - Repeat layer by layer. The propagation continues until it reaches the input features of the NN. At this point, each input feature has received a relevance score that reflects its contribution to the NN prediction. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with other techniques\n",
    "LRP distinguishes itself from other explainability techniques in two ways:\n",
    "- Other techniques are often more computationally expensive, as many of them involve multiple neural network evaluations.\n",
    "- Some alternative techniques replace the gradient with a coarser estimate of effect. That involves optimising some local surrogate model or the explanation itself.\n",
    "\n",
    "In contrast, LRP leverages the graph structure of deep neural networks to compute explanations quickly and reliably​​."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "The main limitation of LRP is the way it handles features' contributions. The method for dealing with positive and negative contributions during the propagation phase may limit how much these relevances can grow. \n",
    "\n",
    "This may result on less detailed representation of how the input features may affect to the NN output. However, this aids in providing more stable explanations. \n",
    "\n",
    "Also, this mean that LRP is more focused on global trend or most influence features instead of granulated details or minor variations... \n",
    "\n",
    "...**and that is exactly what we are looking for here!** \n",
    "\n",
    "Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRP implementations\n",
    "In order to decide a library for implementing LRP into deepvats, ```GitHub``` and ```PyPI``` available libraries have been checked.\n",
    "ctions) |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Library | GitHub URL | Base Framework | Supported Data Types | Associated Paper |\n",
    "|---------|------------|----------------|----------------------|------------------|\n",
    "| iNNvestigate | [GitHub](https://github.com/albermax/innvestigate) [PyPI](https://pypi.org/project/innvestigate/)| Various (Keras, TensorFlow, PyTorch - added in 2019) | Not specified. The paper talks about pixels and classifiers. Assumed to be used for vision classification models | [Paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140&ref=blog.paperspace.com) |\n",
    "| Zennit | [PyPI](https://pypi.org/project/zennit/) | PyTorch | Not specified (Adaptable to different data types) | [Paper](https://arxiv.org/abs/2106.13200) |\n",
    "| PyTorchRelevancePropagation | [GitHub](https://github.com/kaifishr/PyTorchRelevancePropagation) | PyTorch | Not specified | Not available |\n",
    "| TorchLRP | [GitHub](https://github.com/fhvilshoj/TorchLRP) | PyTorch | Not specified | Not available, not in PyPI => Not relevant for this study|\n",
    "| Layerwise-Relevance-Propagation-for-LSTMs | [GitHub](https://github.com/alewarne/Layerwise-Relevance-Propagation-for-LSTMs) | TensorFlow | Time Series (Specifically for LSTMs) | Not available |\n",
    "| LRP Toolbox | [GitHub](https://github.com/sebastian-lapuschkin/lrp_toolbox) | Matlab, Python (no PyTorch in requirements), Caffe | Not specified. Examples show images and text. | Not available |\n",
    "| lrp-pf-auc | [PyPI](https://pypi.org/project/lrp-pf-auc/) [Zenodo](https://zenodo.org/records/6821295) | Python (no PyTorch in requirements) | Not specified | Not available |\n",
    "| keras-explain | [PyPI](https://pypi.org/project/keras-explain/) | Keras | Not specified | Not available |\n",
    "| captum | [PyPI]() [GitHub]() | PyTorch | Not specified | [Paper](https://arxiv.org/abs/2009.07896) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are two options that can potentially be used for training our models and have associated papers: ```iNNvestigate```, ```Zennit``` and ```captum```. For simplicity, as we are focused on ```Pytorch``` and ```LRP```, ```zennit``` has been tested for integrating LRP into ```DeepVATS```. Basic use is shown in the next section going through its [tutorial](https://zennit.readthedocs.io/en/latest/getting-started.html), \n",
    "\n",
    "> Aquí intentos en Google Collab de usar captum y zennit con datos tabulares: [GCollab](https://colab.research.google.com/drive/1Bt-csfh1M-ttU2ww6akY_BwRJUA8uND5?authuser=0#scrollTo=lqgM6bIzHb6I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; display:inline-block;\">\n",
    "\n",
    "> TODO: Posible análisis para el paper/otro paper: analizar si iNNvestigate y captum se pueden incluir en DeepVATS y si merece la pena para poder visualizar otros modelos en el futuro o no. De primeras, Zennit me ha parecido el más sencillo de seguir y permite una selección clara de reglas usando tres clases sencillas para manejar el modelo\n",
    "</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1Bt-csfh1M-ttU2ww6akY_BwRJUA8uND5?authuser=0#scrollTo=QKjyonIIRS_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zennit: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zennit](https://zennit.readthedocs.io/en/latest/getting-started.html) is a library that implements **propagation-based attribution methods** by *overwriting the gradient of PyTorch modules* in PyTorch's auto-differentiation engine (the part associated to the automatic gradient calculus of 'complex' functions).  Zennit uses this engine for modifying the way gradients are computed within the attribution process, allowing to apply attribution methods based on propagation. \n",
    "\n",
    "*Zennit will only work on models which are strictly implemented using Pytorch modules. Including activation functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "\n",
    "\n",
    "##### Attribution process\n",
    "The \"attribution process\" in the context of neural networks and machine learning refers to the technique of determining how different parts of the input to a model contribute to its output. \n",
    "    \n",
    "The goal is to explain a model's decisions or predictions by identifying which input features are responsible for the final prediction and how much these features influence it. This process is crucial for understanding, interpreting, and trusting machine learning models, especially those that are complex and opaque, such as deep neural networks.\n",
    "\n",
    "Some key considerations about the attribution process include:\n",
    "    \n",
    "- **Interpretability:** Provides a clear insight into why a model makes certain decisions, which is especially important in fields where decisions need to be explainable and justifiable, such as in medicine or banking.\n",
    "\n",
    "- **Identification of Important Features:** Helps understand which features are most influential for the model's predictions, which can be useful for feature engineering or gaining a better understanding of the problem under study.\n",
    "\n",
    "- **Attribution Techniques:** There are different methods for conducting the attribution process, such as Layer-wise Relevance Propagation (LRP), Shapley Decomposition, Grad-CAM, and others. Each of these methods has its own advantages, limitations, and suitable use cases.\n",
    "\n",
    "- **Applications in Various Fields:** The attribution process is applied in a variety of fields, from image recognition and natural language processing to disease prediction and financial decision-making.\n",
    "\n",
    "  In summary, the attribution process is a fundamental part of analyzing machine learning models, providing transparency and understanding in how models make predictions or decisions based on input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Propagation-based attribution methods\n",
    "Explainable-AI techniques that propagate the  contribution of output neurons back to the input layers. Essentially, these methods attempt to explain how the input features of a model contribute to its final prediction. In the context of Zennit, these methods modify the gradients of the PyTorch modules during the autodifferentiation process to compute these contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Zennit structures\n",
    "The most important high-level structures in Zennit are ```composites```, ```Attributors```, and ```Canonizers```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composites\n",
    "Structures that map ```Rules``` to modules (torch.nn, MVP) based on their properties and context to modify their gradient. The most common composites for ```LRP``` are implemented in ```zennit.composites```.\n",
    "That is: \n",
    "\n",
    "- Map ```Rules``` to modules. Each module (convolutional layers, ReLU activation layers, ...) may need specifics ways for computing their contribution to the module output. ```Composites``` are used to assign different rules to those modules to define how to get those contributions.\n",
    "- Based on their propierties and context. The assignation of this rules is not random and depends on the charasteristics of each module and its context insithe the module.\n",
    "- This mapping/assignation changes the way gradient is computed within retropropagation.\n",
    "\n",
    "\n",
    "[Predefined composites](https://zennit.readthedocs.io/en/latest/reference/zennit.composites.html#module-zennit.composites): \n",
    "<span style=\"color:red; display:block;\">\n",
    "La relevancia se la he pedido a ChatGPT entendiendola como: demasiado metida en Imagen (esperará 3D), Aparentemente útil, Seguro que se puede usar. Hay que revisarlo investigando un poco más de cada una de ellas a la hora de fijarlas y de escribir el artículo. Tener en cuenta que nos interesa que sea algo especialmente genérico. O proponer las que sean úties y que el destinatario decida cuál quiere usar.\n",
    "</span>\n",
    "| Composite Name            | Description                                                        | Reference                                                                                     | Relevance for Time Series (MVP) |\n",
    "|---------------------------|--------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------|\n",
    "| BetaSmooth                | Explicit composite to modify ReLU gradients to smooth softplus gradients [Dombrowski et al., 2019]. | Dombrowski, A.-K., Alber, M., Anders, C. J., Ackermann, M., Müller, K.-R., & Kessel, P. (2019). Explanations can be manipulated and geometry is to blame. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 13567–13578. [Link](https://proceedings.neurips.cc/paper/2019/hash/bb836c01cdc9120a9c984c525e4b1a4a-Abstract.html) | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| DeconvNet                 | Modifying gradients of all ReLUs according to DeconvNet [Zeiler and Fergus, 2014]. | Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I, volume 8689 of Lecture Notes in Computer Science, 818–833. Springer. [Link](https://doi.org/10.1007/978-3-319-10590-1_53) | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant |\n",
    "| EpsilonAlpha2Beta1        | Alpha2-beta1 rule for convolutional and epsilon rule for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonAlpha2Beta1Flat    | Flat rule for first linear layer, alpha2-beta1 for convolutional, epsilon for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonGammaBox           | ZBox rule for first convolutional layer, gamma for following convolutional, epsilon for fully connected layers. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| EpsilonPlus               | Zplus rule for convolutional layers and epsilon rule for fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| EpsilonPlusFlat           | Flat rule for any first linear layer, zplus for other convolutional, epsilon for other fully connected layers. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| ExcitationBackprop        | Implementing ExcitationBackprop [Zhang et al., 2016]. | Zhang, J., Lin, Z. L., Brandt, J., Shen, X., & Sclaroff, S. (2016). Top-down neural attention by excitation backprop. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, 543–559. Springer. [Link](https://doi.org/10.1007/978-3-319-46493-0_33) | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant |\n",
    "| GuidedBackprop            | Modifying gradients of all ReLUs according to GuidedBackprop [Springenberg et al., 2015]. | Springenberg, J. T., Dosovitskiy, A., Brox, T., & Riedmiller, M. A. (2015). Striving for simplicity: the all convolutional net. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings. [Link](http://arxiv.org/abs/1412.6806) | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| LayerMapComposite         | A Composite for which hooks are specified by a mapping from module types to hooks. | -                                     | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful |\n",
    "| MixedComposite            | A Composite for which hooks are specified by a list of composites.  | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| NameLayerMapComposite     | A Composite for which hooks are specified by both a mapping from module names and module types to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| NameMapComposite          | A Composite for which hooks are specified by a mapping from module names to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n",
    "| SpecialFirstLayerMapComposite | A Composite for which hooks are specified by a mapping from module types to hooks. | -                                     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) Review |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Predefined rules](https://zennit.readthedocs.io/en/latest/how-to/use-rules-composites-and-canonizers.html#rules)\n",
    "| Rule Name              | Description                                                        | Relevance for Time Series (MVP) | Advantages                                      | Disadvantages                                 |\r\n",
    "|------------------------|--------------------------------------------------------------------|---------------------------------|-------------------------------------------------|-----------------------------------------------|\r\n",
    "| AlphaBeta              | Adaptable rule for different layer types.                          | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Adaptable to different layers.                   | Increases in complexity with layer depth.      |\r\n",
    "| Epsilon                | A stable rule, often used as a default for many layers.            | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful    | Simple and stable.                               | Might not capture all relevant features.      |\r\n",
    "| Flat                   | Suitable for input layers; provides a basic relevance mapping.     | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Beneficial for input layers.                     | Less informative for deeper layers.            |\r\n",
    "| Gamma                  | Balances positive and negative contributions in layers.            | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Balances positive and negative contributions.    | Sensitive to hyperparameter tuning.            |\r\n",
    "| ZBox                   | Specific to input normalisation processes.                         | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Specific to input normalization.                | Limited general applicability.                 |\r\n",
    "| ZPlus                  | Focuses on positive contributions from the layers.                 | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Focuses on positive contributions.               | Neglects negative contributions.               |\r\n",
    "| ZB                     | A balanced approach to attributing relevance.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Provides a balanced attribution.                | Requires careful calibration and tuning.       |\r\n",
    "| WSquare                | Emphasises the importance of weights in the network.              | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Highlights weight significance.                  | Not universally suitable for all networks.     |\r\n",
    "| WSquareFlat            | A combination of WSquare and Flat rules.                           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Combines features of WSquare and Flat rules.     | Limited in scope and application.              |\r\n",
    "| GuidedBackpropReLU     | Alters ReLU gradients for visualisation in convolutional networks. | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Focused on visualisation in CNNs.                | Specific to CNNs and similar architectures.    |\r\n",
    "| PatternAttribution     | Considers layer-wise patterns for attribution.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Accounts for layer-specific patterns.            | Complexity due to need for precomputed patterns. |\r\n",
    "| PatternNet             | Provides detailed layer-wise analysis based on patterns.           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Detailed layer analysis.                         | Requires extensive pre-computation.            |\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta los comentarios de ChatGPT y que queremos explicar las atribuciones del recorrido completo, usando varias capas, la tabla quedaría más bien así\n",
    "| Rule Name              | Description                                                        | Relevance for Time Series (MVP) | Advantages                                      | Disadvantages                                 |\n",
    "|------------------------|--------------------------------------------------------------------|---------------------------------|-------------------------------------------------|-----------------------------------------------|\n",
    "| AlphaBeta              | Adaptable rule for different layer types.                          | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Adaptable to different layers.                   | Increases in complexity with layer depth.      |\n",
    "| Epsilon                | A stable rule, often used as a default for many layers.            | ![#008000](https://via.placeholder.com/15/008000/000000?text=+) Useful    | Simple and stable.                               | Might not capture all relevant features.      |\n",
    "| Flat                   | Suitable for input layers; provides a basic relevance mapping.     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Beneficial for input layers.                     | Less informative for deeper layers.            |\n",
    "| Gamma                  | Balances positive and negative contributions in layers.            | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Balances positive and negative contributions.    | Sensitive to hyperparameter tuning.            |\n",
    "| ZBox                   | Specific to input normalisation processes.                         | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Specific to input normalization.                | Limited general applicability.                 |\n",
    "| ZPlus                  | Focuses on positive contributions from the layers.                 | ![#FFA500](https://via.placeholder.com/15/FFA500/000000?text=+) To Review | Focuses on positive contributions.               | Neglects negative contributions.               |\n",
    "| ZB                     | A balanced approach to attributing relevance.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Provides a balanced attribution.                | Requires careful calibration and tuning.       |\n",
    "| WSquare                | Emphasises the importance of weights in the network.              | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Highlights weight significance.                  | Not universally suitable for all networks.     |\n",
    "| WSquareFlat            | A combination of WSquare and Flat rules.                           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Combines features of WSquare and Flat rules.     | Limited in scope and application.              |\n",
    "| GuidedBackpropReLU     | Alters ReLU gradients for visualisation in convolutional networks. | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Focused on visualisation in CNNs.                | Specific to CNNs and similar architectures.    |\n",
    "| PatternAttribution     | Considers layer-wise patterns for attribution.                     | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Accounts for layer-specific patterns.            | Complexity due to need for precomputed patterns. |\n",
    "| PatternNet             | Provides detailed layer-wise analysis based on patterns.           | ![#808080](https://via.placeholder.com/15/808080/000000?text=+) Not Relevant | Detailed layer analysis.                         | Requires extensive pre-computation.            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributors\n",
    "\n",
    "[Attributtors](https://zennit.readthedocs.io/en/latest/how-to/write-custom-attributors.html) provide an additional layer of abstraction over the context of Composites. \n",
    "\n",
    "They are used to directly produce attributions, which may or may not be computed with modified gradients, if they are used, from Composites. \n",
    "More information on Attributors, examples and their use can be found in [Using Attributors](https://zennit.readthedocs.io/en/latest/how-to/use-attributors.html).\r\n",
    "\r\n",
    "Attributors can be used to implement non-layer-wise or only partly layer-wise attribution methods. For this, it is enough to define a subclass of zennit.attribution.Attributor and implement its forward() and optionally its __init__() methods\n",
    "We are focused on Layer-wise-retropropagation. However, in order to check the tool. The example attributor based in Gradient method will be implemented..: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Canonizers(https://zennit.readthedocs.io/en/latest/how-to/use-rules-composites-and-canonizers.html)\n",
    "\n",
    "Zennit implements propagation-based attribution methods by overwriting the gradient of PyTorch modules within PyTorch’s auto-differentiation engine. There are three building blocks in Zennit to achieve attributions: Rules, Composites and Canonizers. In short, Rules specify how to overwrite the gradient, Composites map rules to modules, and Canonizers transform some module types and configurations to a canonical form, necessary in some cases.\n",
    "\n",
    "For some modules and operations, Layer-wise Relevance Propagation (LRP) is not implementation-invariant, eg. ```BatchNorm -> Dense -> ReLU``` will be attributed differently than ```Dense -> BatchNorm -> ReLU```. \n",
    "\n",
    "Therefore, LRP needs a canonical form of the model, which is implemented in Canonizers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute LRP relevance \n",
    "> Following the Startting Guide & using relevat composites, attributtors and canonizers according to chatGPT. Well, attributtors should not at first be used, but let's check how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submodules summary for rules decision\n",
    "\n",
    "Before computing the LRP relevance, let's check the layer in the model so we can better decide the rules that we should use according to their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = learner_module_leaves_subtables(enc_learner, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data to be analysed\n",
    "> TODO: Addapt so only a specific range of TS is used according to a selected ProjectionPoints plot section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.core import get_ts_dls\n",
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input = enc_input #[ ::enc_artifact.metadata['stride']]\n",
    "lrp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 30\n",
    "stride = 5\n",
    "print(enc_artifact.metadata['stride'])\n",
    "#stride = enc_artifact.metadata['stride']\n",
    "splits = get_forecasting_splits(\n",
    "        df = df, \n",
    "        fcst_history = w,\n",
    "        fcst_horizon = 1,\n",
    "        stride = enc_artifact.metadata['stride'], \n",
    "        test_size = 0.2,\n",
    "        show_plot = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [ToFloat(), None]\n",
    "batch_tfms = [\n",
    "    TSStandardize(\n",
    "        by_sample=enc_artifact.metadata['norm_by_sample'],\n",
    "        use_single_batch=enc_artifact.metadata['norm_use_single_batch']\n",
    "    )\n",
    "]\n",
    "dls = get_ts_dls(\n",
    "    lrp_input, \n",
    "    splits=splits, \n",
    "    tfms=tfms, \n",
    "    bs=enc_artifact.metadata['batch_size'], \n",
    "    batch_tfms=batch_tfms\n",
    ")\n",
    "dls.show_at(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input = enc_input\n",
    "print(lrp_input.shape)\n",
    "lrp_input_strided = lrp_input[::enc_artifact.metadata['stride']]\n",
    "print(lrp_input_strided.shape)\n",
    "print(type(lrp_input_strided))\n",
    "rows_with_nan = np.isnan(lrp_input_strided).any(axis=(1,2))\n",
    "lrp_input_strided = lrp_input_strided[~rows_with_nan]\n",
    "lrp_input_strided.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the input requires a gradient\n",
    "lrp_input_torch = torch.cuda.FloatTensor(lrp_input_strided)\n",
    "lrp_input_torch.requires_grad = True\n",
    "# Ensure input to be in GPU\n",
    "lrp_input_torch.to('cuda')\n",
    "lrp_input_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window_size = enc_artifact.metadata['w']\n",
    "#window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = enc_artifact.metadata['batch_size']\n",
    "#batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model to be in GPU\n",
    "model = enc_learner.model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: composite (basic version)\n",
    "\n",
    "> Using ```EpsilonPlusFlat``` composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a composite instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.composites import EpsilonPlusFlat\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite instance\n",
    "composite = EpsilonPlusFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output and gradient within the composite's context\n",
    "def compute_output_and_gradient(model, input_data):\n",
    "    model.eval()\n",
    "    # Just in case to check if the difference in the relevance is cause of no-reversion (not should as it is inside with)\n",
    "    original_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    with composite.context(model) as modified_model:\n",
    "        output = modified_model(input_data)\n",
    "        output.backward(\n",
    "            gradient=torch.ones_like(input_data),\n",
    "            retain_graph = True\n",
    "        ) \n",
    "        relevance = torch.autograd.grad(\n",
    "            output, input_data, \n",
    "            torch.ones_like(input_data),\n",
    "            retain_graph=True\n",
    "        )\n",
    "    model.load_state_dict(original_state_dict)\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Revisar por qué salen NaNs\n",
    "output, relevance = compute_output_and_gradient(model, lrp_input_torch)\n",
    "relevance = relevance[0]\n",
    "relevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce relevance dimensions to get an array with a relevance per each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(relevance):\n",
    "    #Get the mean per each feature\n",
    "    print(relevance.shape)\n",
    "    importances = relevance.nanmean(axis=2)\n",
    "    print(importances.shape)\n",
    "    #Get a global mean instead of one per sample\n",
    "    importances_mean = importances.nanmean(dim=0).detach().cpu().numpy()\n",
    "    print(importances_mean.shape)\n",
    "    #print(importances_mean)\n",
    "    #Take care of negative values\n",
    "    min_importance = min(importances_mean)\n",
    "    if  min_importance < 0:\n",
    "        importances_mean = importances_mean - min_importance\n",
    "    print(min(importances_mean))\n",
    "    print(importances_mean)\n",
    "    #See as percentage\n",
    "    importances_sum = np.nansum(importances_mean)\n",
    "    importances_percentage = (importances_mean/importances_sum)\n",
    "    print(importances_percentage)\n",
    "    return importances_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = reduce_dimensions(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = list(df.columns)\n",
    "features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create auxiliar function to check a diagram bar\n",
    "> TODO: Think how a heatmap would be sawn as and implement if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_importance(title, features_names, importances_percentage):\n",
    "    # Crear diagrama de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(features_names, importances_percentage)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(title + '| Features importance (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance ('LRP', features_names, importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the two most meaningful features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meaningful_ids(importances, features_names, numvars = 2):\n",
    "    meaningful_ids = np.argsort(importances)[-numvars:]\n",
    "    meaningful_features = [(i,features_names[i], importances[i]) for i in meaningful_ids]\n",
    "    return meaningful_features, meaningful_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features, meaningful_ids = get_meaningful_ids(importances, features_names, 2)\n",
    "meaningful_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: attributtors \n",
    "> Using ```SmoothGrad``` attributor as in the example\n",
    ">\n",
    "> Allows to use other attribution-based XAI techniques\n",
    "> \n",
    "> Not relevant as we want to use RLP. But tried for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.attribution import SmoothGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributor = SmoothGrad(model, noise_level=0.1, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_and_relevance_atributor(model, attributor, input):\n",
    "    # we do not need a composite to compute vanilla SmoothGrad\n",
    "    model.eval()\n",
    "    #with atributor_ as attributor:\n",
    "    with attributor:\n",
    "         # gradient/ relevance\n",
    "         output, relevance = attributor(\n",
    "             input, \n",
    "             torch.ones_like(input)\n",
    "        )\n",
    "    print('Attributor:', relevance[:2], relevance.shape)\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_smooth_grad = compute_output_and_relevance_atributor(model, attributor, lrp_input_torch)\n",
    "print(relevance_smooth_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_attributor = reduce_dimensions(relevance_smooth_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Attributor Smooth Grad', features_names, importances_attributor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_attributors, meaningful_ids_attributors = get_meaningful_ids(importances_attributor, features_names)\n",
    "meaningful_features_attributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to compare\n",
    "meaningful_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3: Canonizers & Attributor & Rule\n",
    "> Using ```SequentialMergeBatchNorm``` canonizer\n",
    "\n",
    "> and ```Gradient``` attributor\n",
    "\n",
    "> and ```EpsilonGammaBox``` as rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.attribution import Gradient\n",
    "from zennit.composites import EpsilonGammaBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizers = [SequentialMergeBatchNorm()]\n",
    "composite = EpsilonGammaBox(low=-3., high=3., canonizers=canonizers)\n",
    "attributor = Gradient(model, composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output_and_relevance_canonizer_atributor(model, composite, attributor, input):\n",
    "    composite.register(model)\n",
    "    with attributor:\n",
    "        output, relevance = attributor(\n",
    "             input,\n",
    "             torch.ones_like(input)\n",
    "        )\n",
    "    composite.remove()\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_canonizer_atributor_rule = compute_output_and_relevance_canonizer_atributor(model, composite, attributor, lrp_input_torch)\n",
    "importances_canonizer_atributor_rule = reduce_dimensions(relevance_canonizer_atributor_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_canonizer_atributor_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_canonizer_attributor_rule, meaningful_ids_canonizer_attributor_rule = get_meaningful_ids(importances_canonizer_atributor_rule, features_names)\n",
    "meaningful_features_canonizer_attributor_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumarise\")\n",
    "print(\"Composite meaningful features: \", meaningful_features)\n",
    "print(\"Attributor meaningful features: \", meaningful_features_attributors)\n",
    "print(\"Canonizer & Atributor & Rule meaningful features: \", meaningful_features_canonizer_attributor_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 4: Canonizer & Rule\n",
    "> The best option for our goal. Next step: select specific rules for our model (MPV)\n",
    "\n",
    "> Using ```SequentialMergeBatchNorm``` composite\n",
    "\n",
    "> and ```EpsilonGammaBox``` as rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.composites import EpsilonGammaBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizer2 = SequentialMergeBatchNorm()\n",
    "composite2 = EpsilonGammaBox(low=-3., high=3., canonizers=canonizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output and gradient within the composite's context\n",
    "def compute_output_and_gradient_canonizer_rule(model, composite2, input_data):\n",
    "    model.eval()\n",
    "    \n",
    "    composite.register(model)\n",
    "\n",
    "    #Do something with the model\n",
    "    output = model(input_data)\n",
    "    \n",
    "    relevance, = torch.autograd.grad(\n",
    "        output, input_data, \n",
    "        torch.ones_like(input_data),\n",
    "        retain_graph=True\n",
    "    )\n",
    "    \n",
    "    composite.remove()\n",
    "    return output, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_canonizer_rule = compute_output_and_gradient_canonizer_rule(model, composite2, lrp_input_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_canonizer_rule = reduce_dimensions(relevance_canonizer_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_canonizer_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_canonizer_rule, meaningful_ids_canonizer_rule = get_meaningful_ids(importances_canonizer_rule, features_names)\n",
    "meaningful_features_canonizer_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sumarise\")\n",
    "print(\"Composite meaningful features: \", meaningful_features)\n",
    "print(\"Attributor meaningful features: \", meaningful_features_attributors)\n",
    "print(\"Canonizer & Atributor & Rule meaningful features: \", meaningful_features_canonizer_attributor_rule)\n",
    "print(\"Canonizer & Rule meaningful features: \", meaningful_features_canonizer_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MVP, the modules types are \n",
    "\n",
    "- Add\n",
    "- BatchNorm1d\n",
    "- Concat\n",
    "- Conv1d\n",
    "- Dropout\n",
    "- MaxPool1d\n",
    "- ReLU\n",
    "\n",
    "The main for LRP analysis are the following, sumarised to check the rules and composites that may be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer Type  | Rule Recommendation | Composite Recommendation | Canonizer Recommendation | Notes |\n",
    "|-------------|---------------------|--------------------------|-------------------------|-------|\n",
    "| BatchNorm1d | - | - | SequentialMergeBatchNorm() for ensuring correct execution order | Ideal for normalizing batch layers. |\n",
    "| Conv1d      | Epsilon rule | Epsilon - * | - | Further investigation needed to select the specific composite. |\n",
    "| MaxPool1d   | - | Epsilon | - | Epsilon is stable, no change needed. |\n",
    "| ReLU        | - | BetaSmooth; DeconvNet | - | DeconvNet may be more suited to visual computing. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, final implementation should look like ```compute_output_and_gradient_canonizer_rule``` version. The next step is to select the parameters for epsilon composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you must select a subset of the plot. By default, some random indices will be selected at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U plotly\n",
    "#! pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#! mamba install canvas -c conda-forge | No conseguido, usar pip install canvas si hace falta | En revisión\n",
    "#! mamba install -y -c conda-forge ipympl==0.9.3 #0.5.1\n",
    "#! mamba update -y -c conda-forge nbdime\n",
    "#! conda install -y -c conda-forge ipywidgets\n",
    "#! jupyter nbextension enable --py --sys-prefix ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output, HTML as IPHTML\n",
    "from ipywidgets import Button, Output, VBox, HBox, HTML, Layout, FloatSlider\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import plotly.io as pio\n",
    "import kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_prjs_points_total = min(prjs_umap.shape[0], 10)\n",
    "selected_indices = np.random.permutation(lrp_input_torch.size(0))[:selected_prjs_points_total]\n",
    "print(selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save(fig):\n",
    "    image_bytes = pio.to_image(fig, format='png')\n",
    "    with open(f\"../imgs/w={w}.png\", 'wb') as f:\n",
    "        f.write(image_bytes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies(df, threshold, flag):\n",
    "    df['anomaly'] = [ (score > threshold) and flag for score in df['anomaly_score']]\n",
    "    \n",
    "def get_anomaly_styles(df, threshold, flag = False, print_flag = False):\n",
    "        if print_flag: print(\"Threshold: \", threshold)\n",
    "        if print_flag: print(\"Flag\", flag)\n",
    "        if print_flag: print(\"df ~\", df.shape)\n",
    "        df['anomaly'] = [ (score > threshold) and flag for score in df['anomaly_score'] ]\n",
    "        print(df)\n",
    "        get_anomalies(df, threshold, flag)\n",
    "        anomalies = df[df['anomaly']]\n",
    "        if flag:\n",
    "            df['anomaly'] = [ \n",
    "                (score > threshold) and flag \n",
    "                for score in anomaly_scores \n",
    "            ]\n",
    "            symbols = [\n",
    "                'x' if is_anomaly else 'circle' \n",
    "                for is_anomaly in df['anomaly']\n",
    "            ]\n",
    "            line_colors = [\n",
    "                'black'\n",
    "                if (is_anomaly and flag) else 'rgba(0,0,0,0)'\n",
    "                for is_anomaly in df['anomaly']\n",
    "            ]\n",
    "        else:\n",
    "            symbols = ['circle' for _ in df['x1']]\n",
    "            line_colors = ['rgba(0,0,0,0)' for _ in df['x1']]\n",
    "        if print_flag: print(anomalies)\n",
    "        return symbols, line_colors\n",
    "### Example of use\n",
    "#prjs_df = pd.DataFrame(prjs_umap, columns = ['x1', 'x2'])\n",
    "#prjs_df['anomaly_score'] = anomaly_scores\n",
    "#s, l = get_anomaly_styles(prjs_df, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_initial_config(prjs, cluster_labels, anomaly_scores):\n",
    "    prjs_df = pd.DataFrame(prjs, columns = ['x1', 'x2'])\n",
    "    prjs_df['cluster'] = cluster_labels\n",
    "    prjs_df['anomaly_score'] = anomaly_scores\n",
    "    \n",
    "    cluster_colors_df = pd.DataFrame({'cluster': cluster_labels}).drop_duplicates()\n",
    "    cluster_colors_df['color'] = px.colors.qualitative.Set1[:len(cluster_colors_df)]\n",
    "    cluster_colors = dict(zip(cluster_colors_df['cluster'], cluster_colors_df['color']))\n",
    "    return prjs_df, cluster_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projections_clusters_interactive(\n",
    "    prjs,\n",
    "    cluster_labels, \n",
    "    umap_params,\n",
    "    anomaly_scores = [],\n",
    "    threshold = 0.15,\n",
    "    fig_size = (7,7),\n",
    "    anomaly_flag = False, \n",
    "    print_flag = False\n",
    "):\n",
    "    \n",
    "    global selected_indices\n",
    "    threshold_ = threshold\n",
    "    \n",
    "    selected_indices_tmp = selected_indices\n",
    "    py.init_notebook_mode()\n",
    "    \n",
    "    prjs_df, cluster_colors = plot_initial_config(prjs, cluster_labels, anomaly_scores)\n",
    "\n",
    "    legend_items = [widgets.HTML(f'<b>Cluster {cluster}:</b> <span style=\"color:{color};\">■</span>')\n",
    "                    for cluster, color in cluster_colors.items()]\n",
    "    legend = widgets.VBox(legend_items)\n",
    "\n",
    "    marker_colors = prjs_df['cluster'].map(cluster_colors)\n",
    "    \n",
    "    symbols, line_colors = get_anomaly_styles(prjs_df, threshold_, anomaly_flag, print_flag)\n",
    "    \n",
    "    \n",
    "    fig = go.FigureWidget(\n",
    "        [\n",
    "            go.Scatter(\n",
    "                x=prjs_df['x1'], y=prjs_df['x2'], \n",
    "                mode=\"markers\", \n",
    "                marker= {\n",
    "                    'color': marker_colors,\n",
    "                    'line': { 'color': line_colors, 'width': 1 },\n",
    "                    'symbol': symbols\n",
    "                },\n",
    "                text = prjs_df.index\n",
    "\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    line_trace = go.Scatter(\n",
    "        x=prjs_df['x1'],  # Reemplaza 'x1' y 'x2' con los nombres de tus columnas de datos\n",
    "        y=prjs_df['x2'],  # Reemplaza 'x1' y 'x2' con los nombres de tus columnas de datos\n",
    "        mode=\"lines\",  # Establece el modo en \"lines\"\n",
    "        line=dict(color='rgba(128, 128, 128, 0.5)', width=1),\n",
    "        showlegend=False  # Puedes configurar si deseas mostrar esta línea en la leyenda\n",
    "    )\n",
    "\n",
    "    fig.add_trace(line_trace)\n",
    "\n",
    "    sca = fig.data[0]\n",
    "    \n",
    "    fig.update_layout(\n",
    "        dragmode='lasso',\n",
    "        width=700, \n",
    "        height=500,\n",
    "        title={\n",
    "            'text': '<span style=\"font-weight:bold\">DR params - n_neighbors:{:d} min_dist:{:f}</span>'.format(\n",
    "                     umap_params['n_neighbors'], umap_params['min_dist']),\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='#f0f0f0',\n",
    "        xaxis=dict(gridcolor='lightgray', zerolinecolor='black', title = 'x'), \n",
    "        yaxis=dict(gridcolor='lightgray', zerolinecolor='black', title = 'y'),\n",
    "        margin=dict(l=10, r=20, t=30, b=10)\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    output_tmp = Output()\n",
    "    output_button = Output()\n",
    "    output_anomaly = Output()\n",
    "\n",
    "    \n",
    "    def select_action(trace, points, selector):\n",
    "        global selected_indices_tmp\n",
    "        selected_indices_tmp = points.point_inds\n",
    "        with output_tmp:\n",
    "            output_tmp.clear_output(wait=True)\n",
    "            print(\"Selected indices tmp:\", selected_indices_tmp)\n",
    "        \n",
    "    def button_action(b):\n",
    "        global selected_indices\n",
    "        global selected_indices_tmp\n",
    "        selected_indices = selected_indices_tmp \n",
    "        with output_button: \n",
    "            output_button.clear_output(wait = True)\n",
    "            print(\"Selected indices:\", selected_indices)\n",
    "\n",
    "    \n",
    "    def update_anomalies(anomaly_flag):\n",
    "        nonlocal threshold_\n",
    "        nonlocal anomaly_scores\n",
    "        nonlocal fig\n",
    "        nonlocal print_flag\n",
    "        \n",
    "        if print_flag: print(\"About to update anomalies\")\n",
    "        symbols, line_colors = get_anomaly_styles(prjs_df,threshold_,anomaly_flag)\n",
    "\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].marker.symbol = symbols\n",
    "            fig.data[0].marker.line.color = line_colors\n",
    "\n",
    "        if print_flag: print(\"Threshold: \", threshold_)\n",
    "        if print_flag: print(\"Scores: \", anomaly_scores)\n",
    "        \n",
    "              \n",
    "    def anomaly_action(b):\n",
    "        nonlocal anomaly_flag\n",
    "        anomaly_flag = not anomaly_flag\n",
    "        with output_anomaly:  # Cambia output_flag a output_anomaly\n",
    "            output_anomaly.clear_output(wait=True)\n",
    "            print(\"Show anomalies:\", anomaly_flag)\n",
    "            update_anomalies(anomaly_flag)\n",
    "                \n",
    "            \n",
    "            \n",
    "    \n",
    "    sca.on_selection(select_action)\n",
    "\n",
    "    layout = widgets.Layout(width='auto', height='40px')\n",
    "    button = Button(\n",
    "        description=\"Update selected_indices\",\n",
    "        style = {'button_color': 'lightblue'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    anomaly_button = Button(\n",
    "        description = \"Show anomalies\",\n",
    "        style = {'button_color': 'lightgray'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    \n",
    "    button.on_click(button_action)\n",
    "    anomaly_button.on_click(anomaly_action)\n",
    "\n",
    "    ##### Reactivity buttons\n",
    "    pause_button = Button(\n",
    "        description = \"Pause interactiveness\",\n",
    "        style = {'button_color': 'pink'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "    resume_button = Button(\n",
    "        description = \"Resume interactiveness\",\n",
    "        style = {'button_color': 'lightgreen'},\n",
    "        display = 'flex',\n",
    "        flex_row = 'column',\n",
    "        align_items = 'stretch',\n",
    "        layout = layout\n",
    "    )\n",
    "\n",
    "\n",
    "    threshold_slider = FloatSlider(\n",
    "        value=threshold_,\n",
    "        min=0.0,\n",
    "        max=float(np.ceil(threshold+5)),\n",
    "        step=0.01,\n",
    "        description='Anomaly threshold:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    interaction_enabled = True\n",
    "    def pause_interaction(b):\n",
    "        global interaction_enabled\n",
    "        interaction_enabled = False\n",
    "        fig.update_layout(dragmode='pan')\n",
    "    \n",
    "    def resume_interaction(b):\n",
    "        global interaction_enabled\n",
    "        interaction_enabled = True\n",
    "        fig.update_layout(dragmode='lasso')\n",
    "\n",
    "    \n",
    "    def update_threshold(change):\n",
    "        nonlocal threshold_\n",
    "        nonlocal anomaly_flag\n",
    "        threshold_ = change.new\n",
    "        update_anomalies(anomaly_flag)\n",
    "        \n",
    "\n",
    "    pause_button.on_click(pause_interaction)\n",
    "    resume_button.on_click(resume_interaction)\n",
    "    \n",
    "    threshold_slider.observe(update_threshold, 'value')\n",
    "    \n",
    "    #####\n",
    "    space = HTML(\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\") \n",
    "    \n",
    "    vbox = VBox((output_tmp, output_button, output_anomaly, fig))\n",
    "    hbox = HBox((space, button, space, pause_button, space, resume_button, anomaly_button))\n",
    "    \n",
    "    # Centrar las dos cajas horizontalmente en el VBox\n",
    "\n",
    "    box_layout = widgets.Layout(display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='center',\n",
    "                width='100%')\n",
    "\n",
    "    if anomaly_flag:\n",
    "        box = VBox((hbox,threshold_slider,vbox), layout = box_layout)\n",
    "    else: \n",
    "        box = VBox((hbox,vbox), layout = box_layout)\n",
    "    box.add_class(\"layout\")\n",
    "    plot_save(fig)\n",
    "    \n",
    "    display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Lo suyo sería pasar cosas a funciones/a W&B y que el notebook empezara directamente aquí, usando para prjs el array de proyecciones que se haya calculado en el notebook anterior en lugar de volverlo a recalcular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections_clusters_interactive(\n",
    "    prjs_umap, \n",
    "    clusters_labels, \n",
    "    umap_params, \n",
    "    anomaly_flag = True,\n",
    "    anomaly_scores = anomaly_scores,\n",
    "    threshold = threshold, \n",
    "    print_flag = True #TODO: Revisar que en algún sitio he puesto mal el if print_flag: y deja de mostrar las anomalías en False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold_slider = widgets.FloatSlider(value=2, min=1, max=3, step=0.01, description='Threshold')\n",
    "#widgets.interactive(update_plot, threshold=threshold_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_indices = [31, 100, 64,  53 , 88 , 98 , 61 , 35,  10,  62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain relevance scores for selected projection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input_subset_torch = lrp_input_torch[selected_indices]\n",
    "lrp_input_subset_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "canonizer_subset = SequentialMergeBatchNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relevance_subset = compute_output_and_gradient_canonizer_rule(model, canonizer_subset, lrp_input_subset_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_subset = reduce_dimensions(relevance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_importance('LRP | Canonizer SequentialMergeBatchNorm', features_names, importances_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meaningful_features_subset, meaningful_features_subset_ids = get_meaningful_ids(importances_canonizer_rule, features_names)\n",
    "meaningful_features_subset, meaningful_features_subset_ids = get_meaningful_ids(importances_canonizer_rule, features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_features_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking back points of the 2D projection to the original time series\n",
    "The variable ```selected_indices``` contains an array of the selected points indices selected in the previous 2D projection. From this indices, we will get the corresponding windows of the original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save for checking in windows_range values if neccesary \n",
    "#def original_indices(prjs, selected_indices) : \n",
    "#    prjs_df = pd.DataFrame(prjs)\n",
    "\n",
    "#    if selected_indices is None:\n",
    "#        selected_indices = prjs_df.sample(n=n_windows).index\n",
    "    \n",
    "#    n_windows = len(selected_indices)\n",
    "\n",
    "#    selected_windows = lrp_input[::stride][selected_indices]\n",
    "#    return selected_windows \n",
    "#selected_windows = original_indices(prjs_umap, selected_indices)\n",
    "#print(selected_indices)\n",
    "#print(selected_windows)\n",
    "#selected_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_selected(df, selected_indices, w, stride = 1): #Cuidado con stride\n",
    "    n_windows = len(selected_indices)\n",
    "    window_ranges = [(id*stride, (id*stride)+w) for id in selected_indices]    \n",
    "    #window_ranges = [(id*w, (id+1)*w+1) for id in selected_indices]    \n",
    "    #window_ranges = [(id*stride, (id*stride)+w) for id in selected_indices]\n",
    "    #print(window_ranges)\n",
    "    valores_tramos = [df.iloc[inicio:fin+1] for inicio, fin in window_ranges]\n",
    "    df_selected = pd.concat(valores_tramos, ignore_index=False)\n",
    "    return window_ranges, n_windows, df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_indices)\n",
    "wr, nw, dfs = get_df_selected(df, selected_indices, w, stride) #1)\n",
    "print(wr)\n",
    "print(nw)\n",
    "#dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def shift_datetime(dt, seconds, sign, dateformat=\"%Y-%m-%d %H:%M:%S.%f\", print_flag = False):\n",
    "        if print_flag: print(dateformat)\n",
    "        dateformat2= \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "        dateformat3 = \"%Y-%m-%d\"\n",
    "        ok = False\n",
    "        try: \n",
    "            if print_flag: print(\"dt \", dt, \"seconds\", seconds, \"sign\", sign)\n",
    "            new_dt = datetime.strptime(dt, dateformat)\n",
    "            if print_flag: print(\"ndt\", new_dt)\n",
    "            ok = True\n",
    "        except ValueError as e:\n",
    "            if print_flag: \n",
    "                print(\"Error: \", e)\n",
    "        \n",
    "        if (not ok):\n",
    "            try:\n",
    "                if print_flag: print(\"Parsing alternative dataformat\", dt, \"seconds\", seconds, \"sign\", sign, dateformat2)\n",
    "                new_dt = datetime.strptime(dt, dateformat3)\n",
    "                if print_flag: print(\"2ndt\", new_dt)\n",
    "            except ValueError as e:\n",
    "                print(\"Error: \", e)\n",
    "        if print_flag: print(new_dt)\n",
    "        try:\n",
    "            \n",
    "            if new_dt.hour == 0 and new_dt.minute == 0 and new_dt.second == 0:\n",
    "                if print_flag: \"Aqui\"\n",
    "                new_dt = new_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                if print_flag: print(new_dt)\n",
    "\n",
    "            if print_flag: print(\"ndt\", new_dt)\n",
    "                \n",
    "            if (sign == '+'):\n",
    "                if print_flag: print(\"Aqui\")\n",
    "                new_dt = new_dt + timedelta(seconds = seconds)\n",
    "                if print_flag: print(new_dt)\n",
    "            else: \n",
    "                if print_flag: print(sign, type(dt))\n",
    "                new_dt = new_dt - timedelta(seconds = seconds)\n",
    "                if print_flag: print(new_dt)\n",
    "            \n",
    "            \n",
    "            if new_dt.hour == 0 and new_dt.minute == 0 and new_dt.second == 0:\n",
    "                if print_flag: print(\"replacing\")\n",
    "                new_dt = new_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                \n",
    "            new_dt_str = new_dt.strftime(dateformat)\n",
    "            if print_flag: print(\"new dt \", new_dt)\n",
    "        except ValueError as e:\n",
    "            if print_flag: print(\"Aqui3\")\n",
    "            shift_datetime(dt, 0, sign, dateformat = \"%Y-%m-%d\", print_flag = False)\n",
    "            return str(e)\n",
    "        return new_dt_str\n",
    "\n",
    "shift_datetime('1970-01-01', 10, '+', print_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_plot_interactive(\n",
    "    df, selected_indices, meaningful_features_subset_ids, w, stride = 1, print_flag = False\n",
    "):\n",
    "    window_ranges, n_windows, df_selected = get_df_selected(df, selected_indices, w, stride)\n",
    "\n",
    "    if print_flag: print(n_windows, window_ranges)\n",
    "    if print_flag: print(df_selected.index)\n",
    "    \n",
    "    df.index = df.index.astype(str)\n",
    "    dateformat = '%Y-%m-%d %H:%M:%S'\n",
    "    #df.index = pd.to_datetime(df.index)\n",
    "    #df.index = df.index.strftime(dateformat)\n",
    "    \n",
    "    fig = go.FigureWidget()\n",
    "    \n",
    "    colors = [f'rgb({np.random.randint(0, 256)}, {np.random.randint(0, 256)}, {np.random.randint(0, 256)})' for _ in range(n_windows)]\n",
    "    \n",
    "    # Agregar cada serie al gráfico con sombreado si está en df_selected\n",
    "    output_windows = Output()\n",
    "    for feature_id in df.columns:\n",
    "        feature_pos = df.columns.get_loc(feature_id)\n",
    "        trace = go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[feature_id],\n",
    "            mode='lines',\n",
    "            name=feature_id,\n",
    "            visible=feature_pos in meaningful_features_subset_ids,\n",
    "            text=df.index\n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "        \n",
    "    # Aplicar sombreado a las ventanas dentro de df_selected\n",
    "    for i, (start, end) in enumerate(window_ranges):\n",
    "        \n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=df.index[start],\n",
    "            x1=df.index[end],\n",
    "            y0= df[feature_id].min(),\n",
    "            y1= df[feature_id].max(),\n",
    "            fillcolor=colors[i], #\"LightSalmon\",\n",
    "            opacity=0.25,\n",
    "            layer=\"below\",\n",
    "            line=dict(color=colors[i], width=1),\n",
    "            name = f\"w_{i}\"\n",
    "        )\n",
    "        with output_windows:\n",
    "            print(\"w[\" + str( selected_indices[i] )+ \"]=\"+str(df.index[start])+\", \"+str(df.index[end])+\")\")\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Time Series with time window plot',\n",
    "        xaxis_title='Datetime',\n",
    "        yaxis_title='Value',\n",
    "        legend_title='Variables',\n",
    "        margin=dict(l=10, r=10, t=30, b=10),\n",
    "        xaxis=dict(\n",
    "            tickformat=dateformat#,\n",
    "            #grid_color = 'lightgray', zerolinecolor='black', title = 'x'\n",
    "        ),\n",
    "        #yaxis = dict(grid_color = 'lightgray', zerolinecolor='black', title = 'y'),\n",
    "        #plot_color = 'white',\n",
    "        paper_bgcolor='#f0f0f0'\n",
    "    )\n",
    "\n",
    "    # Función para manejar el evento del botón\n",
    "    def toggle_trace(button):\n",
    "        idx = button.description\n",
    "        trace = fig.data[df.columns.get_loc(idx)]\n",
    "        trace.visible = not trace.visible\n",
    "\n",
    "    # Crear un botón para cada variable\n",
    "    buttons = [\n",
    "        Button(\n",
    "            description=str(feature_id),\n",
    "            button_style='success' if df.columns.get_loc(feature_id) in meaningful_features_subset_ids else '') \n",
    "        for feature_id in df.columns\n",
    "    ]\n",
    "\n",
    "    for button in buttons:\n",
    "        button.on_click(toggle_trace)\n",
    "\n",
    "    output_move = Output()\n",
    "    output_delta_x = Output()\n",
    "    output_delta_y = Output()\n",
    "    \n",
    "\n",
    "    delta_x = 10   \n",
    "    delta_y = 0.1\n",
    "    \n",
    "    def move_left(button):\n",
    "        with output_move:\n",
    "            output_move.clear_output(wait=True)\n",
    "            start_date, end_date = fig.layout.xaxis.range\n",
    "            new_start_date = shift_datetime(start_date, delta_x, '-', dateformat) \n",
    "            new_end_date = shift_datetime(end_date, delta_x, '-', dateformat) \n",
    "            with fig.batch_update():\n",
    "                fig.layout.xaxis.range = [new_start_date, new_end_date]\n",
    "\n",
    "    def move_right(button):\n",
    "        output_move.clear_output(wait=True)\n",
    "        with output_move:\n",
    "            start_date, end_date = fig.layout.xaxis.range\n",
    "            new_start_date = shift_datetime(start_date, delta_x, '+', dateformat) \n",
    "            new_end_date = shift_datetime(end_date, delta_x, '+', dateformat) \n",
    "            with fig.batch_update():\n",
    "                fig.layout.xaxis.range = [new_start_date, new_end_date]\n",
    "        \n",
    "    def move_down(button):\n",
    "        with output_move:\n",
    "            output_move.clear_output(wait=True)\n",
    "            start_y, end_y = fig.layout.yaxis.range\n",
    "            with fig.batch_update():\n",
    "                fig.layout.yaxis.range = [start_y-delta_y, end_y-delta_y]\n",
    "\n",
    "    def move_up(button):\n",
    "        with output_move:\n",
    "            output_move.clear_output(wait=True)\n",
    "            start_y, end_y = fig.layout.yaxis.range\n",
    "            with fig.batch_update():\n",
    "                fig.layout.yaxis.range = [start_y+delta_y, end_y+delta_y]\n",
    "\n",
    "    def delta_x_bigger():\n",
    "        nonlocal delta_x, delta_y\n",
    "        with output_delta_x: \n",
    "            output_delta_x.clear_output(wait = True)\n",
    "            print(\"Delta before\", delta_x)\n",
    "            delta_x = delta_x*10\n",
    "            #print(\"Bigger delta_x\")\n",
    "            print(\"delta_x:\", delta_x)\n",
    "\n",
    "    def delta_y_bigger():\n",
    "        nonlocal delta_x, delta_y\n",
    "        with output_delta_y: \n",
    "            output_delta_y.clear_output(wait = True)\n",
    "            print(\"Delta before\", delta_y)\n",
    "            delta_y = delta_y * 10\n",
    "            print(\"delta_y:\", delta_y)\n",
    "\n",
    "    def delta_x_lower():\n",
    "        nonlocal delta_x, delta_y\n",
    "        with output_delta_x:\n",
    "            output_delta_x.clear_output(wait = True)\n",
    "            print(\"Delta before\", delta_x)\n",
    "            delta_x /= 10\n",
    "            print(\"delta_x:\", delta_x)\n",
    "\n",
    "    def delta_y_lower():\n",
    "        nonlocal delta_x, delta_y\n",
    "        with output_delta_y: \n",
    "            output_delta_y.clear_output(wait = True)\n",
    "            print(\"Delta before\", delta_y)\n",
    "            delta_y = delta_y * 10\n",
    "            print(\"delta_y:\", delta_y)\n",
    "    \n",
    "    button_left = Button(description=\"←\")\n",
    "    button_right = Button(description=\"→\")\n",
    "    button_up = Button(description=\"↑\")\n",
    "    button_down = Button(description=\"↓\")\n",
    "    \n",
    "    button_step_x_up = Button(description=\"dx ↑\")\n",
    "    button_step_x_down = Button(description=\"dx ↓\")\n",
    "    button_step_y_up = Button(description=\"dy↑\")\n",
    "    button_step_y_down = Button(description=\"dy↓\")\n",
    "\n",
    "\n",
    "    # TODO: Arreglar que se pueda modificar el paso con el que se avanza. No se ve el output y no se modifica el valor\n",
    "    button_step_x_up.on_click(delta_x_bigger)\n",
    "    button_step_x_down.on_click(delta_x_lower)\n",
    "    button_step_y_up.on_click(delta_y_bigger)\n",
    "    button_step_y_down.on_click(delta_y_lower)\n",
    "    \n",
    "    \n",
    "    steps_x = VBox([button_step_x_up, button_step_x_down])\n",
    "    steps_y = VBox([button_step_y_up, button_step_y_down])\n",
    "\n",
    "\n",
    "    button_left.on_click(move_left)\n",
    "    button_right.on_click(move_right)\n",
    "    button_up.on_click(move_up)\n",
    "    button_down.on_click(move_down)\n",
    "    \n",
    "    arrow_buttons = HBox([button_left, button_right, button_up, button_down, steps_x, steps_y])\n",
    "    \n",
    "    # Organizar los botones en un layout horizontal\n",
    "    hbox_layout = widgets.Layout(display='flex', flex_flow='row wrap', align_items='flex-start')\n",
    "    \n",
    "    hbox = HBox(buttons, layout=hbox_layout)\n",
    "\n",
    "    # Mostrar el gráfico y los botones\n",
    "    box_layout = widgets.Layout(display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='center',\n",
    "                width='100%')\n",
    "\n",
    "    box = VBox([hbox, arrow_buttons, output_move, output_delta_x, output_delta_y, fig, output_windows], layout=box_layout)\n",
    "    \n",
    "    display(box)\n",
    "\n",
    "\n",
    "\n",
    "# Crear el gráfico interactivo\n",
    "# Cuidado con el stride\n",
    "ts_plot_interactive(df, selected_indices, meaningful_features_subset_ids, w, stride = stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "run_dr.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep(0.025)\n",
    "beep(0.025)\n",
    "beep(0.025)\n",
    "print(\"Execution ended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
